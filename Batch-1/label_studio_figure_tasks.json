[
  {
    "data": {
      "arxiv_id": "2505.23807v3",
      "paper_title": "DLP Dynamic Layerwise Pruning in Large Language Models",
      "figure_filename": "003_2505.23807v3_Figure2.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/003_2505.23807v3_Figure2.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/003_2505.23807v3_DLP Dynamic Layerwise Pruning in Large Language Models.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/003_2505.23807v3_DLP Dynamic Layerwise Pruning in Large Language Models.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "WikiText validation perplexity of LLaMA1-7B, LLaMA1-13B and Vicuna-7B pruned by various $\\text{M}$ at 70% sparsity using OWL.",
      "gpt_annotation": "The plot illustrates the perplexity of three different models, LLAMA1-7B, LLAMA1-13B, and Vicuna-7B, as a function of the parameter M. The x-axis represents the variable M, with a linear scale ranging from 3 to 8, and tick labels at integer intervals. The y-axis denotes perplexity, also on a linear scale, ranging from 20.0 to 40.0, with tick labels at intervals of 5. The green line, representing LLAMA1-7B, utilizes circular markers and a solid line style. It begins at approximately 35.0 perplexity at M=3, reaches a minimum of around 29.0 at M=5, and increases to about 36.0 at M=8. The orange line for LLAMA1-13B features star-shaped markers with a solid line. It starts at approximately 23.0 at M=3, decreases to a low point of near 21.0 at M=6, and then returns to around 23.0 at M=8. The blue line, indicative of Vicuna-7B, uses triangular markers with a solid line. It begins at a perplexity of about 38.0 at M=3, trends downward to reach a minimum of approximately 32.0 at M=5, and rises back to around 38.0 at M=8. Each line demonstrates a distinct pattern across the range of M values."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.23807v3",
      "paper_title": "DLP Dynamic Layerwise Pruning in Large Language Models",
      "figure_filename": "003_2505.23807v3_Figure5.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/003_2505.23807v3_Figure5.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/003_2505.23807v3_DLP Dynamic Layerwise Pruning in Large Language Models.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/003_2505.23807v3_DLP Dynamic Layerwise Pruning in Large Language Models.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Comparison of per-output and per-layer perplexity at different sparsity rates. Due to the significant difference in values between high and low sparsity rates, we use the logarithmic results for comparison.",
      "gpt_annotation": "The bar chart illustrates the relationship between sparsity rate and perplexity, depicted through two different configurations: Per-Output and Per-Layer. The horizontal axis contains the categories labeled as sparsity rates, ordered sequentially from 10% to 80% in increments of 10%, with the axis labeled as \"Sparsity Rate.\" The vertical axis represents the perplexity, labeled \"Perplexity,\" with a logarithmic scale ranging from \\(10^1\\) to \\(10^3\\), marked with regularly spaced tick labels at intervals corresponding to powers of 10. The chart features grouped bars for each category, with blue bars representing the Per-Output configuration and orange bars representing the Per-Layer configuration. The approximate values of the bars increase with higher sparsity rates, starting from around 10 at 10% sparsity to over 100 and approaching 1000 as sparsity reaches 80%. At 70% and 80% sparsity, the bars exhibit a notable increase in height compared to the lower sparsity rates. The bars are not sorted in any particular order beyond the increasing sparsity rate, and none of the bars are visually emphasized with special shading, bolding, or outlining."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.23807v3",
      "paper_title": "DLP Dynamic Layerwise Pruning in Large Language Models",
      "figure_filename": "003_2505.23807v3_Figure6.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/003_2505.23807v3_Figure6.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/003_2505.23807v3_DLP Dynamic Layerwise Pruning in Large Language Models.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/003_2505.23807v3_DLP Dynamic Layerwise Pruning in Large Language Models.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Perplexity of  LLaMA1-7B on different validation datasets under varying quantization bits.",
      "gpt_annotation": "The series of bar charts illustrates the perplexity values associated with different quantization bits for three datasets: WikiText, PTB, and C4. Each chart has the horizontal axis labeled as \"Quantization Bits,\" displaying the categories in increasing order as 3, 4, 8, and 16. The vertical axis for each chart is labeled \"Perplexity,\" with a linear scale ranging from 0 to 400 on the second chart and up to 30 on the first and third charts, with numerical tick intervals of 50 and 5 respectively. In each chart, the bars are grouped in pairs for each quantization bit category; the first bar of each pair is labeled \"Uniform\" and is colored blue, while the second bar is labeled \"Ours\" and is colored orange. In the WikiText chart, the height of the \"Uniform\" bars approximately ranges from 25 to 30, whereas the \"Ours\" bars range from 15 to 20. In the PTB chart, \"Uniform\" bars span approximately from 300 to 400, while \"Ours\" bars range from around 100 to 200. In the C4 chart, \"Uniform\" bars are approximately between 20 and 30, and \"Ours\" bars range from 10 to 20. The bars within each chart are consistently arranged by category for comparison without any specific sorting. A legend is present in each chart, clarifying the colors associated with \"Uniform\" and \"Ours.\" There is no particular emphasis or annotation on any individual bar in the charts."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.23923v1",
      "paper_title": "ChARM Character-based Act-adaptive Reward Modeling for Advanced   Role-Playing Language Agents",
      "figure_filename": "005_2505.23923v1_Figure3.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/005_2505.23923v1_Figure3.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/005_2505.23923v1_ChARM Character-based Act-adaptive Reward Modeling for Advanced   Role-Playing Language Agents.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/005_2505.23923v1_ChARM Character-based Act-adaptive Reward Modeling for Advanced   Role-Playing Language Agents.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The character distribution in RoleplayPref consists of 3 primary categories and 13 subcategories.",
      "gpt_annotation": "The pie chart represents a categorization of different types of characters into three main groups: Virtual Characters, Custom Characters, and Real Characters. It includes a total of ten slices. The central blue segment, labeled \"Virtual Characters,\" occupies about half of the chart and further divides into segments like \"Comic Characters,\" \"Movie Characters,\" \"Teleplay Characters,\" \"Novel Characters,\" and \"Game Characters,\" each shaded in different tones of light blue. \"Custom Characters\" is shown in a light red slice, occupying a smaller portion adjacent to the \"Virtual Characters.\" \"Real Characters\" is depicted in green, with further subdivisions such as \"Scientists,\" \"Artists,\" \"Athletes,\" \"Writers,\" \"Musicians,\" \"Actors,\" and \"Influencers,\" all in varying shades of light green. The chart includes a legend and percentage labels inside the slices to provide exact proportions. There is no visual emphasis such as exploding or offsetting of any slice. The segments within each main category appear to be presented in the same order, without any specific visual ordering based on size."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.23923v1",
      "paper_title": "ChARM Character-based Act-adaptive Reward Modeling for Advanced   Role-Playing Language Agents",
      "figure_filename": "005_2505.23923v1_Figure4.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/005_2505.23923v1_Figure4.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/005_2505.23923v1_ChARM Character-based Act-adaptive Reward Modeling for Advanced   Role-Playing Language Agents.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/005_2505.23923v1_ChARM Character-based Act-adaptive Reward Modeling for Advanced   Role-Playing Language Agents.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Results of Human Evaluations.",
      "gpt_annotation": "The bar chart illustrates the distribution of outcomes categorized as \"Win,\" \"Tie,\" and \"Lose\" for three different entities: \"Claude3.5,\" \"GPT4o,\" and \"Doubao.\" The categories are listed along the vertical axis, labeled with the entity names in a descending order from top to bottom, with no specified scale. The horizontal axis represents the percentage values ranging from 0 to 1.0, with a linear scale and tick marks at intervals of 0.2. Each bar is divided into three segments, corresponding to \"Win,\" \"Tie,\" and \"Lose,\" with each segment represented by distinct colors: pink for \"Win,\" blue for \"Tie,\" and green for \"Lose.\" The bar for \"Claude3.5\" shows approximately 32.0% \"Win,\" 44.0% \"Tie,\" and 24.0% \"Lose.\" The \"GPT4o\" bar displays 56.0% \"Win,\" 18.0% \"Tie,\" and 26.0% \"Lose,\" and the \"Doubao\" bar represents 42.0% \"Win,\" 36.0% \"Tie,\" and 22.0% \"Lose.\" The segments within each bar are ordered consistently from left to right: \"Win,\" \"Tie,\" then \"Lose.\" The chart uses a legend at the bottom to denote the coloring scheme for each outcome category. Bars are not sorted in any specific order beyond the provided categorical arrangement, and there is no additional visual emphasis on any particular bar."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06708v1",
      "paper_title": "Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free",
      "figure_filename": "010_2505.06708v1_Figure6.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/010_2505.06708v1_Figure6.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Mean absolute values before and after gating. The baseline and post-gating values are similar.",
      "gpt_annotation": "The plot illustrates the mean values of hidden states across different layers, providing a comparison before and after a gate, as well as against a baseline. The x-axis is labeled \"Layer\" with a linear scale, ranging from 0 to 25, and tick labels are marked at intervals of 5. The y-axis is labeled \"Abs Mean\" with a linear scale, ranging from 0 to 5, with tick labels at intervals of 1. There are three lines depicted in the plot. The first is a blue dashed line representing the \"hidden state mean before gate,\" which starts at approximately 0.5 in layer 0 and exhibits a generally steady trend until around layer 15, where it climbs significantly, peaking at about 4.8 in layer 24. The second line is an orange dotted line representing the \"hidden state mean after gate,\" which starts near 0.05 at layer 0 and remains fairly constant across the layers, ending slightly below 0.1. The third line is a solid green line for the \"hidden state mean of baseline,\" beginning slightly above 0 at layer 0 and maintaining a nearly flat trajectory throughout, concluding near 0.04 by the end of the x-axis."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06708v1",
      "paper_title": "Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free",
      "figure_filename": "010_2505.06708v1_Figure7.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/010_2505.06708v1_Figure7.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Proportion of SDPA output values below threshold after gating (Left: 1e-2, Right: 1e-3). We also include sparsity measurements obtained by multiplying the average gating score with pre-gating hidden states.",
      "gpt_annotation": "The plot illustrates the sparsity ratio of SDPA output across different layers, under a threshold of 1e-2. The x-axis is labeled \"Layer,\" using a linear scale ranging from 0 to 22, with tick labels at intervals of 5. The y-axis, labeled \"Sparsity Ratio,\" also employs a linear scale and spans from 0.0 to 1.0, featuring tick labels at intervals of 0.2. Three lines are present in the plot. The first line, represented by a blue dashed line with circle markers, shows the \"sparsity ratio before gate\" and begins at around 1.0, decreasing steadily to approximately 0.0 towards the right of the plot. The orange dotted line with no markers represents the \"sparsity ratio after gate,\" starting at about 1.0 and reducing to near 0.1 by layer 10, with slight increases and fluctuations thereafter. The third line, a green dash-dot line with no markers, depicts the \"sparsity ratio after average gate,\" beginning at roughly 1.0, decreasing more smoothly, and stabilizing near 0.1 by layer 15 with minor fluctuations in subsequent layers."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06708v1",
      "paper_title": "Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free",
      "figure_filename": "010_2505.06708v1_Figure8.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/010_2505.06708v1_Figure8.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Proportion of SDPA output values below threshold after gating (Left: 1e-2, Right: 1e-3). We also include sparsity measurements obtained by multiplying the average gating score with pre-gating hidden states.",
      "gpt_annotation": "The plot illustrates the sparsity ratio of SDPA output at a threshold of 1e-3 across various layers. The x-axis is labeled \"Layer,\" using a linear scale, ranging from 0 to 23, with tick labels at intervals of 5. The y-axis is labeled \"Sparsity Ratio,\" also using a linear scale, ranging from 0.0 to 0.8, with tick labels at intervals of 0.2. The first line, represented in dashed blue, indicates the sparsity ratio before the gate, starting at approximately 0.0 at layer 0 and remaining relatively constant throughout. The second line, depicted with an orange dotted line, represents the sparsity ratio after the gate, beginning at about 0.8 at layer 0 and decreasing to around 0.1 by layer 23. The third line, shown with a green dash-dot pattern, reflects the sparsity ratio after the average gate, starting near 0.4 at layer 0 and declining to approximately 0.05 by layer 23."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06708v1",
      "paper_title": "Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free",
      "figure_filename": "010_2505.06708v1_Figure9.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/010_2505.06708v1_Figure9.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Comparison of massive activations and attention sink phenomena across different gating configurations. Row 1 (Baseline): Significant massive activations and attention sinks emerge after the 6th layer. Row 2 (SDPA Gating): Reduced activations and no attention sinks observed. Row 3 (Value Layer Gating): Similar activations to Row 2 but with residual attention sinks. Rows 4\u20135 (Reduced Sparsity via cross-head share and NS-sigmoid): Massive activations and attention sinks resemble the baseline.",
      "gpt_annotation": "The plot illustrates the maximum activation values across different layers in a model under a baseline condition. The x-axis is labeled \"Layer,\" with a linear scale ranging from 0 to 24. The tick labels are spaced at intervals of 5. The y-axis is labeled \"Max Activation Value,\" also using a linear scale, with a range from 0 to 1600 and tick labels at intervals of 200. There are four lines presented in the plot. The blue dashed line, representing \"attn_output,\" starts near zero, remaining relatively flat across the x-axis with slight increases near layers 5 and 20. The orange dotted line, indicating \"attn_residual,\" starts low at the beginning, peaks sharply at layer 5 around 1400, then stabilizes between 1400 and 1600 before descending rapidly after layer 20. The green dash-dot line, representing \"ffn_output,\" remains near zero, with noticeable spikes up to approximately 800 at layers 5 and 15. Lastly, the red solid line labeled \"ffn_residual\" begins at zero, quickly rises to about 1400 near layer 2, maintains a high value until after layer 20 with small fluctuations, and then declines sharply toward the end."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06708v1",
      "paper_title": "Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free",
      "figure_filename": "010_2505.06708v1_Figure10.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/010_2505.06708v1_Figure10.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Comparison of massive activations and attention sink phenomena across different gating configurations. Row 1 (Baseline): Significant massive activations and attention sinks emerge after the 6th layer. Row 2 (SDPA Gating): Reduced activations and no attention sinks observed. Row 3 (Value Layer Gating): Similar activations to Row 2 but with residual attention sinks. Rows 4\u20135 (Reduced Sparsity via cross-head share and NS-sigmoid): Massive activations and attention sinks resemble the baseline.",
      "gpt_annotation": "This plot illustrates the first token attention scores of a baseline model across different layers. The x-axis is labeled \"layer,\" uses a linear scale, ranges from 0 to 23, and has tick labels at intervals of 5, showing the layer number. The y-axis is labeled \"first token attention score,\" also uses a linear scale, with a range from 0.0 to 1.0, and tick labels at intervals of 0.2, indicating the attention score values. There are two lines in the plot. The first line is blue with a solid line style and no markers, representing the first token attention score as it changes across layers. It starts at a value just above 0.0, remaining relatively low until layer 5, after which it shows a sharp increase, reaching a peak above 0.6. The scores fluctuate between approximately 0.4 and 0.8 in successive layers, before declining sharply at the last point. The second line is green, with a dashed line style, and represents the mean attention score of 0.467, remaining constant across all layers."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06708v1",
      "paper_title": "Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free",
      "figure_filename": "010_2505.06708v1_Figure11.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/010_2505.06708v1_Figure11.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Comparison of massive activations and attention sink phenomena across different gating configurations. Row 1 (Baseline): Significant massive activations and attention sinks emerge after the 6th layer. Row 2 (SDPA Gating): Reduced activations and no attention sinks observed. Row 3 (Value Layer Gating): Similar activations to Row 2 but with residual attention sinks. Rows 4\u20135 (Reduced Sparsity via cross-head share and NS-sigmoid): Massive activations and attention sinks resemble the baseline.",
      "gpt_annotation": "The plot illustrates the maximum activation value across different layers for various components labeled as attn_output, ffn_output, attn_residual, and ffn_residual. The x-axis is labeled \"Layer\" and is on a linear scale, ranging from 0 to 23 with tick labels at intervals of 5. The y-axis is labeled \"Max Activation Value,\" also on a linear scale, ranging from 0 to 400 with tick labels at intervals of 50. The line for attn_output is blue and dashed, starting around 3 at Layer 0 and ending near 50 at Layer 23, with some fluctuations in between. The ffn_output line is green and dash-dotted, beginning slightly above 0 at Layer 0 and rising to approximately 180 at Layer 23. The attn_residual line is orange and dotted, starting near 3 at Layer 0 and reaching around 250 at Layer 23, with a sharp increase from Layer 20 onward. The ffn_residual line is solid red, initiating close to 5 at Layer 0 and reaching approximately 350 at Layer 23, showing a steep upward trend, particularly after Layer 15."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06708v1",
      "paper_title": "Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free",
      "figure_filename": "010_2505.06708v1_Figure12.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/010_2505.06708v1_Figure12.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Comparison of massive activations and attention sink phenomena across different gating configurations. Row 1 (Baseline): Significant massive activations and attention sinks emerge after the 6th layer. Row 2 (SDPA Gating): Reduced activations and no attention sinks observed. Row 3 (Value Layer Gating): Similar activations to Row 2 but with residual attention sinks. Rows 4\u20135 (Reduced Sparsity via cross-head share and NS-sigmoid): Massive activations and attention sinks resemble the baseline.",
      "gpt_annotation": "The plot illustrates the \"first token attention score\" of the \"atten_output_gate\" across different layers. The x-axis is labeled \"layer\" and has a linear scale ranging from 0 to 24, with tick labels at intervals of 5. The y-axis is labeled \"first token attention score,\" also with a linear scale ranging from 0.0 to 1.0, and tick labels at intervals of 0.2. The plot features two lines: a solid blue line representing the \"first token attention score\" and a green dashed line indicating the \"mean,\" labeled as 0.048. The blue line starts at around 0.05 at layer 0, exhibits slight fluctuations peaking at approximately 0.16 around layer 5, then varies slightly across the layers, ending near 0.02 at layer 23. The green dashed line remains constant at 0.048 across all layers, serving as a reference point for the average score."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06708v1",
      "paper_title": "Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free",
      "figure_filename": "010_2505.06708v1_Figure13.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/010_2505.06708v1_Figure13.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Comparison of massive activations and attention sink phenomena across different gating configurations. Row 1 (Baseline): Significant massive activations and attention sinks emerge after the 6th layer. Row 2 (SDPA Gating): Reduced activations and no attention sinks observed. Row 3 (Value Layer Gating): Similar activations to Row 2 but with residual attention sinks. Rows 4\u20135 (Reduced Sparsity via cross-head share and NS-sigmoid): Massive activations and attention sinks resemble the baseline.",
      "gpt_annotation": "The plot illustrates the maximum activation value of different components (attn_output, attn_residual, ffn_output, ffn_residual) across layers labeled as v_elementwise. The x-axis is labeled \"Layer,\" with a linear scale ranging from 0 to 25, and tick labels at intervals of 5. The y-axis is labeled \"Max Activation Value,\" also with a linear scale ranging from 0 to 400, with tick labels at intervals of 50. The blue dashed line represents attn_output, starting near 0 at layer 0 and remaining relatively steady with slight fluctuations, ending around 50 at layer 23. The orange dotted line for attn_residual starts near 0 and shows a gradual increase, reaching approximately 150 by layer 23. The green dash-dotted line for ffn_output begins close to 0, exhibiting minor decreases and increases, and reaches about 50 by layer 23. The red solid line for ffn_residual starts near 0, shows a similar initial trend, and then increases rapidly after layer 15, peaking at around 350 by layer 23."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06708v1",
      "paper_title": "Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free",
      "figure_filename": "010_2505.06708v1_Figure14.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/010_2505.06708v1_Figure14.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Comparison of massive activations and attention sink phenomena across different gating configurations. Row 1 (Baseline): Significant massive activations and attention sinks emerge after the 6th layer. Row 2 (SDPA Gating): Reduced activations and no attention sinks observed. Row 3 (Value Layer Gating): Similar activations to Row 2 but with residual attention sinks. Rows 4\u20135 (Reduced Sparsity via cross-head share and NS-sigmoid): Massive activations and attention sinks resemble the baseline.",
      "gpt_annotation": "This plot illustrates the first token attention score of v_elementwise across different layers. The x-axis is labeled \"layer\" and is on a linear scale, ranging from 0 to 24 with tick labels at intervals of 5. The y-axis is labeled \"first token attention score,\" also on a linear scale, ranging from 0.0 to 1.0 with tick labels at intervals of 0.2. There are two lines in the plot. The first line is blue with no markers and a solid style, representing the first token attention score. It starts at a y-value of approximately 0.1 at layer 0, increases to around 0.6 at layer 5, and then fluctuates, ending near 0.2 at layer 23. Peaks are visible at layers 6 and 10, with values near 0.6 and 0.5, respectively. The second line is green with no markers and a dashed style, indicating the mean score of 0.297, which remains constant across all layers."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06708v1",
      "paper_title": "Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free",
      "figure_filename": "010_2505.06708v1_Figure15.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/010_2505.06708v1_Figure15.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Comparison of massive activations and attention sink phenomena across different gating configurations. Row 1 (Baseline): Significant massive activations and attention sinks emerge after the 6th layer. Row 2 (SDPA Gating): Reduced activations and no attention sinks observed. Row 3 (Value Layer Gating): Similar activations to Row 2 but with residual attention sinks. Rows 4\u20135 (Reduced Sparsity via cross-head share and NS-sigmoid): Massive activations and attention sinks resemble the baseline.",
      "gpt_annotation": "This plot illustrates the maximum activation value of different components within neural network layers titled \"Massive Activation of atten_output_gate_head_share.\" The x-axis, labeled \"Layer,\" is on a linear scale ranging from 0 to 25 with tick labels at regular intervals of 5. The y-axis, labeled \"Max Activation Value,\" is also on a linear scale, spanning from 0 to 500 with tick labels at intervals of 100. The blue dashed line with no markers represents \"attn_output,\" starting at approximately 10, peaking around layer 1 at 90, dropping, and then rising again near layer 23 to 80. The orange dotted line with no markers for \"attn_residual\" starts near 0, peaks at layer 4 around 300, and then declines. The green dash-dotted line without markers represents \"ffn_output,\" beginning at 0, peaking at approximately 250 around layer 8, and then descending to near 0. The red solid line with no markers, signifying \"ffn_residual,\" starts at 0, rises sharply to 500 by layer 4, maintains this level until layer 19, and then drops sharply, before the plot ends at about 200 at layer 25."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06708v1",
      "paper_title": "Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free",
      "figure_filename": "010_2505.06708v1_Figure16.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/010_2505.06708v1_Figure16.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Comparison of massive activations and attention sink phenomena across different gating configurations. Row 1 (Baseline): Significant massive activations and attention sinks emerge after the 6th layer. Row 2 (SDPA Gating): Reduced activations and no attention sinks observed. Row 3 (Value Layer Gating): Similar activations to Row 2 but with residual attention sinks. Rows 4\u20135 (Reduced Sparsity via cross-head share and NS-sigmoid): Massive activations and attention sinks resemble the baseline.",
      "gpt_annotation": "The plot illustrates the first token attention score of the \"atten_output_gate_head_share\" across different layers. The x-axis is labeled \"layer\" and uses a linear scale, with a value range from 0 to 22, marked at intervals of 5. The y-axis is labeled \"first token attention score,\" also on a linear scale, ranging from 0.0 to 1.0, with tick intervals of 0.2. There is a solid blue line representing the first token attention score, beginning at around 0.05 at layer 0, experiencing an increase to approximately 0.7 at layer 6, followed by fluctuations and peaking again around 0.7 at layer 15 before declining to about 0.1 at layer 22. A green dashed line represents the mean attention score of 0.301, which remains constant across all layers."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06708v1",
      "paper_title": "Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free",
      "figure_filename": "010_2505.06708v1_Figure17.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/010_2505.06708v1_Figure17.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Comparison of massive activations and attention sink phenomena across different gating configurations. Row 1 (Baseline): Significant massive activations and attention sinks emerge after the 6th layer. Row 2 (SDPA Gating): Reduced activations and no attention sinks observed. Row 3 (Value Layer Gating): Similar activations to Row 2 but with residual attention sinks. Rows 4\u20135 (Reduced Sparsity via cross-head share and NS-sigmoid): Massive activations and attention sinks resemble the baseline.",
      "gpt_annotation": "The plot illustrates the maximum activation values across different layers for four distinct components. The x-axis is labeled \"Layer\" with a linear scale, ranging from 0 to 25, with tick intervals of 5 units. The y-axis represents \"Max Activation Value\" and also uses a linear scale, ranging from 0 to 1600, with tick intervals of 200 units. The blue dashed line with no markers represents \"attn_output,\" starting near 0 at layer 0 and gradually increasing to approximately 1000 at layer 21 before a slight decline. The orange dotted line represents \"attn_residual,\" following a similar trend to the \"ffn_residual\" but starting and ending with lower values, peaking twice, once around 1000 at layer 5 and again at layer 20. The green dash-dot line with no markers represents \"ffn_output,\" showing two distinct peaks at layers 5 and 15, both reaching around 1300. The red solid line signifies \"ffn_residual,\" beginning at layer 0 with a sharp increase to approximately 1500 by layer 1, maintaining high values until it declines sharply after layer 21."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06708v1",
      "paper_title": "Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free",
      "figure_filename": "010_2505.06708v1_Figure18.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/010_2505.06708v1_Figure18.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Comparison of massive activations and attention sink phenomena across different gating configurations. Row 1 (Baseline): Significant massive activations and attention sinks emerge after the 6th layer. Row 2 (SDPA Gating): Reduced activations and no attention sinks observed. Row 3 (Value Layer Gating): Similar activations to Row 2 but with residual attention sinks. Rows 4\u20135 (Reduced Sparsity via cross-head share and NS-sigmoid): Massive activations and attention sinks resemble the baseline.",
      "gpt_annotation": "The plot illustrates the first token attention scores of a component labeled \"atten_output_gate_wo_sparsity\" across various layers. The x-axis is labeled \"layer,\" has a linear scale, and ranges from 0 to 23 with tick labels at regular intervals of 5 (0, 5, 10, 15, 20). The y-axis, labeled \"first token attention score,\" also uses a linear scale, ranging from 0.0 to 1.0, with 0.2 intervals for tick labels. The plot features two lines: one is a solid blue line with no markers, representing the actual first token attention score across the layers. This line starts at approximately 0.0 at layer 0, rises sharply to above 0.6 by layer 5, fluctuates with small peaks and troughs around 0.4-0.7 from layer 6 to layer 21, and then declines towards 0.4 by layer 23. The other line is a green dashed line indicating the mean score, fixed at 0.481 across all layers."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06708v1",
      "paper_title": "Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free",
      "figure_filename": "010_2505.06708v1_Figure19.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/010_2505.06708v1_Figure19.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Distribution of gating scores under different constraints for SDPA output gating variants.",
      "gpt_annotation": "The plot illustrates various gate values of \"atten_output_gate\" across different layers, represented by four different lines indicating mean, standard deviation, minimum, and maximum values. The x-axis is labeled \"layer,\" with a linear scale ranging from 0 to 23, and ticks are labeled at intervals of 5. The y-axis is labeled \"gate value,\" also using a linear scale, with values ranging from 0.0 to 1.0, and tick labels at intervals of 0.2. The blue dashed line, representing the mean, starts at a low value near 0.0 and gradually increases, reaching approximately 0.2 by the final layer. The orange dotted line, indicating the standard deviation, follows a similar trend to the mean, starting near 0.0 and reaching similar levels by the end. The green dash-dot line shows the minimum values, remaining flat near 0.0 across all layers. The red solid line represents the maximum values, maintaining a steady level near 1.0 throughout the plot."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06708v1",
      "paper_title": "Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free",
      "figure_filename": "010_2505.06708v1_Figure20.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/010_2505.06708v1_Figure20.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Distribution of gating scores under different constraints for SDPA output gating variants.",
      "gpt_annotation": "The plot illustrates the gate values of the attention output gate scaler across different layers. The x-axis is labeled \"layer\" and uses a linear scale, ranging from 0 to 25 with tick marks at regular intervals of 5. The y-axis is labeled \"gate value\" and also uses a linear scale, ranging from 0.0 to 1.0 with tick marks at intervals of 0.2. The plot includes four lines. The first line is blue with a dashed style and no markers, representing the \"mean\" gate values. It starts near 0.1, peaks just above 0.2 at around layer 15, and decreases slightly towards the end. The second line is orange with a dotted style, representing the \"std\" gate values. It starts just above 0.0, gradually increasing to about 0.15 by the end of the scale. The third line is green with a dash-dot style, representing the \"min\" gate values, remaining relatively flat near 0.0 across all layers. The fourth line is red with a solid style, representing the \"max\" gate values. It starts around 0.3, sharply increases to above 0.9 at layer 5, and fluctuates slightly while maintaining values above 0.8 towards the end."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06708v1",
      "paper_title": "Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free",
      "figure_filename": "010_2505.06708v1_Figure21.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/010_2505.06708v1_Figure21.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Distribution of gating scores under different constraints for SDPA output gating variants.",
      "gpt_annotation": "The plot illustrates the values of attention output gate head shares across different layers, with four lines representing mean, standard deviation, minimum, and maximum gate values. The x-axis is labeled \"layer,\" has a linear scale, and ranges from 0 to 23, with tick labels at intervals of 5. The y-axis, labeled \"gate value,\" also uses a linear scale ranging from 0.0 to 1.0, with tick labels at intervals of 0.2. The mean line is blue, uses a dashed line style, and begins at a gate value of approximately 0.05 at layer 0, reaching a peak near 0.25 at layer 15 before decreasing slightly towards 0.2 by layer 23. The standard deviation line is orange, with a dotted style, starting near 0.0 and gradually increasing to around 0.1 by layer 23. The minimum line is green with a dash-dot style, largely remaining close to 0.0 across all layers. The maximum line is red with a solid style, starting near 0.4, peaking above 0.9 around layer 6, and maintaining values between 0.8 and 0.9 through layer 23."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06708v1",
      "paper_title": "Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free",
      "figure_filename": "010_2505.06708v1_Figure22.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/010_2505.06708v1_Figure22.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/010_2505.06708v1_Gated Attention for Large Language Models Non-linearity, Sparsity, and   Attention-Sink-Free.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Distribution of gating scores under different constraints for SDPA output gating variants.",
      "gpt_annotation": "The plot illustrates the gate values of \"atten_output_gate_wo_sparsity\" across different layers. The x-axis is labeled \"layer,\" with a linear scale ranging from 0 to 25 and tick marks at intervals of 5. The y-axis is labeled \"gate value,\" with a linear scale ranging from 0.0 to 1.0, featuring tick marks at intervals of 0.2. The plot includes four lines, each representing a different statistical measure. A dashed blue line, labeled \"mean,\" starts at approximately 0.7 at layer 0, remains relatively stable across subsequent layers, and ends slightly above 0.6 at layer 23. The dotted orange line, labeled \"std,\" begins near 0.0 at layer 0 and shows a gradual increase, reaching around 0.2 at layer 24. A dash-dot green line, marked as \"min,\" starts below 0.6 at layer 0, fluctuates slightly, and stabilizes near 0.5 at layer 24. The solid red line, representing \"max,\" starts slightly above 0.8 at layer 0 and gradually increases to a value of 1.0 by layer 6, maintaining this level until the final layer 24."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.22582v1",
      "paper_title": "Less, but Better Efficient Multilingual Expansion for LLMs via   Layer-wise Mixture-of-Experts",
      "figure_filename": "011_2505.22582v1_Figure1.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/011_2505.22582v1_Figure1.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/011_2505.22582v1_Less, but Better Efficient Multilingual Expansion for LLMs via   Layer-wise Mixture-of-Experts.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/011_2505.22582v1_Less, but Better Efficient Multilingual Expansion for LLMs via   Layer-wise Mixture-of-Experts.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The similarity of HSAs in different languages across all layers within Qwen1.5-1.8B.",
      "gpt_annotation": "The plot illustrates the similarity across different layers as indicated by the lines representing multilingual language pairs. The x-axis is labeled \"Layer,\" uses a linear scale, and ranges from 0 to 24 with regularly spaced tick marks at intervals of 2. The y-axis is labeled \"Similarity,\" also using a linear scale, with values ranging from 0.00 to 0.45, having tick marks at intervals of 0.05. The plot features four lines representing different language pairs. The red line with square markers and a solid style represents \"en&en,\" starting at approximately 0.05 at layer 0, peaking at around 0.40 near layer 6, and concluding at about 0.45 by layer 23. The blue line with circular markers and also a solid style denotes \"en&es,\" starting near 0.09, peaking at about 0.41 around layer 6, and ending near 0.41. The green line with triangle markers in a solid style is for \"en&el,\" beginning at approximately 0.06, peaking at about 0.25 at layer 6, and finishing near 0.23. Lastly, the pink line with diamond markers and a solid style represents \"en&bn,\" starting at approximately 0.04, peaking at around 0.25 at layer 6, and concluding near 0.22."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.22582v1",
      "paper_title": "Less, but Better Efficient Multilingual Expansion for LLMs via   Layer-wise Mixture-of-Experts",
      "figure_filename": "011_2505.22582v1_Figure7.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/011_2505.22582v1_Figure7.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/011_2505.22582v1_Less, but Better Efficient Multilingual Expansion for LLMs via   Layer-wise Mixture-of-Experts.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/011_2505.22582v1_Less, but Better Efficient Multilingual Expansion for LLMs via   Layer-wise Mixture-of-Experts.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": Caption not extracted",
      "gpt_annotation": "The bar chart illustrates the distribution of the number of experts across various layers, with the horizontal axis representing the layers labeled from 1 to 24 in ascending order, and the vertical axis denoting the \"Number of Experts\" on a linear scale ranging from 0 to 12, with tick intervals of 2. Each layer's bar is divided into three segments representing different categories of experts, identified by the legend as G0 (blue), G1 (green), and G2 (yellow). The bars are stacked vertically, with G0 at the base, G1 in the middle, and G2 on top. The approximate total heights of the bars vary, with the tallest bar corresponding to layer 1 and reaching about 10 experts, while several bars, such as layers 11 to 15, maintain shorter heights around 4 experts. The bars are organized sequentially by layer number without additional highlighting or emphasis on specific groups."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.22582v1",
      "paper_title": "Less, but Better Efficient Multilingual Expansion for LLMs via   Layer-wise Mixture-of-Experts",
      "figure_filename": "011_2505.22582v1_Figure8.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/011_2505.22582v1_Figure8.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/011_2505.22582v1_Less, but Better Efficient Multilingual Expansion for LLMs via   Layer-wise Mixture-of-Experts.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/011_2505.22582v1_Less, but Better Efficient Multilingual Expansion for LLMs via   Layer-wise Mixture-of-Experts.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": Caption not extracted",
      "gpt_annotation": "The bar chart illustrates the distribution of the number of experts across different layers, represented on the horizontal axis labeled as \"Layer\" with categories numbered sequentially from 1 to 24. The vertical axis represents the \"Number of Experts,\" with a linear scale ranging from 0 to 10 and tick intervals of 2. Each layer contains stacked bars divided into three categories: G0, G1, and G2, represented in blue, green, and yellow, respectively. The height of the bars varies, with the total height of each bar corresponding to the cumulative number of experts in that layer. For example, the sum of expert numbers in Layer 2 reaches approximately 8, while in Layer 24, it is about 4. The individual stacked segments exhibit varying heights among layers, with some bars having a higher count of a specific group, such as G2 in yellow, which is taller in layers like 1 and 16. The arrangement does not appear to follow a specific sorting order, and no visual emphasis is applied to any bars."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.11545v1",
      "paper_title": "TARGET Benchmarking Table Retrieval for Generative Tasks",
      "figure_filename": "012_2505.11545v1_Figure3.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/012_2505.11545v1_Figure3.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/012_2505.11545v1_TARGET Benchmarking Table Retrieval for Generative Tasks.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/012_2505.11545v1_TARGET Benchmarking Table Retrieval for Generative Tasks.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Influence of $k$ on retrieval performance with various baselines on the FeTaQA dataset, confirming the expectation that performance gradually increases with $k$, most significantly for dense embedding approaches.",
      "gpt_annotation": "The plot illustrates how different embedding methods perform in terms of recall as a function of the value of \\( k \\). The x-axis is labeled \"k\" and uses a linear scale ranging from 0 to 50, with tick marks at intervals of 10 units. The y-axis is labeled \"recall,\" also employing a linear scale, ranging from 0.0 to 1.0, with tick marks labeled at intervals of 0.2. The line representing \"sparse lexical repr. (w/ title)\" is a solid gray line with round markers, starting at recall 0.0 at \\( k = 0 \\) and exhibiting a slight upward trend. The \"sparse lexical repr. (w/o title)\" is shown as a purple dashed line with triangle markers, starting at recall 0.1 and gradually increasing. The \"dense metadata emb.\" is a teal solid line with round markers, beginning at recall 0.4 and steadily rising to above 0.6 at \\( k = 50 \\). The \"dense table emb.\" is a dark blue solid line with circular markers, starting high at recall 0.6 at \\( k = 0 \\) and increasing to about 0.9 at \\( k = 50 \\). The \"dense table emb. (col names only)\" is a red dashed line with circular markers, starting at recall 0.2 and showing a consistent rise to 0.5 at the final point. Lastly, the \"dense row emb.\" is a solid dark blue line with circular markers, beginning at recall 0.5 and increasing to approximately 0.85."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.11545v1",
      "paper_title": "TARGET Benchmarking Table Retrieval for Generative Tasks",
      "figure_filename": "012_2505.11545v1_Figure4.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/012_2505.11545v1_Figure4.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/012_2505.11545v1_TARGET Benchmarking Table Retrieval for Generative Tasks.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/012_2505.11545v1_TARGET Benchmarking Table Retrieval for Generative Tasks.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Influence of corpus size on retrieval, illustrating the sensitivity in retrieval performance of dense retrievers when the corpus reaches a large scale.",
      "gpt_annotation": "The plot illustrates the relationship between the number of extra tables and the recall values for two different methods: dense table embedding and sparse lexical representation (tf-idf). The x-axis is labeled \"# extra tables\" and uses a linear scale, ranging from 0 to approximately 50,000, with regular tick intervals at 0, 20,000, and 40,000. The y-axis is labeled \"recall\" and also uses a linear scale, with values ranging from 0 to 0.7, and tick intervals at 0.2, 0.4, and 0.6. The blue line with circle markers represents the dense table embedding using a dashed line style. It starts at a recall value of about 0.65 at 0 extra tables and declines to about 0.55 at 40,000 extra tables. The orange line with circle markers represents the sparse lexical representation (tf-idf) with a dashed line style. It begins at a recall value of slightly above 0.1 at 0 extra tables and remains relatively steady, with a slight decrease, ending slightly below 0.1 at 40,000 extra tables."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00688v1",
      "paper_title": "Existing Large Language Model Unlearning Evaluations Are Inconclusive",
      "figure_filename": "013_2506.00688v1_Figure2.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/013_2506.00688v1_Figure2.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/013_2506.00688v1_Existing Large Language Model Unlearning Evaluations Are Inconclusive.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/013_2506.00688v1_Existing Large Language Model Unlearning Evaluations Are Inconclusive.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": Evidence of spurious generalization. Finetuning attacks significantly improve accuracy on TOFU's test set. This finding indicates that it may be possible to spuriously generalize between the retain and forget sets in finetuning-based unlearning evaluations.",
      "gpt_annotation": "This plot illustrates the accuracy on the TOFU-MCQ holdout set as a function of the number of finetuning samples. The x-axis is labeled \"Number of finetuning samples,\" with a linear scale ranging from 0 to 500. The tick labels are positioned at intervals of 100 units. The y-axis, labeled \"Accuracy,\" also follows a linear scale and spans from 0.475 to 0.600, with tick labels at intervals of 0.025 units. There are two lines depicted in the plot. The first line, representing \"TOFU-MCQ Finetuning,\" is green with a solid line style and circular markers. Starting at an accuracy of approximately 0.475 with 0 samples, it shows an upward trend, peaking at an accuracy of 0.600 at 500 samples. The second line, labeled \"Assumption,\" is orange with a dashed line style and circular markers, and remains constant at an accuracy of approximately 0.475 across all sample values from 0 to 500."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00688v1",
      "paper_title": "Existing Large Language Model Unlearning Evaluations Are Inconclusive",
      "figure_filename": "013_2506.00688v1_Figure4.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/013_2506.00688v1_Figure4.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/013_2506.00688v1_Existing Large Language Model Unlearning Evaluations Are Inconclusive.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/013_2506.00688v1_Existing Large Language Model Unlearning Evaluations Are Inconclusive.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": Evidence of information injection. Relearning effectiveness varies significantly when the data format of the downstream task generation format differs between evaluation and deployment.",
      "gpt_annotation": "The plot illustrates the accuracy results based on MCQ evaluation across different finetuning sample sizes. The x-axis represents the \"Number of finetuning samples\" with a linear scale ranging from 0 to 50. Numeric values are regularly spaced at intervals of 5. The y-axis indicates \"Accuracy\" on a linear scale, ranging from 0.3 to 0.7, with tick marks at intervals of 0.1. The plot contains four lines. The first line is solid green with circular markers, representing \"MCQ Unlearn, Corpora Relearn,\" beginning at approximately (0, 0.3) and ending around (50, 0.5), showing a gradual increase. A dashed green line indicates \"MCQ Unlearn, MCQ Relearn,\" also with circular markers, starting near (0, 0.3) and rising to approximately (50, 0.6), showing a generally increasing trend with a peak around (10, 0.5). An orange solid line with triangular markers represents \"Corpora Unlearn, Corpora Relearn,\" starting at (0, 0.3) and rising sharply to (10, 0.7), followed by a stable section to (50, 0.65). Lastly, a dashed orange line with triangular markers indicates \"Corpora Unlearn, MCQ Relearn,\" starting from approximately (0, 0.3), rising to (5, 0.7), and then stabilizing towards (50, 0.6)."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00688v1",
      "paper_title": "Existing Large Language Model Unlearning Evaluations Are Inconclusive",
      "figure_filename": "013_2506.00688v1_Figure5.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/013_2506.00688v1_Figure5.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/013_2506.00688v1_Existing Large Language Model Unlearning Evaluations Are Inconclusive.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/013_2506.00688v1_Existing Large Language Model Unlearning Evaluations Are Inconclusive.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": Enhanced GCG task dependence. The effectiveness of Enhanced GCG varies widely across downstream tasks. Whereas this method nearly recovers the base model's accuracy for maximum letter probability generation, it offers little improvement in accuracy for maximum text generation tasks.",
      "gpt_annotation": "The bar chart illustrates the evaluation of different models based on tasks, specifically focusing on MCQ Eval and Generation Eval. The horizontal axis categorically represents the evaluation tasks with labels \"MCQ Eval\" and \"Generation Eval.\" The vertical axis is labeled \"Accuracy\" with a linear scale ranging from 0.0 to 0.6, with tick marks at intervals of 0.1. Each task includes stacked bars representing different models, categorized by a legend: \"Base\" is depicted in red, \"RMU with Enhanced GCG\" in light green, and \"RMU\" in yellow. For \"MCQ Eval,\" the \"Base\" model has a bar reaching an approximate value of 0.644, while \"RMU with Enhanced GCG\" and \"RMU\" are stacked with values of about 0.539 and 0.299, respectively. In \"Generation Eval,\" the \"Base\" model shows an accuracy of about 0.597, and the stacked bars for \"RMU with Enhanced GCG\" and \"RMU\" exhibit values of approximately 0.263 and 0.106, respectively. The bars are not sorted by height or alphabetically but are grouped by task type, with no additional visual emphasis such as shading or annotations."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00688v1",
      "paper_title": "Existing Large Language Model Unlearning Evaluations Are Inconclusive",
      "figure_filename": "013_2506.00688v1_Figure7.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/013_2506.00688v1_Figure7.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/013_2506.00688v1_Existing Large Language Model Unlearning Evaluations Are Inconclusive.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/013_2506.00688v1_Existing Large Language Model Unlearning Evaluations Are Inconclusive.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Inconclusive ACR evaluations on WMDP-Bio. Success rates of different models on different datasets and tasks. The error bars show the standard errors. For each task, the conclusion about the relative effectiveness of unlearning methods differ. Top: CHOOSE task. The success rates of the base and unlearned models are relatively similar; unlearning tends to decrease the success rate marginally. Middle: OPTION task. The success rates of the unlearned models are significantly lower than the base model. Bottom: GENERATE task. The RMU model has a lower success rate than the base model on WMDP-Bio and WMDP-Cyber. However, NPO tends to increase success rates.",
      "gpt_annotation": "The chart consists of three sections, each labeled as a different downstream task: CHOOSE, OPTION, and GENERATE. The purpose of the chart is to represent the success rate associated with various models or methods across these tasks. For each section, categories are displayed on the horizontal axis, labeled with different models: Base, RMU, NPO-Bio, NPO-Cyber, and Forget Set. The vertical axis represents the \"Success Rate,\" ranging from 0.0 to 1.0, with a linear scale and incremented by 0.1. Each section consists of grouped bars representing the different models, color-coded as per the attached legend: Base (orange), RMU (yellow with stripes), NPO-Bio (light yellow), NPO-Cyber (green), and Forget Set (green with stripes). In the CHOOSE section, the success rates range from approximately 0.80 (NPO-Cyber) to 0.98 (Base). In the OPTION section, values vary from 0.42 (RMU) to 0.92 (NPO-Bio), while in the GENERATE section, they range from 0.38 (Forget Set) to 0.71 (Base). Bars are ordered by model as per the legend and are not particularly emphasized or sorted beyond their groupings."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17120v1",
      "paper_title": "Self-Interpretability LLMs Can Describe Complex Internal Processes that   Drive Their Decisions, and Improve with Training",
      "figure_filename": "014_2505.17120v1_Figure3.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/014_2505.17120v1_Figure3.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/014_2505.17120v1_Self-Interpretability LLMs Can Describe Complex Internal Processes that   Drive Their Decisions, and.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/014_2505.17120v1_Self-Interpretability LLMs Can Describe Complex Internal Processes that   Drive Their Decisions, and.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Results of Experiments 1 and 2. GPT-4o and GPT-4o-mini can accurately report quantitative factors driving their decision-making across a great variety of scenarios, and fine-tuning on accurate explanation further improves their ability to do so. Left: Models were making choices based on preferences instilled in them by fine-tuning. Each point corresponds to a single attribute (e.g., condo ceiling height; 5 per choice contexts, 100 choice contexts). Location in the x-dimension corresponds to the weight that a model assigned to an attribute (as reflected in their decisions). Location in the y-dimension corresponds to the weight that a model reported assigning to that attribute when prompted explicitly. The weights the models reported meaningfully correlated with the weights that actually guided their decisions, and fine-tuning on examples of accurate reports further improved their accuracy. Right: The Pearson correlation between the models' reported and learned attribute weights before and after training (blue and purple, respectively). The reports of the base models that had not undergone this fine-tuning were almost entirely uncorrelated with the learned preferences (gray). Thus, the accuracy of the fine-tuned models must reflect their ability to report their new (fine-tuned) preferences and not an informed guess about their preferences based on their general background knowledge. Error bars indicate 95% HDIs.",
      "gpt_annotation": "The bar chart illustrates the self-report accuracy related to instilled preferences for two models, GPT-4o and GPT-4o-mini, under different training conditions. The categories are displayed along the horizontal axis and include GPT-4o and GPT-4o-mini, arranged in this order. The value axis is vertical, labeled as \"Self-Report Accuracy (Instilled Preferences)\" and ranges from 0.00 to 1.00 on a linear scale, with tick marks at intervals of 0.25. Each model category is represented by three bars: gray for \"Control,\" light blue for \"Before Introspection Training,\" and dark blue for \"After Introspection Training,\" as indicated in the legend. For GPT-4o, the gray bar is the shortest, at approximately 0.10, the light blue bar is around 0.65, and the dark blue bar is nearly 0.80. For GPT-4o-mini, the gray bar is about 0.10, the light blue bar is approximately 0.70, and the dark blue bar is around 0.85. Bars are grouped side by side within each model category, without any specific sorting, and no bars are visually emphasized."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17120v1",
      "paper_title": "Self-Interpretability LLMs Can Describe Complex Internal Processes that   Drive Their Decisions, and Improve with Training",
      "figure_filename": "014_2505.17120v1_Figure5.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/014_2505.17120v1_Figure5.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/014_2505.17120v1_Self-Interpretability LLMs Can Describe Complex Internal Processes that   Drive Their Decisions, and.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/014_2505.17120v1_Self-Interpretability LLMs Can Describe Complex Internal Processes that   Drive Their Decisions, and.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Results of Experiment 3. Introspection training generalized to improving the models' accuracy about the attribute weights that they natively used in other choice contexts (weights that were unchanged by fine-tuning). Left: As in Figure <ref>, each point corresponds to a single attribute (5 per choice contexts, 100 choice contexts). Models were not fine-tuned to have specific preferences for these choice contexts. Nevertheless, fine-tuning on examples of accurate introspection made the models more accurate in reporting the weights that they assigned to these attributes. Right: Comparison of the Pearson correlations between the attribute weights that the models reported and those they natively used (in choice contexts that were not been part of the preference training), before and after introspection training. Error bars indicate 95% HDIs.",
      "gpt_annotation": "The bar chart illustrates self-report accuracy related to native preferences, comparing performance before and after introspection training for two models: GPT-4o and GPT-4o-mini. The horizontal axis contains the categories labeled as GPT-4o and GPT-4o-mini, organized from left to right. The vertical axis represents the self-report accuracy, labeled as \"Self-Report Accuracy (Native Preferences)\" with a linear scale ranging from 0.00 to 1.00, with tick marks at intervals of 0.25. Each category contains two bars: a light blue bar representing accuracy before introspection training and a dark blue bar representing accuracy after introspection training. For GPT-4o, the light blue bar reaches approximately 0.55, while the dark blue bar is around 0.75. For GPT-4o-mini, the light blue bar also reaches approximately 0.55, and the dark blue bar is approximately 0.75. Each bar is associated with error bars to denote variability in the data. The bars are color-coded as per a legend at the top left and show no specific emphasis or sorting."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.20215v1",
      "paper_title": "Dependency Parsing is More Parameter-Efficient with Normalization",
      "figure_filename": "015_2505.20215v1_Figure1.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/015_2505.20215v1_Figure1.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/015_2505.20215v1_Dependency Parsing is More Parameter-Efficient with Normalization.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/015_2505.20215v1_Dependency Parsing is More Parameter-Efficient with Normalization.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Effective rank $\\rho(W)$ reduces over training epochs as we increase $N$ of BiLSTM layers.",
      "gpt_annotation": "The plot illustrates the behavior of multiple series across certain values of N, represented by different lines labeled from 1 to 10. The x-axis is labeled with a scale from 0 to 1000, in linear format, with numerical tick marks at intervals of 200. The y-axis ranges from 40 to 55, also in linear format, with numerical tick marks at intervals of 5. The plot contains ten lines, each with distinct characteristics. The line corresponding to N=1 is blue, solid, and decreases sharply from about 52.7 at x=0 to about 47 at x=1000. The orange line for N=2 is solid and flat, maintaining a constant value of around 51 throughout. The green line, representing N=3, also remains mostly flat at 49. Similarly, the red line for N=4 stays steady around 48. The purple line for N=5 begins at around 49.6, gradually decreasing and displaying minor fluctuations before stabilizing around 46. The brown line for N=6 starts at 49.5, also following a declining trend to approximately 46, with slight variability. The pink line for N=7 displays a minor initial descent from 48.6 to 46.5, with fluctuations. The gray line for N=8 maintains an almost constant level near 48. The khaki line for N=9 maintains a flat trend around 47. The cyan line for N=10 starts near 52.5, descends consistently to around 43, and displays slight ups and downs along its path."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.10981v1",
      "paper_title": "Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling A   Perspective of Probability Theory",
      "figure_filename": "017_2505.10981v1_Figure19.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/017_2505.10981v1_Figure19.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/017_2505.10981v1_Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling A   Perspective of Probability.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/017_2505.10981v1_Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling A   Perspective of Probability.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Results of combining adaptively scaling and dynamically choosing the optimal $\\prompt{i}$ on Qwen2.5-7B-Instruct on MATH.",
      "gpt_annotation": "The plot illustrates the accuracy of various methods over different sampling times. The x-axis is labeled \"Sampling Time\" with a linear scale ranging from 0 to 70, marked at intervals of 10 units. The y-axis is labeled \"Accuracy,\" also on a linear scale, ranging from 60 to 90, marked at intervals of 5 units. There are several lines in the plot: a purple line with triangle markers using a solid style for \"DiP,\" starting at approximately 63 at x=0, peaking around 75 at x=50, and remaining steady thereafter. The blue line with circle markers, labeled \"CoT,\" starts at about 64 and rises to approximately 76 at x=50, maintaining its position. The light blue line with triangle markers, labeled \"L2M,\" starts at around 67, increases to approximately 77 by x=40, and levels off. An orange line with triangle markers, labeled \"AnP,\" begins at around 69, reaching close to 74 by x=40, and shows little change thereafter. The black line with square markers, labeled \"Dynamic w/ Oracle,\" begins at approximately 84, fluctuating slightly around this value across the x-axis. The red line with circle markers, labeled \"Adaptive + Dynamic w/ Oracle,\" starts at about 87, with slight variations around this value throughout the plotted range."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.10981v1",
      "paper_title": "Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling A   Perspective of Probability Theory",
      "figure_filename": "017_2505.10981v1_Figure20.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/017_2505.10981v1_Figure20.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/017_2505.10981v1_Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling A   Perspective of Probability.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/017_2505.10981v1_Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling A   Perspective of Probability.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Results of combining adaptively scaling and dynamically choosing the optimal $\\prompt{i}$ on GLM-4-9B-Instruct on MATH.",
      "gpt_annotation": "The line plot illustrates the relationship between sampling time and accuracy for various methods. The x-axis, labeled \"Sampling Time,\" is linear with a range from 0 to 70, and is marked at 10-unit intervals. The y-axis, labeled \"Accuracy,\" is also linear, ranging from 45 to 85, with tick marks at 5-unit intervals. The plot includes several lines representing different methods. The purple line with cross markers and a solid style for \"DiP\" starts at approximately 48 and ends around 57. The blue line with circle markers and a solid style for \"CoT\" begins near 49 and reaches about 61. The light blue line with triangle-up markers and a solid style for \"L2M\" starts at 50 and rises steadily to about 63. The orange line with triangle-down markers and a solid style for \"SBP\" starts around 49 and gradually increases to about 58. The red line with circle markers and a dashed style for \"Adaptive + Dynamic w/ Oracle\" begins around 73 and slightly increases to about 79, showing a notable peak early in the sampling period. The black line with square markers and a dashed style for \"Dynamic w/ Oracle\" starts and maintains a value close to 69 throughout the time range. Finally, the brown line with triangle-up markers for \"AnP\" starts near 47 and increases to about 57. Each line shows variations and trends in accuracy over the given sampling times."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.10981v1",
      "paper_title": "Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling A   Perspective of Probability Theory",
      "figure_filename": "017_2505.10981v1_Figure21.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/017_2505.10981v1_Figure21.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/017_2505.10981v1_Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling A   Perspective of Probability.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/017_2505.10981v1_Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling A   Perspective of Probability.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Results of combining adaptively scaling and dynamically choosing the optimal $\\prompt{i}$ on Phi-3.5-mini-Instruct on MATH.",
      "gpt_annotation": "The plot illustrates the accuracy of different methods over a range of sampling times. The x-axis is labeled \"Sampling Time,\" uses a linear scale, and spans from 0 to 70 with tick labels at intervals of 10. The y-axis is labeled \"Accuracy,\" also utilizing a linear scale, ranging from 35 to 80 with ticks every 5 units. The plot contains six lines representing different methods. The purple line with triangle markers and a solid style represents \"DiP,\" starting near 38 and peaks at approximately 54 around the 20 mark before stabilizing. The blue line with circle markers and a solid style represents \"CoT,\" starting near 38 and ending around 52 with a consistent rise. The cyan line with triangle markers and a solid style represents \"L2M,\" starting around 39 and finishing near 55, also showing a steady increase. The orange line with triangle markers and a solid style represents \"AnP,\" starting near 39 and peaking at around 57 before leveling off. The pink line with triangle markers and a solid style represents \"SBP,\" demonstrating a similar trend to \"AnP,\" starting around 38 and ending near 54. The black line with square markers and a dashed style represents \"Dynamic w/ Oracle,\" remaining relatively steady across the plot, starting at approximately 64 and slightly increasing to around 66. The red line with circle markers and a dashed style represents \"Adaptive + Dynamic w/ Oracle,\" starting at 69 and increasing steadily to stabilize around 76."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.10981v1",
      "paper_title": "Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling A   Perspective of Probability Theory",
      "figure_filename": "017_2505.10981v1_Figure22.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/017_2505.10981v1_Figure22.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/017_2505.10981v1_Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling A   Perspective of Probability.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/017_2505.10981v1_Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling A   Perspective of Probability.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Results of combining adaptively scaling and dynamically choosing the optimal $\\prompt{i}$ on Gemini-1.5-Flash on MATH.",
      "gpt_annotation": "The purpose of the given plot is to illustrate accuracy over time for different methods. The x-axis, labeled \"Sampling Time,\" uses a linear scale with values ranging from 0 to 25 and has tick labels at intervals of 5. The y-axis, labeled \"Accuracy,\" also uses a linear scale, ranging from 74 to 90, with tick labels at intervals of 2. The plot includes several lines: a purple line with triangle markers and a solid style represents \"DiP,\" starting around 78 at time 0 and ending around 84 at time 25. The blue line with cross markers in a solid style represents \"CoT,\" beginning at approximately 76 and rising to about 83. The cyan line with triangle markers, representing \"L2M,\" starts near 77 and reaches around 82. An orange line with triangle markers for \"SBP\" begins at 74 and increases steadily to about 80. The red line with square markers and a dashed style for \"Dynamic w/ Oracle\" starts at 88 and remains relatively steady throughout. The black line with square markers and a dashed style for \"Adaptive + Dynamic w/ Oracle\" begins at 90 and maintains consistency across the plot. Finally, a brown line with triangle markers for \"AnP\" starts near 77 and ends around 83, with similar gradual changes across the x-axis."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.10981v1",
      "paper_title": "Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling A   Perspective of Probability Theory",
      "figure_filename": "017_2505.10981v1_Figure23.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/017_2505.10981v1_Figure23.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/017_2505.10981v1_Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling A   Perspective of Probability.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/017_2505.10981v1_Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling A   Perspective of Probability.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Results of combining adaptively scaling and dynamically choosing the optimal $\\prompt{i}$ on GPT-4o-mini on MATH.",
      "gpt_annotation": "The plot illustrates the changes in accuracy over time for various methods. The x-axis, labeled \"Sampling Time,\" is linear, ranging from 0 to 25 with tick labels at intervals of 5 units. The y-axis, labeled \"Accuracy,\" is also linear, spanning from 66 to 87, with tick labels at intervals of 3 units. The purple line with triangle markers, representing \"DiP,\" starts at approximately 71 at a sampling time of 0 and rises to around 78 by time 25, displaying a gradual upward trend. The red dashed line with circle markers, representing \"Adaptive + Dynamic w/ Oracle,\" starts at about 85 and remains steady around 87 across the sampling times. The light blue line with triangle-up markers, showing \"L2M,\" begins at around 70, increasing to a peak of approximately 75 at time 12, and remains stable afterward. The orange line with triangle-right markers, representing \"AnP,\" starts at 72, peaks at 76 around time 7, then declines slightly to stabilize near 74. The blue line with triangle-down markers, illustrating \"CoT,\" starts at approximately 71, peaking around 76 at time 13, and remaining consistent after. The black dashed line with square markers, representing \"Dynamic w/ Oracle,\" starts at about 84, showing a minimal decline to around 83.5 by time 25. Lastly, the tan line with triangle-left markers, representing \"SBP,\" starts at 71, peaks near 75 at time 10, and then gradually declines to about 73 by the end of the plot."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.01713v1",
      "paper_title": "SRPO Enhancing Multimodal LLM Reasoning via Reflection-Aware   Reinforcement Learning",
      "figure_filename": "020_2506.01713v1_Figure2.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/020_2506.01713v1_Figure2.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/020_2506.01713v1_SRPO Enhancing Multimodal LLM Reasoning via Reflection-Aware   Reinforcement Learning.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/020_2506.01713v1_SRPO Enhancing Multimodal LLM Reasoning via Reflection-Aware   Reinforcement Learning.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Left: Illustrative examples of reflection improving reasoning. Right: Quantitative comparison on benchmark datasets.",
      "gpt_annotation": "The bar chart illustrates benchmark scores for three different models across four categories. The horizontal axis contains the categories labeled as \"MathVista,\" \"MathVerse,\" \"MMMU-Pro,\" and \"Physics.\" These are evenly spaced and named accordingly. The vertical axis is the value axis labeled \"Score,\" with a linear scale ranging from 35 to 80, and tick mark intervals of 5. Each category features three grouped bars, representing different models: \"SRPO-7B,\" \"GRPO-7B,\" and \"Qwen-2.5-VL-7B.\" The bars are color-coded, with \"SRPO-7B\" in dark blue, \"GRPO-7B\" in medium blue, and \"Qwen-2.5-VL-7B\" in light blue. In \"MathVista,\" the \"SRPO-7B\" bar reaches approximately 75.8, \"GRPO-7B\" is at 72.3, and \"Qwen-2.5-VL-7B\" is at 68.2. For \"MathVerse,\" \"SRPO-7B\" is around 55.8, \"GRPO-7B\" is 52.9, and \"Qwen-2.5-VL-7B\" is 46.3. In \"MMMU-Pro,\" the scores are approximately 42.3 for \"SRPO-7B,\" 39.9 for \"GRPO-7B,\" and 36.9 for \"Qwen-2.5-VL-7B.\" Lastly, in \"Physics,\" \"SRPO-7B\" is at 60.6, \"GRPO-7B\" at 53.5, and \"Qwen-2.5-VL-7B\" at 45.4. The bars are not sorted in a specific visual order other than by model grouping, and there is no visual emphasis such as shading or annotation noted on any of the bars."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.01713v1",
      "paper_title": "SRPO Enhancing Multimodal LLM Reasoning via Reflection-Aware   Reinforcement Learning",
      "figure_filename": "020_2506.01713v1_Figure6.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/020_2506.01713v1_Figure6.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/020_2506.01713v1_SRPO Enhancing Multimodal LLM Reasoning via Reflection-Aware   Reinforcement Learning.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/020_2506.01713v1_SRPO Enhancing Multimodal LLM Reasoning via Reflection-Aware   Reinforcement Learning.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance of various RL methods with and without self-reflection.",
      "gpt_annotation": "The bar chart illustrates the accuracy percentages for different methods across three categories: Mathverse, MMLU, and Physics. The horizontal axis contains the categorical labels Mathverse, MMLU, and Physics, with each category having grouped bars. The vertical axis represents accuracy, labeled \"Accuracy (%)\", with a linear scale ranging from 50% to 62% and tick labels at intervals of 2%. Each category features six bars that are color-coded and patterned according to a legend: DAPO+Self-reflection (solid blue), DAPO (blue with diagonal stripes), SRPO (solid orange), GRPO (orange with diagonal stripes), PPO+Self-reflection (solid green), and PPO (green with diagonal stripes). In Mathverse, the DAPO+Self-reflection bar reaches approximately 56%, while the PPO bar is around 52%. For MMLU, bars range from about 53% (SRPO) to 56% (GRPO), and in Physics, the DAPO+Self-reflection bar reaches close to 60%, with the PPO bar at approximately 58%. The arrangement within each category follows the order from the legend, and no bars appear to be visually emphasized with additional formatting such as shading or bolding."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.01713v1",
      "paper_title": "SRPO Enhancing Multimodal LLM Reasoning via Reflection-Aware   Reinforcement Learning",
      "figure_filename": "020_2506.01713v1_Figure7.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/020_2506.01713v1_Figure7.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/020_2506.01713v1_SRPO Enhancing Multimodal LLM Reasoning via Reflection-Aware   Reinforcement Learning.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/020_2506.01713v1_SRPO Enhancing Multimodal LLM Reasoning via Reflection-Aware   Reinforcement Learning.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": Self-reflection SFT data statistic",
      "gpt_annotation": "The bar chart illustrates the number of samples across various categories of reasoning. The horizontal axis contains the categories, labeled as: Mathematical Reasoning, General Reasoning, Natural Scene Reasoning, Physical Reasoning, Logical Puzzle Reasoning, and Chemical Reasoning. These categories are presented in a descending order based on their values. The vertical axis represents the number of samples, labeled as \"Number of Samples,\" with a logarithmic scale ranging from 10\u00b2 to 10\u2074, and tick labels at regular intervals of 10\u00b2, 10\u00b3, and 10\u2074. Each bar corresponds to a category and is colored light blue. The approximate values for the bars are: Mathematical Reasoning at 7025, General Reasoning at 1709, Natural Scene Reasoning at 413, Physical Reasoning at 232, Logical Puzzle Reasoning at 91, and Chemical Reasoning at 49. The shortest bar is for Chemical Reasoning, and the tallest bar is for Mathematical Reasoning. No bars are visually emphasized beyond the standard display, and there are no grouped or stacked arrangements."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.01713v1",
      "paper_title": "SRPO Enhancing Multimodal LLM Reasoning via Reflection-Aware   Reinforcement Learning",
      "figure_filename": "020_2506.01713v1_Figure8.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/020_2506.01713v1_Figure8.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/020_2506.01713v1_SRPO Enhancing Multimodal LLM Reasoning via Reflection-Aware   Reinforcement Learning.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/020_2506.01713v1_SRPO Enhancing Multimodal LLM Reasoning via Reflection-Aware   Reinforcement Learning.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": RL training data statistic",
      "gpt_annotation": "The bar chart illustrates the number of samples across various categories. The horizontal axis contains the categories labeled as \"Other Math,\" \"Geometric Math,\" \"Charts Diagrams,\" \"Natural Science,\" and \"General Reasoning,\" arranged in alphabetical order. The vertical axis represents the \"Number of Samples\" with a linear scale ranging from 0 to 16,000, marked at intervals of 2,000. Each bar is colored orange and labeled with numerical values on top of them. The \"Other Math\" category has a bar reaching approximately 14,333, the tallest in the chart, while the \"Natural Science\" category's bar is the shortest, reaching about 3,343. Other bars include \"Geometric Math\" at approximately 10,227, \"Charts Diagrams\" at 5,953, and \"General Reasoning\" at 3,534. The bar heights indicate the sample numbers, with no specific sorting beyond alphabetical categorization, and no bars are visually emphasized with additional styling."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.23830v1",
      "paper_title": "EvoMoE Expert Evolution in Mixture of Experts for Multimodal Large   Language Models",
      "figure_filename": "021_2505.23830v1_Figure1.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/021_2505.23830v1_Figure1.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/021_2505.23830v1_EvoMoE Expert Evolution in Mixture of Experts for Multimodal Large   Language Models.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/021_2505.23830v1_EvoMoE Expert Evolution in Mixture of Experts for Multimodal Large   Language Models.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": EvoMoE",
      "gpt_annotation": "The bar chart illustrates the distribution of four experts' contributions across different MOE (Mixture of Experts) layer indices for text and image data. The horizontal axis represents the MOE layer indices labeled from 1 to 23, with evenly spaced, linear scale tick marks appearing at each odd number. The vertical axis denotes the percentage with a linear scale ranging from 0% to 100%, marked with intervals at 0%, 25%, 50%, 75%, and 100%. Each bar comprises four segments representing different experts, with colors assigned as follows: Expert 1 in blue, Expert 2 in orange, Expert 3 in green, and Expert 4 in red. In the text data chart, the height contributions of the experts vary slightly among layers, with certain layers showing one expert more prominently, such as layer 5 and 11 where Expert 4 has a substantial portion. In the image data chart, most layers exhibit a relatively even distribution of expert contributions, with notable exceptions where individual experts dominate certain layers, such as layers 3 and 17. In both charts, the bars are organized such that each expert's contribution is stacked to collectively reach the 100% mark. No specific sorting or emphasis is visually applied to the bars, and the legend atop each chart provides clear identification of each expert's color designation."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.08600v1",
      "paper_title": "Automatic Task Detection and Heterogeneous LLM Speculative Decoding",
      "figure_filename": "022_2505.08600v1_Figure2.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/022_2505.08600v1_Figure2.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/022_2505.08600v1_Automatic Task Detection and Heterogeneous LLM Speculative Decoding.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/022_2505.08600v1_Automatic Task Detection and Heterogeneous LLM Speculative Decoding.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The word \"reaction\" tends to co-occur with different tokens in the domains of chemistry (a) and mathematics (b).",
      "gpt_annotation": "The provided image contains two pie charts labeled (a) and (b). Chart (a) represents a distribution across five categories. The slices are as follows: \"the\" in blue at 31%, \"this\" in solid blue at 21%, \"for\" in green at 17%, \"chemical\" in light green at 15%, and \"to\" in yellow at 9%, with \"in\" in light blue at 7%. A legend at the top specifies the labels and their corresponding colors, and each slice has its percentage labeled inside. The slices in chart (a) do not appear to follow a specific ordering pattern. In chart (b), there are also five categories. The slices include \"the\" in blue at 38%, \"$\" in red at 27%, \"equilibrium\" in yellow at 15%, and \"constant\" in gray at 19%. This chart mirrors the first in including a legend and percentage labels within. The slices in chart (b) are arranged in descending order from largest to smallest, starting with the blue slice. Both charts use distinct colors and patterns for differentiation, but neither chart features any visually emphasized slices."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.08600v1",
      "paper_title": "Automatic Task Detection and Heterogeneous LLM Speculative Decoding",
      "figure_filename": "022_2505.08600v1_Figure5.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/022_2505.08600v1_Figure5.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/022_2505.08600v1_Automatic Task Detection and Heterogeneous LLM Speculative Decoding.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/022_2505.08600v1_Automatic Task Detection and Heterogeneous LLM Speculative Decoding.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The impact of fine-tuning dataset size on the average acceptance length of speculative decoding across text generation, logical reasoning, translation, and question answering tasks.",
      "gpt_annotation": "The plot illustrates the relationship between dataset size and the variable \u03c4 for different tasks. The x-axis represents the dataset size, labeled with evenly spaced tick marks at 1024, 2048, 4096, 8192, and 16384, using a linear scale without specific units. The y-axis represents \u03c4, also on a linear scale, with values ranging from 2.9 to 4.4, marked at intervals of 0.5. The blue line with triangular markers is solid and represents text generation. It starts at approximately \u03c4 = 3.05 for a dataset size of 1024 and shows a slight increase to around \u03c4 = 3.2 at 2048, then decreases to \u03c4 = 2.9 at 16384. The pink line with cross markers is solid and represents translation. It begins at \u03c4 = 3.15 at 1024, peaks at \u03c4 = 4.3 at 8192, and finishes at \u03c4 = 4.05 at 16384. The orange line with asterisk markers, representing logical reasoning, is solid, starting at \u03c4 = 3.9 at 1024, peaking at \u03c4 = 4.0 at 8192, and ending at \u03c4 = 3.85 at 16384. The green line with triangular markers, also solid and representing question answering, begins at \u03c4 = 3.0 for 1024, peaks at \u03c4 = 3.9 at 8192, and declines slightly to \u03c4 = 3.7 at 16384."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.16410v1",
      "paper_title": "Tool-Star Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement   Learning",
      "figure_filename": "026_2505.16410v1_Figure4.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/026_2505.16410v1_Figure4.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/026_2505.16410v1_Tool-Star Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement   Learning.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/026_2505.16410v1_Tool-Star Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement   Learning.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance Comparison of Tool-Star(3B) in Local and Web Search Modes",
      "gpt_annotation": "The bar chart illustrates different search modes and their associated overall performance across four categories. The categories are presented on the horizontal axis, labeled with \"HotpotQA,\" \"2Wiki,\" \"MuSiQue,\" and \"Bamboogle.\" The vertical axis represents \"Overall Performance\" with a linear scale ranging from 0 to 70, marked at intervals of 10 units. Each category contains two bars, corresponding to \"Tool-Star-3B(Local)\" in light blue and \"Tool-Star-3B(Web)\" in dark blue, as indicated by the legend. In the \"HotpotQA\" category, both bars reach approximately 50. For \"2Wiki,\" the local search mode is around 35, while the web search mode is approximately 55. The \"MuSiQue\" category shows both bars around 25. Lastly, in \"Bamboogle,\" the local bar is about 55, and the web bar is around 65. The bars are sorted in a specific order based on the category names, and no bars are visually emphasized beyond the color coding."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.16410v1",
      "paper_title": "Tool-Star Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement   Learning",
      "figure_filename": "026_2505.16410v1_Figure7.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/026_2505.16410v1_Figure7.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/026_2505.16410v1_Tool-Star Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement   Learning.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/026_2505.16410v1_Tool-Star Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement   Learning.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The result of tool-call masking",
      "gpt_annotation": "The plot illustrates the progression of the \"Val Score\" as a function of training \"Step\", comparing two models indicated by different lines. The x-axis represents \"Step\", with a linear scale ranging from 0 to 40, with numeric tick intervals at every 5 units. The y-axis denotes \"Val Score\", also linear, with values ranging from -1.0 to 0.4, displaying tick labels at intervals of 0.2 units. There are two lines depicted: a blue solid line for \"Tool-Star(3B)\", which starts near 0.38 and displays a slight increase, peaking around 0.4 near step 30, and ends just below 0.4 at step 40; an orange solid line for \"Tool-Star(3B) w/o mask\", which starts at approximately 0.35 and sharply decreases to -1.0 by step 5, maintaining this value through step 40."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.13890v1",
      "paper_title": "Mapping the Minds of LLMs A Graph-Based Analysis of Reasoning LLM",
      "figure_filename": "031_2505.13890v1_Figure3.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/031_2505.13890v1_Figure3.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/031_2505.13890v1_Mapping the Minds of LLMs A Graph-Based Analysis of Reasoning LLM.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/031_2505.13890v1_Mapping the Minds of LLMs A Graph-Based Analysis of Reasoning LLM.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Few-shot prompting accuracy on GPQA-Diamond$^*$ dataset using reasoning Qwen-7B (distilled from DeepSeek-R1). The accuracy drops dramatically with respect to the increasing number of examples within the prompt.",
      "gpt_annotation": "The bar chart illustrates the accuracy of three different methods\u2014Minimal, Concise, and Explanatory\u2014across varying numbers of shots. The horizontal axis categorizes the data by the \"Number of Shots,\" labeled 0 through 4 in ascending order. The vertical axis represents \"Accuracy,\" with a linear scale ranging from 0.00 to 0.50, with tick marks at intervals of 0.05. The chart contains grouped bars for each shot number, with each method represented by a different colored bar: purple for Minimal, orange for Concise, and blue for Explanatory. For shot number 0, the Minimal method shows an approximate accuracy of 0.41, Concise about 0.45, and Explanatory around 0.43. At shot number 1, Minimal drops to approximately 0.27, Concise to about 0.38, and Explanatory to around 0.30. For shot number 2, Minimal is roughly 0.31, Concise approximately 0.42, and Explanatory around 0.38. At shot number 3, Minimal stands at about 0.32, Concise at approximately 0.43, and Explanatory at around 0.40. Finally, for shot number 4, Minimal reaches approximately 0.27, with Concise around 0.38 and Explanatory about 0.35. The bars are neither sorted in a specific order nor visually emphasized."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.13890v1",
      "paper_title": "Mapping the Minds of LLMs A Graph-Based Analysis of Reasoning LLM",
      "figure_filename": "031_2505.13890v1_Figure4.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/031_2505.13890v1_Figure4.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/031_2505.13890v1_Mapping the Minds of LLMs A Graph-Based Analysis of Reasoning LLM.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/031_2505.13890v1_Mapping the Minds of LLMs A Graph-Based Analysis of Reasoning LLM.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Average number of tokens under different numbers of shots with  few-shot style. Few-shot prompting leads to significantly fewer reasoning tokens compared with zero-shot prompting.",
      "gpt_annotation": "The bar chart illustrates the token count across different numbers of shots, specifically comparing \"Reasoning\" and \"Demonstration\" components. The horizontal axis represents the categories labeled with the number of shots, ranging from 0 to 4 in sequential order. The vertical axis, labeled \"Token Count,\" uses a linear scale with values ranging from 0 to 7000, marked in intervals of 1000. Each bar consists of two stacked sections: an orange section representing \"Reasoning\" and a blue section representing \"Demonstration,\" as indicated by the legend at the top. The first bar, representing 0 shots, has a total value of 6393, comprising entirely of \"Reasoning\" in orange. The subsequent bars show a decrease in total height with labels indicating the reduction from the initial bar, specifically -1952 for 1 shot, -1613 for 2 shots, -1040 for 3 shots, and -1379 for 4 shots. Each of these bars includes a smaller blue segment for \"Demonstration,\" visibly stacked above the orange \"Reasoning\" segment. The bars are sorted sequentially by the number of shots and do not feature any particular visual emphasis beyond the color differentiation provided by the stacking."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.13890v1",
      "paper_title": "Mapping the Minds of LLMs A Graph-Based Analysis of Reasoning LLM",
      "figure_filename": "031_2505.13890v1_Figure6.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/031_2505.13890v1_Figure6.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/031_2505.13890v1_Mapping the Minds of LLMs A Graph-Based Analysis of Reasoning LLM.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/031_2505.13890v1_Mapping the Minds of LLMs A Graph-Based Analysis of Reasoning LLM.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": Exploration Density $\\rho_E$",
      "gpt_annotation": "The bar chart illustrates the relationship between the number of shots and the values of \u03c1_E, differentiating between \"Incorrect\" and \"Correct\" outcomes. The horizontal axis represents the categories labeled as \"Number of Shots,\" ordered sequentially from 0 to 4. The vertical axis is labeled as \u03c1_E and employs a linear scale ranging from 0.00 to 0.10, with tick marks at intervals of 0.05. Each category has two bars: one labeled \"Incorrect\" in blue with a dotted pattern, and the other labeled \"Correct\" in red with diagonal stripes. For each category from 0 to 4, the \"Incorrect\" bars range from approximately 0.02 to 0.05, while the \"Correct\" bars all reach about 0.10. The bars are grouped for each number of shots, and none of the bars are distinctly emphasized beyond their patterns and colors. The chart does not appear to sort bars by value but rather by category presence."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.13890v1",
      "paper_title": "Mapping the Minds of LLMs A Graph-Based Analysis of Reasoning LLM",
      "figure_filename": "031_2505.13890v1_Figure7.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/031_2505.13890v1_Figure7.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/031_2505.13890v1_Mapping the Minds of LLMs A Graph-Based Analysis of Reasoning LLM.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/031_2505.13890v1_Mapping the Minds of LLMs A Graph-Based Analysis of Reasoning LLM.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": Linearity $\\ell$",
      "gpt_annotation": "The bar chart illustrates the linearity of two types of results, \"Incorrect\" and \"Correct,\" categorized by the \"Number of Shots\" ranging from 0 to 4. The horizontal axis represents the categories labeled \"Number of Shots\" with a linear scale, while the vertical axis shows \"Linearity,\" ranging from 0.00 to 1.00, marked at intervals of 0.25. The chart features grouped bars for each category of \"Number of Shots,\" consisting of two bars per group. The \"Incorrect\" category is represented by bars colored in a blue pattern, and the \"Correct\" category by bars with a maroon pattern. For the \"Incorrect\" category, approximate values are consistent and nearly reach the top of the scale around 0.95. In contrast, the \"Correct\" category bars are relatively shorter, measuring around 0.72 across each group. The bars are arranged in ascending order from 0 to 4, with no additional visual emphasis or annotations on any specific bars."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.13890v1",
      "paper_title": "Mapping the Minds of LLMs A Graph-Based Analysis of Reasoning LLM",
      "figure_filename": "031_2505.13890v1_Figure8.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/031_2505.13890v1_Figure8.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/031_2505.13890v1_Mapping the Minds of LLMs A Graph-Based Analysis of Reasoning LLM.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/031_2505.13890v1_Mapping the Minds of LLMs A Graph-Based Analysis of Reasoning LLM.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": Branching Ratio $\\gamma_B$",
      "gpt_annotation": "The bar chart illustrates the distribution of values labeled as \"Correct\" and \"Incorrect\" across different \"Number of Shots.\" The horizontal axis contains the categories, labeled as \"Number of Shots,\" and includes values from 0 to 4. The vertical axis is labeled \\(\\gamma_B\\), with a linear scale ranging from 0.0 to 0.8 with intervals marked at 0.2. There are grouped bars for each category, with \"Incorrect\" bars represented in blue with a dotted pattern, and \"Correct\" bars in red with a diagonal stripe pattern. For each \"Number of Shots,\" both bars are displayed side by side, illustrating their respective values. At 0 shots, \"Incorrect\" has a value of approximately 0.4, while \"Correct\" is about 0.7. At 1 shot, \"Incorrect\" is around 0.3, and \"Correct\" is approximately 0.7. For 2 shots, \"Incorrect\" is near 0.3, and \"Correct\" reaches about 0.7. At 3 shots, the \"Incorrect\" value is approximately 0.3, and \"Correct\" is about 0.6. Lastly, at 4 shots, \"Incorrect\" is close to 0.3, and \"Correct\" reaches approximately 0.5. The bars are not sorted in any specific order, and no bars are visually emphasized beyond the standard color and pattern distinction."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.13890v1",
      "paper_title": "Mapping the Minds of LLMs A Graph-Based Analysis of Reasoning LLM",
      "figure_filename": "031_2505.13890v1_Figure9.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/031_2505.13890v1_Figure9.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/031_2505.13890v1_Mapping the Minds of LLMs A Graph-Based Analysis of Reasoning LLM.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/031_2505.13890v1_Mapping the Minds of LLMs A Graph-Based Analysis of Reasoning LLM.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": Convergence Ratio $\\gamma_C$",
      "gpt_annotation": "The bar chart illustrates the relationship between the number of shots and the corresponding values of \\(\\gamma_C\\) in two categories: \"Correct\" and \"Incorrect.\" The horizontal axis represents the \"Number of Shots,\" labeled with integers ranging from 0 to 4. The vertical axis, labeled \\(\\gamma_C\\), uses a linear scale ranging from 0.0 to 0.7, with tick marks at intervals of 0.1. The chart consists of grouped bars for each category of shots, where each group contains two bars with different patterns to distinguish between \"Incorrect\" and \"Correct\" outcomes. The \"Incorrect\" bars are shaded blue with a dot pattern, and the values range approximately from 0.05 to 0.25. The \"Correct\" bars are shaded red with diagonal lines and have values approximately between 0.3 and 0.65. For each group, the \"Correct\" bars are consistently taller than the \"Incorrect\" bars, except for shot number 4, where the difference is narrower. The bars are arranged in numerical order according to the shot numbers without any apparent emphasis on specific bars."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.02596v1",
      "paper_title": "EssayBench Evaluating Large Language Models in Multi-Genre Chinese   Essay Writing",
      "figure_filename": "032_2506.02596v1_Figure3.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/032_2506.02596v1_Figure3.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/032_2506.02596v1_EssayBench Evaluating Large Language Models in Multi-Genre Chinese   Essay Writing.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/032_2506.02596v1_EssayBench Evaluating Large Language Models in Multi-Genre Chinese   Essay Writing.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Dataset Statistics. Note that Open denotes Open-Ended sets, Cons. refers to Constrained sets.",
      "gpt_annotation": "The bar chart illustrates the sample count for different text types across two categories: \"Open\" and \"Cons.\" The horizontal axis represents the categories of text types, labeled as \"Argumentative,\" \"Narrative,\" \"Descriptive,\" and \"Expository.\" The vertical axis represents the sample count, labeled \"Sample Count,\" with a linear scale ranging from 0 to 140, marked at intervals of 20. Each text type category contains a pair of bars: one blue representing \"Open\" and one pink representing \"Cons.\" In the \"Argumentative\" category, the blue bar for \"Open\" has a value of approximately 79, while the pink bar for \"Cons.\" reaches about 122. The \"Narrative\" category shows the blue bar at approximately 80 and the pink bar at around 87. For \"Descriptive\" texts, the blue bar measures about 68, and the pink bar is marked at 100. In the \"Expository\" category, the blue bar has a value of about 62, contrasted by the pink bar at 130. The bars are grouped by text type and do not appear to be sorted in a specific order. No bars are visually emphasized beyond their color coding for \"Open\" and \"Cons.\" noted in the chart's legend."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.23108v1",
      "paper_title": "Generating Diverse Training Samples for Relation Extraction with Large   Language Models",
      "figure_filename": "033_2505.23108v1_Figure5.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/033_2505.23108v1_Figure5.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/033_2505.23108v1_Generating Diverse Training Samples for Relation Extraction with Large   Language Models.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/033_2505.23108v1_Generating Diverse Training Samples for Relation Extraction with Large   Language Models.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Average cosine similarity between generated training samples (K=32) for each relation category.",
      "gpt_annotation": "The plot illustrates the mean interclass similarity across different relation categories. The x-axis is labeled \"Relation Category\" and has a linear scale, ranging from approximately 0 to 70 with tick marks at intervals of 10. The y-axis is labeled \"Mean Interclass Similarity\" and has a linear scale, ranging from 0.0 to 0.5 with tick marks at intervals of 0.1. The plot contains three primary lines. The first line is blue with circle markers and a solid style, representing \"w/o DPO obo.\" It starts at approximately 0.0, peaking around 0.5 at the 5th category, and fluctuates across the axis, ending near 0.1. The second line is orange with triangle markers and a solid style, representing \"w/o DPO obo & w/o div-hint.\" It starts just above 0.1, peaks multiple times with the highest point around 0.5 near the 38th category, and ends slightly below 0.2. The third line is green with square markers and a solid style, showing \"DPO.\" This line begins near 0.1 and maintains a relatively stable course, with minor fluctuations, ending just above 0.0. Additionally, there are three horizontal dashed lines representing average values: one orange at 0.158, one blue at 0.107, and one green at 0.086."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.23108v1",
      "paper_title": "Generating Diverse Training Samples for Relation Extraction with Large   Language Models",
      "figure_filename": "033_2505.23108v1_Figure6.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/033_2505.23108v1_Figure6.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/033_2505.23108v1_Generating Diverse Training Samples for Relation Extraction with Large   Language Models.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/033_2505.23108v1_Generating Diverse Training Samples for Relation Extraction with Large   Language Models.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Average repetition rate of words between generated training samples (K=32) for each relation category.",
      "gpt_annotation": "The plot illustrates the average repetition rate across different relation categories, comparing three scenarios. The x-axis represents the relation categories, labeled as \"Relation Category,\" with a linear scale ranging from 0 to roughly 65, with tick labels at intervals of 5. The y-axis is labeled \"Average Repetition Rate,\" also on a linear scale, ranging from 0.0 to 0.6 with tick labels at intervals of 0.1. The blue line with circular markers and a solid style represents \"w/o DPO obo,\" beginning at approximately 0.1, peaking around 0.5 near category 5, and ending around 0.1. The orange line with triangular markers and a solid style denotes \"w/o DPO obo & w/o div-hint,\" also starting near 0.1, peaking at about 0.4 near category 6, and finishing around 0.2. The green line, featuring square markers and a solid style, signifies \"DPO,\" starting around 0.1, fluctuating slightly, and ending near 0.1. Each line exhibits variability with several peaks and troughs throughout the range of categories. Horizontal dashed lines indicate average values: 0.227 for the orange line, 0.163 for the blue line, and 0.117 for the green line."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.23108v1",
      "paper_title": "Generating Diverse Training Samples for Relation Extraction with Large   Language Models",
      "figure_filename": "033_2505.23108v1_Figure7.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/033_2505.23108v1_Figure7.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/033_2505.23108v1_Generating Diverse Training Samples for Relation Extraction with Large   Language Models.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/033_2505.23108v1_Generating Diverse Training Samples for Relation Extraction with Large   Language Models.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Micro F1 (%) of KnowPrompt using different numbers of training samples generated by LLMs.",
      "gpt_annotation": "The plot illustrates the performance of different datasets as measured by the Micro F1 score against the number of samples generated. The x-axis is labeled \"Number of Samples Generated,\" with values of K=8, K=16, K=32, and K=64 marked at regular intervals, and it follows a linear scale without specified units. The y-axis is labeled \"Micro F1 (%)\" and also uses a linear scale, ranging from 20 to 50, with tick intervals of 5 percent. There are four lines in the plot: the blue line with circular markers represents TACRED, following a solid line style. It begins at approximately 25% at K=8 and slightly decreases to around 27% by K=64, showing a minor downward trend after K=32. The orange line with triangular markers represents TACRED-Revisit, also solid; it starts just above the blue line at about 26% and ends at approximately 28%, showing a similar pattern with a peak at K=32. The green line with diamond markers represents Re-TACRED, increasing from around 35% at K=8 to about 44% at K=32 before slightly declining to around 41% at K=64. Lastly, the red line with star-shaped markers represents SemEval, steadily rising from about 42% at K=8 to approximately 48% at K=64, maintaining a consistent upward trend."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.19756v1",
      "paper_title": "Efficient Reasoning via Chain of Unconscious Thought",
      "figure_filename": "034_2505.19756v1_Figure1.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/034_2505.19756v1_Figure1.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/034_2505.19756v1_Efficient Reasoning via Chain of Unconscious Thought.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/034_2505.19756v1_Efficient Reasoning via Chain of Unconscious Thought.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "width=0.95: Average Performance and Tokens of CoUT and Baselines for 4 LRMs over 4 Benchmarks.",
      "gpt_annotation": "The chart provides a visual representation of the accuracy percentages and token counts for four different categories: CoT, CoD, CCoT, and CoUT. The horizontal axis contains the categories labeled as CoT, CoD, CCoT, and CoUT, while the left vertical axis represents accuracy in percentages with a scale from 70 to 100, featuring labels at regular intervals of 5%. The right vertical axis represents token count with a scale from 0 to 800, also labeled at intervals of 100. The bar labeled CoT in red shows an accuracy of 91.19% and a token count of approximately 676.85. The orange bar for CoD has an accuracy of 80.89% and a token count of approximately 534.24. The light blue bar for CCoT displays an accuracy of 88.50% and a token count of about 445.91. The green bar representing CoUT demonstrates an accuracy of 88.40% and a token count of approximately 354.46. Each category has two bars of the same color: the left bar indicates accuracy, and the right bar indicates token count. The bars are not sorted in any specific order and maintain an equal width across each category without any visual emphasis beyond color differentiation."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure2.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure2.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of ChatGPT under different question popularity.",
      "gpt_annotation": "The plot illustrates the change in accuracy, confidence, and alignment values of ChatGPT on movies as related to question popularity. The x-axis, labeled \"Question Popularity,\" has a linear scale with values ranging from 15 to 55, spaced at intervals of 5. The y-axis, labeled \"Values,\" also uses a linear scale ranging from 0.86 to 1.00, with tick labels at 0.02 intervals. The blue line, representing accuracy, has circular markers and a solid style. It begins at approximately 0.90 for a popularity of 15, peaks at about 0.98 near 30, then trends slightly upward, reaching around 0.98 at 55. The green line, representing confidence, uses triangular markers and a solid style, starting at about 0.98 at a popularity of 15 and maintaining a slight upward trend, reaching slightly above 0.99 by 55. The red line, representing alignment, has square markers and a solid style. It starts at approximately 0.92 at a popularity of 15, experiences minor fluctuations, and generally trends upward, reaching just below 0.98 at 55."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure3.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure3.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of ChatGPT under different question popularity.",
      "gpt_annotation": "The plot illustrates how three different metrics\u2014Accuracy, Confidence, and Alignment\u2014change with Question Popularity. The x-axis is labeled \"Question Popularity\" and uses a linear scale ranging from 5 to 25 with tick labels at intervals of 5. The y-axis is labeled \"Values,\" also on a linear scale, ranging from 0.55 to 1.00 with tick labels at intervals of 0.05. The blue line, representing Accuracy, has circle markers and a solid line style. It starts around 0.65 at an x-value of 5, dips slightly, and then generally increases, ending near 0.90 at an x-value of 25, with a noticeable rise after x=15. The green line, indicating Confidence, uses triangle markers and a solid line style, starting above 0.90 at x=5 and maintaining a steady, slightly increasing trend, peaking slightly above 0.95 at x=25. The red line, representing Alignment, is depicted with square markers and a solid line style, starting slightly above 0.70 at x=5, experiencing fluctuations between 0.70 and 0.85, and finishing at around 0.90 at x=25, showing an upward trend toward the end."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure4.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure4.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of ChatGPT under different question popularity.",
      "gpt_annotation": "The plot illustrates the relationship between question popularity and three different values being tracked: accuracy, confidence, and alignment. The x-axis is labeled \"Question Popularity,\" with a linear scale ranging from 5.0 to 25.0, showing regular interval ticks at every 2.5 units. The y-axis is labeled \"Values,\" also with a linear scale, ranging from 0.2 to 1.0, with ticks at intervals of 0.1 units. Three lines are depicted: a blue line with circular markers representing \"Accuracy\" starts at approximately 0.25 at a question popularity of 5.0 and ends near 0.7 at 25.0, showing a steady upward trend. A green line with triangular markers for \"Confidence\" begins at around 0.7 at 5.0 and increases to about 0.9 at 25.0, also depicting an upward trend. The red line with square markers denoting \"Alignment\" starts at about 0.5 and finishes just above 0.6, exhibiting a consistent upward progression. Each line steadily rises without any apparent peaks or declines."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure5.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure5.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Comparison of the correlation between ChatGPT's QA performance and ground-truth vs. generated answers: $\\text{Pop}_\\text{GT}$ vs. $\\text{Pop}_\\text{Ge}$, and $\\text{RPop}_\\text{GT}$ vs. $\\text{RPop}_\\text{Ge}$.",
      "gpt_annotation": "The bar chart illustrates the correlation values between four different categories labeled as Pop_GT, Pop_GE, RPop_GT, and RPop_GE across three domains: Movies, Songs, and Basketball. The horizontal axis represents the categories Movies, Songs, and Basketball, while the vertical axis represents the correlation with a linear scale ranging from 0 to 0.7, marked at intervals of 0.1. Each domain has four grouped bars, each with a different color and pattern: Pop_GT in solid orange, Pop_GE in orange with crosshatch lines, RPop_GT in solid blue, and RPop_GE in blue with crosshatch lines. In the Movies category, the bars indicate correlation values of approximately 0.2 for Pop_GT, 0.1 for Pop_GE, 0.6 for RPop_GT, and 0.4 for RPop_GE. In the Songs category, the values are about 0.5 for Pop_GT, 0.2 for Pop_GE, 0.6 for RPop_GT, and 0.7 for RPop_GE. For Basketball, the values are approximately 0.1 for Pop_GT, 0.2 for Pop_GE, 0.25 for RPop_GT, and 0.2 for RPop_GE. The bars are not sorted in any particular order, and there is no visible emphasis applied to any specific bar."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure6.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure6.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "$\\text{Pop}_\\text{GT}$, $\\text{Pop}_\\text{Ge}$ in incorrectly answered samples and $\\text{Pop}_\\text{GT}$ (also $\\text{Pop}_\\text{Ge}$) in correctly answered samples.",
      "gpt_annotation": "The bar chart illustrates the popularity of three categories\u2014Movies, Songs, and Basketball\u2014based on different AI models represented with varying colors and patterns. The horizontal axis contains the categories labeled as Movies, Songs, and Basketball, while the vertical axis represents the popularity, with a linear scale ranging from 20 to 220 in increments of 20. Each category includes three stacked and patterned bars that represent different conditions: \"Acc=0 & GT Ans,\" \"Acc=0 & Gene Ans,\" and \"Acc=1 & GT Ans.\" The AI models are color-coded with Llama3 in blue, Qwen2 in red, and ChatGPT in yellow. For Movies, each model's total bar height is approximately 60 to 100, with ChatGPT having the shortest total bar. In Songs, the heights range around 80 to 140, while in Basketball, all models show a marked increase with heights between approximately 180 to 220, with Qwen2 reaching the highest point at around 220. The bars are not sorted in any specific order beyond their categorical grouping, and there are no additional visual emphases such as bolding or shading on specific bars."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure7.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure7.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Proportion of incorrectly answered samples where $\\text{RPop}_\\text{Ge}$ is less than $\\text{RPop}_\\text{GT}$.",
      "gpt_annotation": "The bar chart illustrates the ratio percentages of three different models\u2014Llama3, Qwen2, and ChatGPT\u2014across three categories: Movies, Songs, and Basketball. The horizontal axis contains the categories labeled as Movies, Songs, and Basketball, without any specific sorting order. The vertical axis represents the ratio in percentage, ranging from 50% to 90%, with a linear scale and tick intervals of 10%. For the Movies category, Llama3 is represented by a blue bar with an approximate value of 90%, Qwen2 by a red bar also at 90%, and ChatGPT by a yellow bar around 70%. In the Songs category, Llama3's blue bar is approximately 80%, Qwen2's red bar reaches 90%, and ChatGPT's yellow bar is near 70%. In the Basketball category, the blue bar for Llama3 is approximately 70%, the red bar for Qwen2 is around 60%, and the yellow bar for ChatGPT is roughly 70%. Each category includes three bars distinctly colored for each model as per the legend, with no specific sorting or visual emphasis given to any bar."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure9.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure9.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The prediction accuracy obtained by performing confidence calibration using knowledge popularity generated from different numbers of examples. Each point represents the average prediction accuracy of the model across three datasets.",
      "gpt_annotation": "The line plot illustrates the prediction accuracy of three different models, denoted as LLaMA 8B, Qwen2, and ChatGPT, across various numbers of shots. The x-axis is labeled \"Shots\" and uses a linear scale, ranging from 0 to 10, with tick labels at regular intervals of 0, 3, 5, and 10. The y-axis is labeled \"Prediction Accuracy (%)\", also on a linear scale, spanning from 77 to 85, with tick labels at intervals of 1%. The blue line, representing LLaMA 8B, is marked with circular markers and a solid line style. It starts at approximately 78% accuracy at 0 shots, increases slightly reaching around 78.5% by 3 shots, and maintains a similar level close to 78.5% through to 10 shots. The orange line, representing Qwen2, features square markers and a solid line style. It begins at approximately 78.5% accuracy at 0 shots, remains steady to 3 shots, and then shows a slight decline, finishing just under 78% at 10 shots. The green line, representing ChatGPT, is marked with triangular markers and a solid line style. It starts at 85% accuracy at 0 shots, decreases slightly to just above 84% by 3 shots, and then remains consistent through to 10 shots."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure10.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure10.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The difference in answer correctness prediction on LLaMA3 between using PC+ALL and using PC. Blue indicates that both methods make the same prediction, yellow indicates cases where only PC+ALL predictes correctly, and red indicates cases where only PC predictes correctly.",
      "gpt_annotation": "The bar chart illustrates the distribution of outcomes categorized as \"Same,\" \"Win,\" and \"Fail\" across three different activities: Basketball, Songs, and Movies. The categories are listed on the vertical axis, with Basketball, Songs, and Movies ordered from top to bottom. The horizontal axis represents the values of each category, without a specified label, but with values indicated numerically within each segmented bar, suggesting a linear scale. Each bar is divided into segments representing each outcome: \"Same\" is shown in blue, \"Win\" in yellow, and \"Fail\" in red. For Basketball, the segments indicate values of approximately 74 for \"Same,\" 16 for \"Win,\" and 10 for \"Fail.\" For Songs, the respective values are approximately 80 for \"Same,\" 14 for \"Win,\" and 6 for \"Fail.\" The Movies category contains values of approximately 86 for \"Same,\" 12 for \"Win,\" and 2 for \"Fail.\" The bars are not sorted alphabetically or numerically, and no specific bar is visually emphasized beyond the use of color coding to differentiate each outcome category."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure14.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure14.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of Llama3 under different question popularity.",
      "gpt_annotation": "This plot illustrates three metrics related to Llama3-8B's performance on movie questions: Accuracy, Confidence, and Alignment. The x-axis represents \"Question Popularity\" on a linear scale, ranging from 15 to 55, with tick labels at intervals of 5. The y-axis indicates \"Values\" also on a linear scale, spanning from 0 to 1 with tick labels at intervals of 0.2. The blue line with circle markers and a solid style represents Accuracy, starting at approximately 0.6 at the x-value of 15 and rising steadily to around 0.95 at 55. The green line with triangle markers and a dashed style depicts Confidence, beginning near 0.7 at the start of the x-axis and steadily increasing to slightly above 0.95. Lastly, the red line with square markers and a solid style shows Alignment, starting around 0.7 and gradually rising to about 0.97 at the end of the x-axis range."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure15.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure15.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of Llama3 under different question popularity.",
      "gpt_annotation": "The plot illustrates how three different metrics\u2014Accuracy, Confidence, and Alignment\u2014vary with Question Popularity for the Llama3-8B on Songs dataset. The x-axis represents Question Popularity with a linear scale ranging from 5 to 25, labeled in increments of 5. The y-axis shows Values also on a linear scale, ranging from 0.0 to 1.0, in increments of 0.2. The blue line with circle markers, representing Accuracy, starts at approximately 0.2 and ends at around 0.5, showing an upward trend with fluctuations around Question Popularity values of 10 and 15. The green line with triangle markers, indicating Confidence, begins at about 0.6 and ends near 0.9, displaying a generally increasing trend with a slight drop around a Question Popularity of 15. The red line with square markers, representing Alignment, starts close to 0.55 and ends slightly below 0.65, showing a slight rise with a peak near a Question Popularity of 10 and a gradual decline toward the end."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure16.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure16.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of Llama3 under different question popularity.",
      "gpt_annotation": "The plot illustrates the relationship between question popularity and three different performance metrics: accuracy, confidence, and alignment. The x-axis is labeled \"Question Popularity\" and is presented on a linear scale, ranging from 5.0 to 25.0 with tick intervals of 2.5. The y-axis, labeled \"Values,\" also uses a linear scale ranging from 0.0 to 1.0 with tick intervals of 0.2. The blue line represents accuracy, with circular markers and a solid line style. It starts at approximately 0.1 at a question popularity of 5.0 and increases steadily to about 0.2 at 25.0. The green line represents confidence, with triangular markers and a solid line style. This line begins at roughly 0.55 and increases to around 0.7 over the same range. The red line signifies alignment, using square markers and a solid line style, beginning at approximately 0.5 and gradually increasing to about 0.6. All lines display a generally upward trend with varying rates of increase."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure17.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure17.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of Qwen2 under different question popularity.",
      "gpt_annotation": "The plot illustrates the performance metrics of \"Qwen2-7B on Movies\" by showing the relationship between question popularity and three different values. The x-axis represents \"Question Popularity\" with a linear scale ranging from 15 to 55, marked at intervals of 5. The y-axis shows \"Values,\" also on a linear scale, ranging from 0.0 to 1.0 marked at 0.2 intervals. There are three lines presented: a blue line with circular markers, a green line with triangular markers, and a red line with square markers. The blue line represents \"Accuracy\" and starts at approximately 0.2 for a question popularity of 15 and ends at about 0.8 for a popularity of 55, showing a steady increase. The green line, indicating \"Confidence,\" starts around 0.8 and slightly rises to about 0.9 across the same range, maintaining higher values throughout. The red line, representing \"Alignment,\" begins at approximately 0.4 and increases to about 0.8, depicting a moderate upward trend similar to that of \"Accuracy.\""
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure18.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure18.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of Qwen2 under different question popularity.",
      "gpt_annotation": "The plot illustrates the relationship between different metrics and question popularity for the Qwen2-7B model on songs. The x-axis is labeled \"Question Popularity,\" with a linear scale ranging from 5 to 25, featuring regularly spaced tick marks at intervals of 5. The y-axis is labeled \"Values,\" also on a linear scale, ranging from 0.0 to 1.0, with tick marks at intervals of 0.2. There are three lines in the plot. The first line, representing \"Accuracy,\" is blue with circle markers and a solid line style. It starts at approximately 0.15 for a question popularity of 5 and increases steadily to about 0.55 by a popularity of 25. The second line, representing \"Confidence,\" is green with triangle markers and a solid line style. It starts roughly at 0.75 and shows slight increases, reaching around 0.85 by the end of the popularity range. The third line, labeled \"Alignment,\" is red with square markers and a solid line style, beginning at about 0.4 and increasing steadily to approximately 0.65 at the end of the x-axis."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure19.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure19.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of Qwen2 under different question popularity.",
      "gpt_annotation": "The plot illustrates the relationship between different metrics (Accuracy, Confidence, and Alignment) and Question Popularity for a model named Qwen2-7B on the subject of Basketball. The x-axis is labeled \"Question Popularity,\" uses a linear scale, and ranges from 5.0 to 25.0 with tick labels at intervals of 2.5. The y-axis is labeled \"Values,\" also utilizes a linear scale, and ranges from 0.0 to 1.0 with tick labels at intervals of 0.2. The plot includes three lines representing different metrics. The blue line with circular markers, representing Accuracy, is solid and shows a slight upward trend starting at approximately 0.15 and ending around 0.17. The green line with triangular markers, representing Confidence, is solid and shows an upward trend beginning at about 0.75 and ending at approximately 0.85. The red line with square markers, representing Alignment, is solid and demonstrates a slight downward trend, starting at approximately 0.45 and ending around 0.4."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure20.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure20.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of Llama3 under different answer popularity.",
      "gpt_annotation": "The plot illustrates the relationship between question popularity and various performance metrics of a model labeled Llama3-8B on movies. The x-axis is labeled \"Question Popularity,\" has a linear scale ranging from 0 to 100, without stated units, and tick marks appear at intervals of 20. The y-axis is labeled \"Values\" and it also employs a linear scale, ranging from 0.0 to 1.0, with tick marks at intervals of 0.2. Three lines are represented in the plot. The blue line, using circle markers and a solid style, represents \"Accuracy\" and shows an increasing trend starting at approximately 0.6 and reaching near 0.9. The green line, with triangle markers and a solid style, represents \"Confidence,\" starting around 0.8 and increasing slightly to nearly 1.0. Lastly, the red line, with square markers and a solid style, denotes \"Alignment,\" starting just under 0.8 and steadily rising to approach 0.9. Each line describes different aspects of performance as a function of question popularity."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure21.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure21.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of Llama3 under different answer popularity.",
      "gpt_annotation": "The plot illustrates the relationship between question popularity and three variables: accuracy, confidence, and alignment for Llama3-8B on songs. The x-axis is labeled \"Question Popularity,\" with a linear scale ranging from 0 to 160, and tick marks are placed at intervals of 20. The y-axis is labeled \"Values,\" also with a linear scale ranging from 0 to 1.0, and tick marks appear at intervals of 0.2. The plot has three lines representing different metrics. The blue line, with circular markers and a solid style, represents accuracy, starting around 0.2 at a question popularity of 0 and ending around 0.5 at 160, with a notable peak at approximately 0.6 around 100. The green line, with triangular markers and a solid style, represents confidence, beginning near 0.8 and ending slightly above 0.8, showing minor fluctuations but maintaining a generally steady trend. The red line, with square markers and a solid style, depicts alignment, starting near 0.5 and finishing around 0.6, with a noticeable peak near 0.7 around a question popularity of 80."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure22.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure22.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of Llama3 under different answer popularity.",
      "gpt_annotation": "This plot illustrates the performance metrics of Llama3-8B on questions related to basketball, specifically focusing on accuracy, confidence, and alignment as a function of question popularity. The x-axis is labeled \"Question Popularity,\" with a linear scale ranging from 0 to 300, marked at intervals of 50 units. The y-axis is labeled \"Values,\" also utilizing a linear scale, ranging from 0.0 to 1.0 with intervals of 0.2. The plot includes three lines: The blue line, representing accuracy, is marked with circles and follows a solid line style. It starts near 0.1 at the beginning and rises to a peak around 0.3 at about 200, then declines slightly towards the end. The green line, indicating confidence, is marked with triangles and also follows a solid line style. It starts at approximately 0.6 and maintains a relatively steady trend with slight variations, peaking just above 0.6 around the same area as the peak of the accuracy line. Lastly, the red line represents alignment, marked with squares and a solid line style, beginning close to 0.4, rising steadily to a peak above 0.7 around 200, before a slight decline towards the end."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure23.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure23.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of Qwen2 under different answer popularity.",
      "gpt_annotation": "The plot illustrates the relationship between question popularity and various measured values related to a model named Qwen2-7B on a movies dataset. The x-axis is labeled \"Question Popularity\" and uses a linear scale with values ranging from 0 to 100, with tick marks at intervals of 20. The y-axis is labeled \"Values,\" also on a linear scale, ranging from 0.0 to 1.0, with ticks at intervals of 0.2. There are three lines on the plot. The blue line, marked by circles and represented with a solid style, illustrates \"Accuracy,\" starting at a value below 0.2 at x=0 and rising to approximately 0.6 at x=100, showing an upward trend throughout. The green line, denoted by triangles and a solid style, represents \"Confidence,\" beginning around 0.75 at x=0 and increasing slightly to about 0.9 at x=100, maintaining a generally steady upward slope. The red line, shown with squares and a solid style, indicates \"Alignment\" and starts at about 0.55 at x=0, fluctuating slightly but trending upward to approximately 0.75 at x=100. The plot is titled \"Qwen2-7B on Movies,\" and there is a legend differentiating the lines by their respective attributes."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure24.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure24.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of Qwen2 under different answer popularity.",
      "gpt_annotation": "The plot illustrates the relationship between different metrics and question popularity for the Qwen2-7B model on songs. The x-axis is labeled \"Question Popularity,\" features a linear scale, and has a value range from 0 to 160 with tick marks at intervals of 20. The y-axis is labeled \"Values,\" also uses a linear scale, and ranges from 0.0 to 1.0 with tick marks at intervals of 0.2. There are three lines on the plot. The blue line with circle markers represents \"Accuracy\" and follows a solid line style, starting near 0.1 at a question popularity of 0 and increasing gradually, ending near 0.6 at a popularity of 160. The highest point is slightly before a popularity of 120. The green line with triangle markers represents \"Confidence\" and follows a solid line style, beginning near 0.8 at the start and remaining relatively stable with minor fluctuations, ending just below 0.8. The red line with square markers indicates \"Alignment\" and uses a solid line style, starting around 0.4, peaking at over 0.6 before 80 and slightly declining before rising again toward 0.6 at the end."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure25.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure25.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of Qwen2 under different answer popularity.",
      "gpt_annotation": "The plot illustrates the relationship between question popularity and three metrics: accuracy, confidence, and alignment, for a model referred to as Qwen2-7B on the topic of basketball. The x-axis represents \"Question Popularity,\" with a linear scale ranging from 0 to 300, with tick labels at intervals of 50. The y-axis is labeled \"Values,\" also using a linear scale, ranging from 0 to 1, with tick labels at intervals of 0.2. There are three lines in the plot: a blue line with circle markers representing accuracy, a green line with triangle markers representing confidence, and a red line with square markers representing alignment. The blue accuracy line starts around 0.1 at an x-value of 0, increases up to approximately 0.55 near x-value 200, and decreases slightly towards the end. The green confidence line begins around 0.75, stays relatively stable with minor fluctuations, and ends slightly below its starting value. The red alignment line starts at about 0.3, increases steadily, peaks slightly above 0.5 near x-value 200, and then levels off towards the end."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure26.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure26.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of ChatGPT under different answer popularity.",
      "gpt_annotation": "The plot illustrates the relationship between question popularity and three metrics: accuracy, confidence, and alignment for ChatGPT on movie-related queries. The x-axis is labeled \"Question Popularity\" and is set on a linear scale ranging from 0 to 100, with tick labels at intervals of 20. The y-axis is labeled \"Values,\" also on a linear scale, ranging from 0.0 to 1.0, with tick labels at intervals of 0.2. The blue line with circular markers represents accuracy, which starts at approximately 0.95 at the lower end and slightly increases to a value near 1.0 towards the higher end. The green line with triangular markers denotes confidence, beginning around 0.98 and maintaining a consistent level close or equal to 1.0 throughout. The red line, represented with square markers, indicates alignment, which starts at about 0.98 and also remains steady near 1.0 across the x-axis range. All lines exhibit a solid style and show slight increases or stability as the question popularity grows."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure27.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure27.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of ChatGPT under different answer popularity.",
      "gpt_annotation": "The line plot illustrates the relationship between question popularity and three performance metrics labeled as Accuracy, Confidence, and Alignment for a subject titled \"ChatGPT on Songs.\" The x-axis is labeled \"Question Popularity,\" displayed on a linear scale with a range from 0 to 160 and tick labels at intervals of 20. The y-axis is labeled \"Values,\" with a linear scale ranging from 0.0 to 1.0, featuring tick labels at intervals of 0.2. The blue line, representing Accuracy, begins at approximately 0.6 at x=20, initially declines to around 0.52 at x=40, then increases and maintains between 0.75 and 0.84 from x=60 to x=160, using circular markers and a solid line style. The green line, indicating Confidence, uses triangular markers and a solid line, beginning at about 0.84 at x=20 and consistently remains between 0.9 and 1.0 across the x-axis range. The red line, representing Alignment, starts at approximately 0.62 at x=20, rises sharply to around 0.82 at x=60, and stabilizes close to 0.85 from x=80 to x=160, marked by square markers and a solid line style."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure28.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure28.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of ChatGPT under different answer popularity.",
      "gpt_annotation": "The plot illustrates the relationship between question popularity and three measured metrics: accuracy, confidence, and alignment. The x-axis is labeled \"Question Popularity,\" uses a linear scale, ranges from 0 to 300, and has tick labels at intervals of 50. The y-axis is labeled \"Values,\" also uses a linear scale, ranges from 0.0 to 1.0, with tick labels at intervals of 0.2. There are three lines: one blue line with circular markers, representing accuracy, which starts at approximately 0.25, dips to about 0.15, and then rises to around 0.45 by the end of the axis. A green line with triangular markers, representing confidence, begins at roughly 0.75, remains relatively steady above 0.75, and ends close to 0.8. The red line with square markers, representing alignment, starts at about 0.6, slightly decreases to approximately 0.55, and subsequently increases to around 0.7 towards the end of the axis."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure29.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure29.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of Llama3 under different relation popularity.",
      "gpt_annotation": "The plot illustrates the relationship between answer popularity and three different metrics: accuracy, confidence, and alignment for \"Llama3-8B on Movies.\" The x-axis is labeled \"Answer Popularity,\" with a linear scale ranging from 0 to 120, and tick labels at intervals of 20. The y-axis is labeled \"Values,\" also using a linear scale, ranging from 0.0 to 1.0, with tick labels at intervals of 0.2. There are three lines in the plot. The blue line with circular markers and a solid style represents \"Accuracy,\" beginning at approximately 0.64 and ending near 0.85, showing an upward trend. The green line with triangular markers and a solid style represents \"Confidence,\" starting around 0.84 and increasing slightly to end close to 0.95, showing a steady upward movement. The red line with square markers and a solid style represents \"Alignment,\" starting near 0.74 and increasing to approximately 0.9, reflecting a consistent upward trend."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure30.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure30.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of Llama3 under different relation popularity.",
      "gpt_annotation": "The plot illustrates the relationship between answer popularity and three metrics: accuracy, confidence, and alignment for the Llama3-8B on songs. The x-axis represents \"Answer Popularity,\" with a linear scale ranging from 0 to 160, marked at regular intervals of 20. The y-axis is labeled \"Values,\" also using a linear scale, ranging from 0.0 to 1.0, with tick marks at intervals of 0.2. Three lines are present: The blue line with circular markers and a solid style represents accuracy, starting at approximately 0.3, dipping below 0.2 around 20, peaking around 0.5 at 100, and concluding at approximately 0.4. The green line, denoted by triangular markers with a solid line style, represents confidence, beginning around 0.7, peaking near 0.8 at 100, and ending just above 0.8. The red line with square markers and a solid line style depicts alignment, starting near 0.6, peaking around 0.7 between 80 and 100, then dropping to around 0.5 at 120, and finishing near 0.6 at the end."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure31.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure31.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of Llama3 under different relation popularity.",
      "gpt_annotation": "The plot illustrates the relationship between answer popularity and various performance metrics of Llama3-8B on a basketball task. The x-axis represents \"Answer Popularity,\" which is on a linear scale with a range from 0 to 300 and tick intervals at every 50 units. The y-axis denotes \"Values,\" also on a linear scale, ranging from 0.0 to 1.0 with tick intervals at every 0.2 units. There are three lines depicted in the plot. The blue line, representing \"Accuracy,\" uses circular markers and a solid line style. It starts near 0.1 at an answer popularity of 0 and peaks at around 0.25 at 200 before slightly declining. The green line for \"Confidence\" features triangular markers and a solid line. It begins around 0.6, fluctuates mildly, reaches approximately 0.65 between 100 and 150, and then decreases slightly towards 0.6 at 300. The red line, \"Alignment,\" with square markers and a solid line style, starts close to 0.5, increases to around 0.75 at 200, and then declines to just above 0.6 at 300."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure32.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure32.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of Qwen2 under different relation popularity.",
      "gpt_annotation": "The plot illustrates the relationship between answer popularity and three different metrics: accuracy, confidence, and alignment for Qwen2-7B on movies. The x-axis is labeled \"Answer Popularity,\" with a linear scale ranging from 0 to 100, and tick intervals of 20. The y-axis is labeled \"Values,\" also on a linear scale, with a range from 0 to 1, and tick intervals of 0.2. The blue line represents accuracy, featuring circular markers and a solid line style. It starts at approximately 0.2 and increases steadily to just above 0.6 at the end of the x-axis. The green line, indicating confidence, uses triangular markers and a solid line style. It begins just above 0.7 and rises gradually to around 0.9. The red line represents alignment, with square markers and a solid line style. It starts near 0.4, rises to around 0.6, and remains relatively steady towards the end."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure33.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure33.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of Qwen2 under different relation popularity.",
      "gpt_annotation": "The plot illustrates the relationship between \"Answer Popularity\" and three measured metrics: \"Accuracy,\" \"Confidence,\" and \"Alignment\" for the model Qwen2-7B on a set of songs. The x-axis is labeled \"Answer Popularity,\" uses a linear scale, and ranges from 0 to 160 with tick labels at intervals of 20. The y-axis is labeled \"Values,\" also uses a linear scale, and ranges from 0.0 to 1.0 with tick labels at intervals of 0.2. The blue line represents \"Accuracy,\" marked with circles and a solid line style. It starts at a value just above 0.0 at an x-value close to 0, increases with small bumps, reaching about 0.5 around an x-value of 100, and ends at a higher position nearing 0.6 at an x-value of 160. The green line represents \"Confidence,\" marked with triangles and a solid line style. It begins at approximately 0.7, showing a slight rise and some minor fluctuations, peaking slightly above 0.8, and ends around the same level at x-value 160. The red line symbolizes \"Alignment,\" marked with squares and a solid line style. It begins at about 0.4, presents minor undulations with larger increases, reaching slightly above 0.6 around an x-value of 80, and then follows a gentle increase to about 0.65 at the end."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure34.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure34.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of Qwen2 under different relation popularity.",
      "gpt_annotation": "The plot illustrates the relationship between answer popularity and three performance metrics for Qwen2-7B on basketball data. The x-axis, labeled \"Answer Popularity,\" uses a linear scale ranging from 0 to 300, with tick intervals of 50 units and no specified units. The y-axis, labeled \"Values,\" also uses a linear scale ranging from 0.0 to 1.0, with tick intervals of 0.2 units. The plot features three lines: a blue line with circle markers and a solid style representing \"Accuracy,\" which starts near 0.1, increases to about 0.6, and then decreases slightly by the endpoint. A green line with triangle markers and a solid style represents \"Confidence,\" starting at around 0.7, showing slight fluctuations, and ending slightly below its initial level. A red line with square markers and a solid style represents \"Alignment,\" starting at 0.3, gradually rising to 0.5, and then leveling off toward the endpoint."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure35.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure35.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of ChatGPT under different relation popularity.",
      "gpt_annotation": "The plot illustrates the relationship between different metrics and answer popularity in the context of ChatGPT's performance on movie data. The x-axis is labeled \"Answer Popularity,\" utilizing a linear scale with a range from 0 to 100, featuring intervals of 20 units. The y-axis is labeled \"Values,\" also on a linear scale, with a range from 0 to 1 and intervals of 0.2 units. The plot contains three lines: the \"Accuracy\" line is blue with circle markers and a solid style, beginning at approximately (0, 0.92) and ending around (100, 0.98), indicating a slight upward trend. The \"Confidence\" line is green with triangle markers and a solid style, starting near (0, 0.95) and ending at (100, 0.99), also exhibiting a gradual increase. The \"Alignment\" line is red with square markers and a solid style, beginning at (0, 0.95) and ending at (100, 1.0), showing a steady increase, slightly leveling off towards the end."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure36.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure36.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of ChatGPT under different relation popularity.",
      "gpt_annotation": "The plot illustrates the relationship between answer popularity and three metrics: accuracy, confidence, and alignment, for ChatGPT on song-related data. The x-axis is labeled \"Answer Popularity\" with a linear scale, ranging from 0 to 160, with tick labels at intervals of 20. The y-axis is labeled \"Values,\" also on a linear scale, ranging from 0.0 to 1.0, with tick labels at intervals of 0.2. The blue line represents the \"Accuracy\" metric, characterized by circular markers and a solid line style. It starts at approximately 0.6 at an answer popularity of 20 and ends near 0.8 at 160, showing a rise with some fluctuations. The green line represents \"Confidence,\" featuring triangular markers and a solid line style, starting around 0.95 at 20 and maintaining a generally steady high level, closing slightly higher around 0.98 at 160. The red line, representing \"Alignment,\" uses square markers with a solid line style, beginning near 0.65 at 20 and increasing, leveling off around 0.8 towards the end of the x-axis. Each line shows distinct behaviors in terms of consistency and relative elevation across the range of answer popularity."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17537v1",
      "paper_title": "How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception",
      "figure_filename": "035_2505.17537v1_Figure37.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/035_2505.17537v1_Figure37.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/035_2505.17537v1_How Knowledge Popularity Influences and Enhances LLM Knowledge Boundary   Perception.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The QA performance, confidence, and alignment of ChatGPT under different relation popularity.",
      "gpt_annotation": "The plot illustrates the relationship between answer popularity and three parameters: accuracy, confidence, and alignment. The x-axis is labeled \"Answer Popularity,\" presented on a linear scale ranging from 0 to 300, with tick marks at intervals of 50. The y-axis is labeled \"Values,\" also on a linear scale, ranging from 0.0 to 1.0, with tick marks at intervals of 0.2. The blue line represents accuracy, with circular markers and a solid style. It starts at approximately 0.25 at 0 popularity, decreases to about 0.2 at 50, climbs steadily to around 0.55 at 200, and slightly decreases towards 0.5 by 300. The green line denotes confidence, with triangular markers and a dashed style. It remains fairly steady around 0.8 throughout the entire range, starting slightly below 0.8 at 0 and ending around 0.78 at 300. The red line stands for alignment, with square markers and a dashed style. It begins near 0.55 at 0 popularity, dips to around 0.5 at 50, then rises steadily to about 0.7 at 200, maintaining a similar value towards the end at 300."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure7.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure7.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "We compare error rate (one minus accuracy), computation cost, and memory usage across our approach and baselines when fine-tuning Llama-3-8B on ten NLP tasks. MTL-FT refers to first fine-tuning a shared LoRA on all the datasets, and then fine-tuning the low-rank adapter on each dataset, while Full FT refers to full fine-tuning of the entire model. Our approach boosts the test accuracy of QLoRA by $10\\%$ on average, only incurring $8\\%$ additional computation and 9 GB more memory. It performs on par with the best baseline with $45\\%$ less FLOPs.",
      "gpt_annotation": "The bar chart illustrates the comparison of different methods by their respective number of FLOPs, represented on a vertical scale. The horizontal axis contains the categories labeled as \"Full FT,\" \"TAG,\" \"LoRA,\" \"MTL-FT,\" \"QLoRA,\" and \"Ours,\" ordered from left to right, with a rotated orientation. The vertical axis is labeled \"# FLOPs\" and uses a logarithmic scale ranging from 10^18 to 10^20, with tick labels marked at each power of ten. Each method is represented by a bar, with the first five categories (\"Full FT,\" \"TAG,\" \"LoRA,\" \"MTL-FT,\" and \"QLoRA\") shown in orange, while the \"Ours\" category is depicted in green. The \"Full FT\" bar is the tallest, approximately reaching 10^20 FLOPs, while the \"Ours\" bar, the shortest, is just above 10^18 FLOPs. No specific sorting method is applied beyond the given order, and the \"Ours\" bar is visually emphasized by its color and bolded label."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure9.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure9.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "We compare error rate (one minus accuracy), computation cost, and memory usage across our approach and baselines when fine-tuning Llama-3-8B on ten NLP tasks. MTL-FT refers to first fine-tuning a shared LoRA on all the datasets, and then fine-tuning the low-rank adapter on each dataset, while Full FT refers to full fine-tuning of the entire model. Our approach boosts the test accuracy of QLoRA by $10\\%$ on average, only incurring $8\\%$ additional computation and 9 GB more memory. It performs on par with the best baseline with $45\\%$ less FLOPs.",
      "gpt_annotation": "The bar chart illustrates the GPU memory consumption in gigabytes (GB) for different methods, with the purpose of showcasing memory usage across various approaches labeled as QLoRA. The categories are placed along the horizontal axis, which contains the labels \"Full FT,\" \"MTL-FT,\" \"TAG,\" \"LoRA,\" \"QLoRA,\" and \"Ours,\" all oriented diagonally. The vertical axis represents the GPU Memory in GB, using a linear scale with a range from 0 to 70, marked at intervals of 10 GB. The bars are colored orange for all methods except \"Ours,\" which is green. The values for the orange bars are approximately: \"Full FT\" at about 65 GB, \"MTL-FT\" at about 45 GB, \"TAG\" and \"LoRA\" both around 25 GB, and \"QLoRA\" around 15 GB. The green bar labeled \"Ours\" has a height of approximately 20 GB. There is no specific sorting order for the bars, and the \"Ours\" bar is visually emphasized by its distinct color and bold typeface."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure10.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure10.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Illustrating the empirical generalization errors and sharpness measures with respect to QLoRA weights. (<ref>) Smaller adapters with a rank of $16$ achieve the lowest generalization errors. Additionally, the Hessian trace values correlate with generalization errors, suggesting that smaller adapters tend to converge to flatter minima. (<ref>) An ensemble of $k$ adapters leads to lower generalization errors and Hessian traces. Here, we fix the sum of the dimensions of $k$ adapters to be equal to $256$. (<ref>) QLoRA, which is trained on a quantized base model, yields lower generalization errors and Hessian trace values compared to LoRA.",
      "gpt_annotation": "The plot illustrates the relationship between rank and error (RTE). The x-axis is labeled \"Rank\" and features a linear scale with values ranging from 4 to 256. The tick labels are placed at 4, 16, 64, and 256, with no units specified. The y-axis is labeled \"Error (RTE)\" and also uses a linear scale, with values ranging from 0.3 to 0.9. The y-axis tick labels are regularly spaced at 0.3, 0.6, and 0.9. The plot contains a single black solid line with green circular markers representing data points. Starting at an x-axis value of 4, the y-axis value is approximately 0.6. The line shows a slight decline, reaching its lowest point around an x-value of 64 with a corresponding y-value of approximately 0.4. It then rises steadily, ending at an x-value of 256 with a y-value just above 0.6. Error bars are present at each marker location."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure11.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure11.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Illustrating the empirical generalization errors and sharpness measures with respect to QLoRA weights. (<ref>) Smaller adapters with a rank of $16$ achieve the lowest generalization errors. Additionally, the Hessian trace values correlate with generalization errors, suggesting that smaller adapters tend to converge to flatter minima. (<ref>) An ensemble of $k$ adapters leads to lower generalization errors and Hessian traces. Here, we fix the sum of the dimensions of $k$ adapters to be equal to $256$. (<ref>) QLoRA, which is trained on a quantized base model, yields lower generalization errors and Hessian trace values compared to LoRA.",
      "gpt_annotation": "The plot illustrates the relationship between rank and errors in the COPA dataset. The x-axis is labeled \"Rank\" and is linear, with values at 4, 16, 64, and 256. The y-axis is labeled \"Errors\" with a linear scale, ranging from 0.0 to 0.7, with tick labels at intervals of 0.1. The plot features a single black solid line that connects data points marked by green circles. At the start of the x-axis, the data point at rank 4 shows an error value of approximately 0.35 with some error bars indicating variability. As the rank increases to 16, the error value drops to around 0.25, reaching a minimum for this plot. The error value at rank 64 slightly increases to about 0.3. Finally, the error value significantly rises to approximately 0.65 at rank 256."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure12.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure12.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Illustrating the empirical generalization errors and sharpness measures with respect to QLoRA weights. (<ref>) Smaller adapters with a rank of $16$ achieve the lowest generalization errors. Additionally, the Hessian trace values correlate with generalization errors, suggesting that smaller adapters tend to converge to flatter minima. (<ref>) An ensemble of $k$ adapters leads to lower generalization errors and Hessian traces. Here, we fix the sum of the dimensions of $k$ adapters to be equal to $256$. (<ref>) QLoRA, which is trained on a quantized base model, yields lower generalization errors and Hessian trace values compared to LoRA.",
      "gpt_annotation": "The plot illustrates the relationship between rank and traces multiplied by \\(10^{-5}\\) in the context of RTE. The x-axis is labeled \"Rank,\" uses a linear scale, and has numeric tick marks at 4, 16, 64, and 256, representing regular intervals. The y-axis is labeled \"Traces \u00d7 \\(10^{-5}\\) (RTE),\" also uses a linear scale, and ranges from 0.0 to 6.0 with tick labels at 0.0, 3.0, and 6.0. The plot features a single black line with square markers that have an orange fill. The line starts at approximately 4.0 on the y-axis when the x-axis is at 4, descends to around 1.5 when the x-axis reaches 16, remains relatively steady as the x-axis reaches 64, and then rises to approximately 5.0 at an x-axis value of 256. The line is solid, showing dips at 16 and 64 and an increase towards the end of the x-axis range."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure13.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure13.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Illustrating the empirical generalization errors and sharpness measures with respect to QLoRA weights. (<ref>) Smaller adapters with a rank of $16$ achieve the lowest generalization errors. Additionally, the Hessian trace values correlate with generalization errors, suggesting that smaller adapters tend to converge to flatter minima. (<ref>) An ensemble of $k$ adapters leads to lower generalization errors and Hessian traces. Here, we fix the sum of the dimensions of $k$ adapters to be equal to $256$. (<ref>) QLoRA, which is trained on a quantized base model, yields lower generalization errors and Hessian trace values compared to LoRA.",
      "gpt_annotation": "The plot illustrates the variation of \"Traces \u00d7 10^-5 (COPA)\" with respect to different ranks. The x-axis is labeled \"Rank,\" using a linear scale with a value range from 4 to 256, and tick marks at intervals of 4, 16, 64, and 256. The y-axis represents \"Traces \u00d7 10^-5 (COPA)\" on a linear scale, ranging from 0.0 to 8.0 with tick label intervals of 2.0. The plot features a single line in black with square markers highlighted in orange, connected by a solid line style. At the initial rank of 4, the trace value is approximately 0.5. This value remains nearly constant until rank 16. The line then shows a gradual increase, reaching around 2.0 at rank 64. Finally, the line exhibits a steeper rise, culminating at approximately 8.0 at rank 256."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure14.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure14.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Illustrating the empirical generalization errors and sharpness measures with respect to QLoRA weights. (<ref>) Smaller adapters with a rank of $16$ achieve the lowest generalization errors. Additionally, the Hessian trace values correlate with generalization errors, suggesting that smaller adapters tend to converge to flatter minima. (<ref>) An ensemble of $k$ adapters leads to lower generalization errors and Hessian traces. Here, we fix the sum of the dimensions of $k$ adapters to be equal to $256$. (<ref>) QLoRA, which is trained on a quantized base model, yields lower generalization errors and Hessian trace values compared to LoRA.",
      "gpt_annotation": "The plot illustrates the relationship between an unspecified variable \\( k \\) and the corresponding errors represented by RTE. The x-axis is labeled \\( k \\), with a linear scale ranging from 1 to 8, marked at intervals of 1 unit. The y-axis represents the errors (RTE) on a linear scale, ranging from 0.3 to 0.9, with tick labels at intervals of 0.3. A single line is shown, which is black with no markers, and it displays a solid line style. Green circular markers indicate data points at specific values of \\( k \\). The line starts at \\( k = 1 \\) with an error of approximately 0.6 and decreases steadily to \\( k = 8 \\), where the error is approximately 0.3. Each data point is accompanied by error bars, indicating variability or uncertainty in the measurements."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure15.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure15.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Illustrating the empirical generalization errors and sharpness measures with respect to QLoRA weights. (<ref>) Smaller adapters with a rank of $16$ achieve the lowest generalization errors. Additionally, the Hessian trace values correlate with generalization errors, suggesting that smaller adapters tend to converge to flatter minima. (<ref>) An ensemble of $k$ adapters leads to lower generalization errors and Hessian traces. Here, we fix the sum of the dimensions of $k$ adapters to be equal to $256$. (<ref>) QLoRA, which is trained on a quantized base model, yields lower generalization errors and Hessian trace values compared to LoRA.",
      "gpt_annotation": "The plot illustrates the errors associated with different values of \\( k \\) in the context of COPA. The x-axis is labeled \\( k \\) and uses a linear scale ranging from 1 to 8, with tick marks at intervals of 1 unit, although some values like 3, 5, 6, and 7 do not have tick labels. The y-axis is labeled with errors and is also on a linear scale, ranging from 0.0 to 0.9, with tick marks at intervals of 0.3. The plot features a single line that is black and solid, with circular markers that are green in color. This line shows a decreasing trend across the x-axis. At \\( k = 1 \\), the error value is approximately 0.65. The line declines sharply to about 0.3 at \\( k = 2 \\). It continues to decrease, reaching around 0.2 at \\( k = 4 \\), and then slowly decreases to approximately 0.1 at \\( k = 8 \\). Error bars are present at each data point."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure16.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure16.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Illustrating the empirical generalization errors and sharpness measures with respect to QLoRA weights. (<ref>) Smaller adapters with a rank of $16$ achieve the lowest generalization errors. Additionally, the Hessian trace values correlate with generalization errors, suggesting that smaller adapters tend to converge to flatter minima. (<ref>) An ensemble of $k$ adapters leads to lower generalization errors and Hessian traces. Here, we fix the sum of the dimensions of $k$ adapters to be equal to $256$. (<ref>) QLoRA, which is trained on a quantized base model, yields lower generalization errors and Hessian trace values compared to LoRA.",
      "gpt_annotation": "The plot illustrates the relationship between the variable \\( k \\) on the x-axis and the \"Traces \\(\\times 10^{-5}\\) (RTE)\" on the y-axis. The x-axis is labeled \\( k \\) with a linear scale ranging from 0 to 8, with tick marks at intervals of 2. The y-axis represents \"Traces \\(\\times 10^{-5}\\) (RTE)\" with a linear scale from 0 to 3.5, with tick marks at intervals of 0.5. A single black solid line with square markers is present, starting at \\( k = 1 \\) with a value of approximately 3.2 on the y-axis. The line sharply declines to around 1.0 at \\( k = 2 \\), and then it shows a slight decrease, maintaining values close to 1.0 at \\( k = 4 \\) and \\( k = 8 \\). The markers are highlighted with orange squares, indicating specific data points, and error bars are present at each marker, representing variability or uncertainty."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure17.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure17.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Illustrating the empirical generalization errors and sharpness measures with respect to QLoRA weights. (<ref>) Smaller adapters with a rank of $16$ achieve the lowest generalization errors. Additionally, the Hessian trace values correlate with generalization errors, suggesting that smaller adapters tend to converge to flatter minima. (<ref>) An ensemble of $k$ adapters leads to lower generalization errors and Hessian traces. Here, we fix the sum of the dimensions of $k$ adapters to be equal to $256$. (<ref>) QLoRA, which is trained on a quantized base model, yields lower generalization errors and Hessian trace values compared to LoRA.",
      "gpt_annotation": "The plot illustrates the relationship between the variable \\( k \\) and the number of traces in the study, measured against a scaled factor of \\( 10^{-5} \\) for the COPA dataset. The x-axis is labeled \\( k \\) and uses a linear scale with values ranging from 1 to 9, having regular tick intervals at every integer. The y-axis, labeled \"Traces \\( \\times 10^{-5} \\)\", also utilizes a linear scale, with a range from 0 to 7 and tick labels at intervals of 1. The plot contains a single black line with solid connectivity, using square markers filled with orange. Starting at \\( k = 1 \\), the line begins at approximately 6.0, indicating a decline to about 1.0 at \\( k = 2 \\). Subsequently, it decreases slightly towards \\( k = 8 \\), ending around 0.5. The line features markers at each tick on the x-axis, representing data points with slight variations indicated by minimal vertical error bars."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure18.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure18.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Illustrating the empirical generalization errors and sharpness measures with respect to QLoRA weights. (<ref>) Smaller adapters with a rank of $16$ achieve the lowest generalization errors. Additionally, the Hessian trace values correlate with generalization errors, suggesting that smaller adapters tend to converge to flatter minima. (<ref>) An ensemble of $k$ adapters leads to lower generalization errors and Hessian traces. Here, we fix the sum of the dimensions of $k$ adapters to be equal to $256$. (<ref>) QLoRA, which is trained on a quantized base model, yields lower generalization errors and Hessian trace values compared to LoRA.",
      "gpt_annotation": "The bar chart illustrates the error rates (RTE) associated with different ranks for two methods, LoRA and QLoRA. The horizontal axis displays the categories labeled as ranks with values 4, 16, 64, and 256 in a standard numerical order, indicating distinct groups. The vertical axis represents the error, labeled as \"Error (RTE)\" with a linear scale, ranging from 0.3 to 0.9 and marked at intervals of 0.3, 0.6, and 0.9. The bars are grouped within each rank category to depict the results for the two different methods: LoRA, represented with blue bars, and QLoRA, depicted with green bars. For rank 4, the blue bar (LoRA) shows an approximate value of 0.6, while the green bar (QLoRA) is slightly below this value. For rank 16, the values are similar, with LoRA slightly above QLoRA. At rank 64 and 256, the trend continues with LoRA remaining consistently higher than QLoRA. Each bar is accompanied by a black error line, indicating variability in the measurements. There is no specific sorting order for the ranks beyond their numerical progression, and no particular bar is emphasized beyond the use of distinct colors for each method."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure19.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure19.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Illustrating the empirical generalization errors and sharpness measures with respect to QLoRA weights. (<ref>) Smaller adapters with a rank of $16$ achieve the lowest generalization errors. Additionally, the Hessian trace values correlate with generalization errors, suggesting that smaller adapters tend to converge to flatter minima. (<ref>) An ensemble of $k$ adapters leads to lower generalization errors and Hessian traces. Here, we fix the sum of the dimensions of $k$ adapters to be equal to $256$. (<ref>) QLoRA, which is trained on a quantized base model, yields lower generalization errors and Hessian trace values compared to LoRA.",
      "gpt_annotation": "The bar chart illustrates the number of errors for the COPA task across different ranks. The horizontal axis contains the categories labeled as \"Rank,\" with values of 4, 16, 64, and 256. The vertical axis represents the \"Errors (COPA)\" on a linear scale, ranging from 0.0 to 0.6, with tick intervals at 0.0, 0.3, and 0.6. The chart contains grouped bars for each rank value: blue and green bars represent different error measurements. In the chart, the blue bar for Rank 4 is approximately 0.25, and the green bar is about 0.3. For Rank 16, the blue bar is approximately 0.3, while the green bar is around 0.1. The Rank 64 bars show the blue at about 0.35 and the green at 0.15. Finally, for Rank 256, both blue and green bars are around 0.6. Each bar includes an error line on top. The data shows no specific sorting order beyond the numerical increasing order of the ranks. There is no emphasis on any particular bar through shading, bolding, or additional annotations."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure20.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure20.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Illustrating the empirical generalization errors and sharpness measures with respect to QLoRA weights. (<ref>) Smaller adapters with a rank of $16$ achieve the lowest generalization errors. Additionally, the Hessian trace values correlate with generalization errors, suggesting that smaller adapters tend to converge to flatter minima. (<ref>) An ensemble of $k$ adapters leads to lower generalization errors and Hessian traces. Here, we fix the sum of the dimensions of $k$ adapters to be equal to $256$. (<ref>) QLoRA, which is trained on a quantized base model, yields lower generalization errors and Hessian trace values compared to LoRA.",
      "gpt_annotation": "The bar chart illustrates the comparison of two categories, LoRA and QLoRA, across different rank values in terms of traces multiplied by 10\u207b\u2075, specific to the RTE dataset. The horizontal axis contains the categories labeled as \"Rank,\" with values 4, 16, 64, and 256 presented in ascending order. The vertical axis represents \"Traces \u00d7 10\u207b\u2075 (RTE),\" using a linear scale, ranging from 0.0 to 6.0, with tick marks at intervals of 1.0 units. Each group comprises two bars representing LoRA in blue and QLoRA in orange. The approximate values for LoRA bars are 2.0 for rank 4, 3.0 for rank 16, 3.5 for rank 64, and 6.0 for rank 256, while QLoRA bars show values slightly below 2.0 for rank 4, just below 3.0 for rank 16, near 2.0 for rank 64, and around 4.0 for rank 256. The bars are grouped side-by-side for each rank, and no bars are visually emphasized."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure21.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure21.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Illustrating the empirical generalization errors and sharpness measures with respect to QLoRA weights. (<ref>) Smaller adapters with a rank of $16$ achieve the lowest generalization errors. Additionally, the Hessian trace values correlate with generalization errors, suggesting that smaller adapters tend to converge to flatter minima. (<ref>) An ensemble of $k$ adapters leads to lower generalization errors and Hessian traces. Here, we fix the sum of the dimensions of $k$ adapters to be equal to $256$. (<ref>) QLoRA, which is trained on a quantized base model, yields lower generalization errors and Hessian trace values compared to LoRA.",
      "gpt_annotation": "The bar chart illustrates the number of traces, scaled by \\(10^{-5}\\), associated with different ranks, specifically for COPA. The horizontal axis contains the categories labeled as ranks 4, 16, 64, and 256, and the arrangement follows an ascending numerical order. The vertical axis is the value axis, labeled as \"Traces \u00d7 \\(10^{-5}\\),\" with a linear scale ranging from 0.0 to 8.0, with tick marks at intervals of 2.0. For each rank, there are two bars: a blue bar and an orange bar. For rank 4, the blue bar has an approximate value of 1.6 and the orange bar about 0.8. Rank 16 has the blue bar at approximately 2.0 and the orange bar nearly 0.4. Rank 64 shows the blue bar at about 5.0, and the orange bar around 3.8. Finally, rank 256 features a blue bar approximately reaching 8.5, with the orange bar at about 6.5. Each bar has an error bar indicating variability or uncertainty in the measurements, denoted by black lines. The bars are not visually emphasized beyond the use of distinct colors for differentiation, and the chart does not sort bars beyond the ascending order of categories."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure24.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure24.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "This figure compares the error rate (one average minus test accuracy), computation cost, and GPU memory across our approach and baselines, for fine-tuning Llama-3-8B on ten NLP tasks with QAdapter.",
      "gpt_annotation": "The bar chart illustrates the computational requirements, measured in FLOPs, for different methods related to \"QAdapter.\" The horizontal axis contains the categories labeled as Full FT, TAG, Adapter, MTL-FT, QAdapter, and Ours, ordered from left to right, each with text tilted and positioned at the bottom. The vertical axis represents the number of FLOPs, using a logarithmic scale ranging from 10^18 to 10^20 with tick labels at each power of ten. The bars corresponding to each method are distinguished by color, with all but the last bar in orange, and the \"Ours\" category in green. The tallest bar, representing Full FT, reaches approximately 10^20 FLOPs, while the shortest bar, \"Ours,\" is slightly above 10^18 FLOPs. The chart does not display any specific sorting method for the categories, and the \"Ours\" bar is visually emphasized by being in green, as opposed to the orange color used for the other bars."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure25.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure25.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "This figure compares the error rate (one average minus test accuracy), computation cost, and GPU memory across our approach and baselines, for fine-tuning Llama-3-8B on ten NLP tasks with QAdapter.",
      "gpt_annotation": "The bar chart, titled \"QAdapter,\" illustrates GPU memory usage in gigabytes (GB) for different methods. The horizontal axis contains the categories, labeled as \"Full FT,\" \"MTL-FT,\" \"TAG,\" \"Adapter,\" \"QAdapter,\" and \"Ours,\" in a slanted format, and it does not follow a specific sorting order. The vertical axis represents the GPU Memory in gigabytes, with a linear scale ranging from 0 to 80, marked in intervals of 20 GB. Each bar represents a different method, with bars colored in orange except for \"Ours,\" which is green. The \"Full FT\" bar is the tallest, approximately reaching 70 GB, followed by \"MTL-FT\" at about 45 GB. The \"TAG,\" \"Adapter,\" and \"QAdapter\" bars have lower heights, with approximate values of 20 GB, 15 GB, and 10 GB, respectively. The \"Ours\" bar is slightly above 20 GB in height. All bars are distinct and uniformly styled except for the bolded label of the \"Ours\" category, which is also visually emphasized using a different color."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure26.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure26.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "We show the empirical generalization errors (termed Error in the figures) and estimated Hessian traces of the loss for fine-tuned adapters (using QAdapter on Llama-3-8B) across various hidden dimensions. Smaller adapters achieve the lowest generalization errors. Additionally, the Hessian trace values correlate with generalization errors, suggesting that smaller adapters tend to converge to flatter minima.",
      "gpt_annotation": "The plot illustrates the relationship between \"Rank\" and \"Errors (RTE).\" The x-axis is labeled \"Rank,\" with a linear scale ranging from 4 to 256, featuring tick labels at 4, 16, 64, and 256. The y-axis is labeled \"Errors (RTE),\" with a linear scale ranging from 0.0 to 0.9, displaying tick labels at intervals of 0.3. There is one line in the plot, which is black with circular green markers. The line style is solid, and it shows a trend that starts at approximately 0.3 at an x-value of 4, then slightly decreases slightly around 16, maintains a similar level at 64, and finally increases to approximately 0.6 by 256. The plot includes error bars at each marker."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure27.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure27.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "We show the empirical generalization errors (termed Error in the figures) and estimated Hessian traces of the loss for fine-tuned adapters (using QAdapter on Llama-3-8B) across various hidden dimensions. Smaller adapters achieve the lowest generalization errors. Additionally, the Hessian trace values correlate with generalization errors, suggesting that smaller adapters tend to converge to flatter minima.",
      "gpt_annotation": "The plot illustrates the relationship between \"Rank\" on the x-axis and \"Traces \u00d7 10\u207b\u2075 (RTE)\" on the y-axis. The x-axis is labeled as \"Rank,\" using a linear scale with values at 4, 16, 64, and 256, which are evenly spaced. The y-axis is labeled as \"Traces \u00d7 10\u207b\u2075 (RTE)\" and also uses a linear scale with values ranging from 0.0 to 8.0, marked in intervals of 2.0. The plot features a single black line with orange square markers and error bars, following a solid line style. The line starts at approximately 4.5 at Rank 4, decreases slightly to around 3.5 at Rank 16, then increases steadily, reaching values of about 4.0 at Rank 64 and approximately 7.5 at Rank 256, highlighting a peak at the higher Rank values."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure28.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure28.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "We show the empirical generalization errors (termed Error in the figures) and estimated Hessian traces of the loss for fine-tuned adapters (using QAdapter on Llama-3-8B) across various hidden dimensions. Smaller adapters achieve the lowest generalization errors. Additionally, the Hessian trace values correlate with generalization errors, suggesting that smaller adapters tend to converge to flatter minima.",
      "gpt_annotation": "The plot illustrates the relationship between rank and errors (COPA). The x-axis is labeled \"Rank,\" uses a linear scale, and displays discrete values of 4, 16, 64, and 256, with no units specified. The y-axis is labeled \"Errors (COPA),\" also uses a linear scale, and ranges from 0.0 to 0.6 with tick marks at intervals of 0.1. The plot features a single line that is black with a solid style, and green circle markers. The line starts at a value of approximately 0.3 at a rank of 4, slightly decreases at a rank of 16, increases to around 0.4 at a rank of 64, and ends at about 0.5 at a rank of 256. The markers are accompanied by vertical error bars."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure29.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure29.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "We show the empirical generalization errors (termed Error in the figures) and estimated Hessian traces of the loss for fine-tuned adapters (using QAdapter on Llama-3-8B) across various hidden dimensions. Smaller adapters achieve the lowest generalization errors. Additionally, the Hessian trace values correlate with generalization errors, suggesting that smaller adapters tend to converge to flatter minima.",
      "gpt_annotation": "The plot illustrates the relationship between Rank and Traces multiplied by \\(10^{-5}\\) for a dataset labeled COPA. The x-axis is labeled \"Rank\" with a linear scale ranging from 4 to 256, with tick marks at 4, 16, 64, and 256. The y-axis is labeled \"Traces \u00d7 10^{-5} (COPA)\" with a linear scale ranging from 0.0 to 1.0 and tick marks at intervals of 0.5. The plot features a single black line with square markers filled in orange. The line connects data points representing Rank values, starting near 0.25 at Rank 4 and ending near 0.75 at Rank 256. The line follows a generally increasing trend, with data points at Rank 16 and 64 showing values near 0.25 and 0.50 respectively, indicating an upward progression with some variability as depicted by error bars."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure30.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure30.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "We show the empirical generalization errors (termed Error in the figures) and estimated Hessian traces of the loss for fine-tuned adapters (using QAdapter on Llama-3-8B) across various hidden dimensions. Smaller adapters achieve the lowest generalization errors. Additionally, the Hessian trace values correlate with generalization errors, suggesting that smaller adapters tend to converge to flatter minima.",
      "gpt_annotation": "This plot illustrates the error rates associated with different ranks in the BoolQ dataset. The x-axis, labeled \"Rank,\" has a linear scale, covering values 4, 16, 64, and 256 with no specific units, and tick labels are shown at these distinct values. The y-axis is labeled \"Errors (BoolQ)\" and also uses a linear scale with a range from 0.0 to 0.6, with tick intervals of 0.2. A single line is present, which is black and solid, with green circular markers. Starting at a rank of 4, the error is approximately 0.23. The line shows a decrease to around 0.18 at rank 16, followed by a gradual increase to about 0.36 at rank 256. Error bars are present at each marker, indicating variability around the plotted mean values."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure31.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure31.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "We show the empirical generalization errors (termed Error in the figures) and estimated Hessian traces of the loss for fine-tuned adapters (using QAdapter on Llama-3-8B) across various hidden dimensions. Smaller adapters achieve the lowest generalization errors. Additionally, the Hessian trace values correlate with generalization errors, suggesting that smaller adapters tend to converge to flatter minima.",
      "gpt_annotation": "The plot illustrates the relationship between rank and trace values, scaled by \\(10^{-5}\\), in the context of BoolQ. The x-axis is labeled \"Rank\" and employs a linear scale with values at specific ranks: 4, 16, 64, and 256. The y-axis is labeled \"Traces \u00d7 10^{-5} (BoolQ)\" and also uses a linear scale, ranging from 0.0 to 3.0, with tick marks at intervals of 1.0. The plot displays a single line graph that is solid and black with orange square markers. The line begins at a y-value of approximately 1.0 at an x-value of 4, decreases slightly to just under 1.0 at an x-value of 16, then rises gradually to around 1.2 at an x-value of 64, and continues to increase, reaching approximately 1.8 at an x-value of 256."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure36.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure36.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "We measure the average similarity between models fine-tuned on random subsets $S$ and single-task models. We observe that the similarity score becomes lower as the size of $S$ increases.",
      "gpt_annotation": "The plot illustrates the relationship between the parameter \\(|S|\\) and cosine similarity for two different methods. The x-axis is labeled \\(|S|\\) and uses a linear scale with values ranging from 2 to 10, with tick marks at intervals of 2. The y-axis is labeled \"Cosine similarity\" and also uses a linear scale, with values ranging from 0.1 to 0.35, depicted by unlabeled tick marks at consistent spacing. The plot includes two lines: one in blue and another in green. The blue line represents \"Weight averaging,\" is dashed, and uses square markers. It begins at a cosine similarity of approximately 0.32 at \\(|S| = 2\\) and decreases to about 0.26 at \\(|S| = 10\\), with notable decreases evident across the axis. The green line represents \"QLoRA,\" is dashed, and uses circular markers. Starting at approximately 0.27 at \\(|S| = 2\\), it steadily decreases to around 0.19 at \\(|S| = 10\\), with a general downward trend throughout the plot."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21930v1",
      "paper_title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets",
      "figure_filename": "037_2505.21930v1_Figure37.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/037_2505.21930v1_Figure37.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/037_2505.21930v1_Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "We measure the average similarity between models fine-tuned on random subsets $S$ and single-task models. We observe that the similarity score becomes lower as the size of $S$ increases.",
      "gpt_annotation": "The plot illustrates the relationship between a parameter \\(|S|\\) and cosine similarity for two different methods. The x-axis is labeled \\(|S|\\) and uses a linear scale, ranging from 1 to 10 with tick marks at intervals of 1. The y-axis is labeled \"Cosine similarity,\" also on a linear scale, ranging from 0.1 to 0.5, with tick labels at intervals of 0.1. The plot includes two lines: The first line is blue with square markers and a dashed style representing \"Weight averaging.\" It starts at approximately 0.5 on the y-axis at \\(|S| = 1\\) and shows a gradual decline, ending near 0.4 at \\(|S| = 10\\). The second line is green with circle markers and a dashed style representing \"QAdapter.\" This line begins at around 0.35 when \\(|S| = 1\\) and shows a decreasing trend, ending near 0.25 at \\(|S| = 10\\)."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.02678v1",
      "paper_title": "TL;DR Too Long, Do Re-weighting for Effcient LLM Reasoning Compression",
      "figure_filename": "039_2506.02678v1_Figure3.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/039_2506.02678v1_Figure3.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/039_2506.02678v1_TL;DR Too Long, Do Re-weighting for Effcient LLM Reasoning Compression.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/039_2506.02678v1_TL;DR Too Long, Do Re-weighting for Effcient LLM Reasoning Compression.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Comparison of accuracy and generation length between Vanilla CoT and our method on four benchmark datasets (GSM8K, MATH500, AIME, AMC) using DeepSeek-R1-Distill-Qwen models. consistently reduces generation length while maintaining or improving accuracy across both 7B and 14B model scales.",
      "gpt_annotation": "The bar chart titled \"DeepSeek-R1-Distill-Qwen-7B Comparison\" illustrates the accuracy percentages of different models across four categories. The horizontal axis displays the categories labeled as GSM8K, MATH500, AIME, and AMC, with no specific order or scale type indicated. The vertical axis, labeled \"Accuracy (%)\", has a linear scale ranging from 0 to 100, with tick labels marked at intervals of 20 units. Each category contains two bars, distinguished by color: a yellow bar representing \"Vanilla CoT\" and a green bar representing \"TLDR (Ours)\" as indicated in the legend. In the GSM8K category, the yellow bar reaches a value of approximately 89.4%, and the green bar is slightly shorter, showing a value of about 1.7% less. For MATH500, the yellow bar is at 86.8%, while the green bar exceeds it by roughly 0.6%. In the AIME category, the yellow bar is tallest at 42.9%, with the green bar 1.7% lower. The AMC category shows a yellow bar at 81.5%, with the green bar being higher by about 1.6%. Each category's bars appear in pairs, with annotations showing percentage differences in red or green on top of the bars. The chart does not exhibit particular sorting or visual emphasis on any category, apart from the annotations."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.02678v1",
      "paper_title": "TL;DR Too Long, Do Re-weighting for Effcient LLM Reasoning Compression",
      "figure_filename": "039_2506.02678v1_Figure4.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/039_2506.02678v1_Figure4.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/039_2506.02678v1_TL;DR Too Long, Do Re-weighting for Effcient LLM Reasoning Compression.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/039_2506.02678v1_TL;DR Too Long, Do Re-weighting for Effcient LLM Reasoning Compression.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Comparison of accuracy and generation length between Vanilla CoT and our method on four benchmark datasets (GSM8K, MATH500, AIME, AMC) using DeepSeek-R1-Distill-Qwen models. consistently reduces generation length while maintaining or improving accuracy across both 7B and 14B model scales.",
      "gpt_annotation": "The bar chart titled \"DeepSeek-R1-Distill-Qwen-7B Generation Length Comparison\" illustrates the generation lengths in words for two different conditions, \"Vanilla CoT\" and \"TLDR (Ours),\" across four categories. The horizontal axis contains the categories: GSM8K, MATH500, AIME, and AMC. The vertical axis represents the generation length in words, with a linear scale ranging from 0 to 8000, marked at intervals of 1000 words. Each category features two bars: \"Vanilla CoT\" is depicted in light pink, while \"TLDR (Ours)\" is shown in dark red. For GSM8K, the light pink bar measures approximately 554 words, and the dark red bar is noticeably shorter at 46% less. In MATH500, the light pink bar is around 2861 words, with the dark red bar shortened by 46%. The AIME category has a tall light pink bar reaching 6820 words, while its dark red counterpart is 7% shorter. Lastly, in the AMC category, the light pink bar indicates 4510 words, and the dark red bar is shorter by 25%. Each pair of bars indicates a percentage difference, annotated above the dark red bars. The bars do not appear to be sorted in any specific order and no bars are distinctly emphasized beyond their coloring and annotations."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.02678v1",
      "paper_title": "TL;DR Too Long, Do Re-weighting for Effcient LLM Reasoning Compression",
      "figure_filename": "039_2506.02678v1_Figure5.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/039_2506.02678v1_Figure5.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/039_2506.02678v1_TL;DR Too Long, Do Re-weighting for Effcient LLM Reasoning Compression.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/039_2506.02678v1_TL;DR Too Long, Do Re-weighting for Effcient LLM Reasoning Compression.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Comparison of accuracy and generation length between Vanilla CoT and our method on four benchmark datasets (GSM8K, MATH500, AIME, AMC) using DeepSeek-R1-Distill-Qwen models. consistently reduces generation length while maintaining or improving accuracy across both 7B and 14B model scales.",
      "gpt_annotation": "The bar chart titled \"DeepSeek-R1-Distill-Qwen-14B Comparison\" illustrates the accuracy percentages of two models, Vanilla CoT and TLDR (Ours), across four different datasets labeled GSM8K, MATH500, AIME, and AMC. The horizontal axis contains the category labels for the datasets, using a linear scale with evenly spaced labels. The vertical value axis is labeled as \"Accuracy (%)\" and features a linear scale ranging from 0 to 100, with tick marks at intervals of 20 percent. Each dataset features two bars: one for Vanilla CoT, colored yellow, and one for TLDR (Ours), colored green. For GSM8K, the Vanilla CoT bar reaches approximately 92.5%, while the TLDR bar is slightly shorter at roughly 91%, as indicated by a red annotation \"-1.6%\". In MATH500, the bars are close, with Vanilla CoT at around 86.2% and TLDR slightly higher at 86.4%, marked by a green \"+0.2%\". For AIME, Vanilla CoT reaches 43.4%, while TLDR shows no visible difference marked as \"-0.1%\". In AMC, the TLDR bar is taller, reaching 79.6%, compared to the shorter Vanilla CoT bar, with the difference marked by \"+3.9%\". The bars are not sorted in a particular order and are not visually emphasized beyond their color differentiation and annotated percentage differences."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.02678v1",
      "paper_title": "TL;DR Too Long, Do Re-weighting for Effcient LLM Reasoning Compression",
      "figure_filename": "039_2506.02678v1_Figure6.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/039_2506.02678v1_Figure6.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/039_2506.02678v1_TL;DR Too Long, Do Re-weighting for Effcient LLM Reasoning Compression.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/039_2506.02678v1_TL;DR Too Long, Do Re-weighting for Effcient LLM Reasoning Compression.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Comparison of accuracy and generation length between Vanilla CoT and our method on four benchmark datasets (GSM8K, MATH500, AIME, AMC) using DeepSeek-R1-Distill-Qwen models. consistently reduces generation length while maintaining or improving accuracy across both 7B and 14B model scales.",
      "gpt_annotation": "The bar chart titled \"DeepSeek-R1-Distill-Qwen-14B Generation Length Comparison\" illustrates the generation length in words for different tasks using two methods: Vanilla CoT and TLDR (Ours). The horizontal axis categorizes the tasks as GSM8K, MATH500, AIME, and AMC. The vertical axis, labeled \"Generation Length (words),\" is linear and ranges from 0 to 8000 with tick marks at intervals of 1000. Each task category features two bars: the light pink bars represent Vanilla CoT, and the dark red bars represent TLDR (Ours). The bars for GSM8K show lengths of approximately 679 words for Vanilla CoT and an emphasized reduction of 65% for TLDR. In the MATH500 category, the Vanilla CoT length is around 2951 words, with a 31% decrease in TLDR. The AIME category shows a Vanilla CoT length of 6701 words, with a reduction labeled as 4% for TLDR. Similarly, the AMC category has a Vanilla CoT length of 4584 words, with a 16% reduction depicted for TLDR. Each category has its percentage reduction indicated above the TLDR bars, which are visually emphasized by a bolder shade. The bars are sorted alphabetically by category."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.16552v4",
      "paper_title": "Think Silently, Think Fast Dynamic Latent Compression of LLM Reasoning   Chains",
      "figure_filename": "040_2505.16552v4_Figure4.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/040_2505.16552v4_Figure4.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/040_2505.16552v4_Think Silently, Think Fast Dynamic Latent Compression of LLM Reasoning   Chains.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/040_2505.16552v4_Think Silently, Think Fast Dynamic Latent Compression of LLM Reasoning   Chains.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": Accuracy and reasoning chain length (#\u00a0L) of \u00a0on GSM8k dataset when trained with $c\\in\\{1,3,5,7\\}$ and tested with extra $c\\in\\{2,4,6\\}$ (under gray bars).",
      "gpt_annotation": "The plot illustrates the relationship between the compression factor and two different measurements: accuracy and log of some value labeled as #L. The x-axis is labeled \"Compression Factor (c)\" and features a linear scale with values ranging from 1 to 7, with tick labels at each integer. The y-axis on the left is labeled \"Accuracy (%)\", following a linear scale ranging from 10 to 40, with tick intervals of 10 units. The right y-axis is labeled \"log(#L)\" and also uses a linear scale, ranging from 1.5 to 3.0, with tick intervals of 0.5 units. There are two lines depicted in the plot. The purple line, marked with circle markers and a solid line style, represents accuracy. It starts at the compression factor of 1 with an accuracy of 40%, then generally declines, reaching a value of approximately 25% at a compression factor of 7, showing some fluctuations at compression factors 3 and 6. The gold line, also using circle markers and a solid line style, represents the log of #L. It begins at a compression factor of 1 with a value of about 3.0, subsequently decreasing steadily to a value below 1.5 at a compression factor of 7."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.16552v4",
      "paper_title": "Think Silently, Think Fast Dynamic Latent Compression of LLM Reasoning   Chains",
      "figure_filename": "040_2505.16552v4_Figure6.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/040_2505.16552v4_Figure6.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/040_2505.16552v4_Think Silently, Think Fast Dynamic Latent Compression of LLM Reasoning   Chains.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/040_2505.16552v4_Think Silently, Think Fast Dynamic Latent Compression of LLM Reasoning   Chains.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Layer-wise norm differences from -1 to -5.",
      "gpt_annotation": "The plot illustrates the layerwise norm difference across different layers of an unspecified system, represented by different colored lines. The x-axis is labeled \"LLM layer index,\" with a linear scale ranging from 0 to 15, using integer tick labels at an interval of 1. The y-axis is labeled \"Layerwise norm difference,\" also with a linear scale, ranging from 0.6 to 1.0, with tick marks at intervals of 0.1. The plot features five lines, each representing a different condition labeled as c=1 to c=5, depicted with various colors, marker shapes, and line styles. The red line (c=1) starts at a value of 1.0 and ends at approximately 1.0, showing a sharp decline to about 0.65 at the index 2, followed by minor fluctuations and another peak at 15. The orange line (c=2) begins at 0.9 and ends at roughly 0.85, with a dip near 0.7 at index 4 and slight fluctuations throughout. The yellow line (c=3) starts near 0.95 and ends close to 0.85, experiencing a decline to about 0.7 near index 3 and remaining relatively stable thereafter. The green line (c=4) starts at about 0.9 and ends near 0.85, with a notable dip down to approximately 0.68 at index 5, showing a small peak later. Lastly, the blue line (c=5) starts at approximately 1.0 and ends at the same value, with a significant drop to around 0.7 at index 3, and a gradual rise to its initial value."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.16552v4",
      "paper_title": "Think Silently, Think Fast Dynamic Latent Compression of LLM Reasoning   Chains",
      "figure_filename": "040_2505.16552v4_Figure7.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/040_2505.16552v4_Figure7.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/040_2505.16552v4_Think Silently, Think Fast Dynamic Latent Compression of LLM Reasoning   Chains.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/040_2505.16552v4_Think Silently, Think Fast Dynamic Latent Compression of LLM Reasoning   Chains.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance of \u00a0when implemented with base LLMs ranging from 1B to 8B parameters.",
      "gpt_annotation": "The plot illustrates the relationship between compression factor and accuracy (%) for three different models. The x-axis is labeled \"Compression Factor (c)\" and uses a linear scale, with a value range from 1 to 5, marked at intervals of 1. The y-axis is labeled \"Accuracy (%)\" and also uses a linear scale, ranging from 30 to 70 with tick marks every 10 units. Three lines are shown: the \"Llama3-1B\" model's performance is depicted with a brown line with circular markers and a solid style, starting at approximately 50% accuracy at a compression factor of 1 and decreasing steadily to around 35% at a compression factor of 5. The \"Llama3-3B\" model is represented with a maroon line with circular markers and a solid style, beginning at about 60% and declining to approximately 45% across the same range. The \"Llama3-8B\" model is shown with a purple line with circular markers and a solid style, starting near 70% and decreasing to about 55% from compression factors 1 to 5."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.15781v1",
      "paper_title": "dKV-Cache The Cache for Diffusion Language Models",
      "figure_filename": "046_2505.15781v1_Figure3.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/046_2505.15781v1_Figure3.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/046_2505.15781v1_dKV-Cache The Cache for Diffusion Language Models.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/046_2505.15781v1_dKV-Cache The Cache for Diffusion Language Models.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "table : figure",
      "gpt_annotation": "The plot illustrates the relationship between the cache ratio and GSM8K accuracy under two different conditions: \"No Delay\" and \"Delay.\" The x-axis represents the cache ratio, using a linear scale with values ranging from 0.2 to 0.5, and tick labels appear at regular intervals of 0.1. The y-axis represents GSM8K accuracy, also on a linear scale, with values ranging from 0 to 80 and tick labels at every 20 units. The \"No Delay\" condition is shown with a blue line featuring circular markers and a solid line style. The line begins at an accuracy of 70 at a cache ratio of 0.2 and declines sharply to about 5 as the cache ratio increases to 0.5. The \"Delay\" condition is depicted with a green line that has square markers and a solid line style. This line shows a relatively stable trend, starting at an accuracy of 77 at a cache ratio of 0.2 and ending at 78 at a cache ratio of 0.5, with only minor fluctuations."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.15781v1",
      "paper_title": "dKV-Cache The Cache for Diffusion Language Models",
      "figure_filename": "046_2505.15781v1_Figure4.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/046_2505.15781v1_Figure4.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/046_2505.15781v1_dKV-Cache The Cache for Diffusion Language Models.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/046_2505.15781v1_dKV-Cache The Cache for Diffusion Language Models.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "(left) and (right) on GSM8K with different settings: decoding length $L$, sampling steps $S$, refresh intervals and the window size.",
      "gpt_annotation": "The plot illustrates the relationship between speed, measured in tokens per second, and performance, measured as a percentage of accuracy, for various configurations. The x-axis is labeled \"Speed (Tokens/s)\" and uses a linear scale ranging from 0 to 160, with tick labels at intervals of 20. The y-axis is labeled \"Performance (Accuracy %)\" and uses a linear scale ranging from 60 to 90, with tick labels at intervals of 5. There are multiple lines present, each with distinctive styles and markers. A dashed gray line, labeled \"w/o Cache,\" shows a steady decline from approximately 83% accuracy at 10 tokens/s to about 66% at 170 tokens/s. The solid blue line with square markers labeled \"L=512, S=512\" begins at about 83% accuracy at 10 tokens/s and ends at approximately 80% accuracy at 50 tokens/s, showing a slight decline. Another solid blue line with circular markers labeled \"L=512, S=256\" starts at about 80% accuracy at 30 tokens/s and decreases to around 78% at 60 tokens/s. A solid blue line with triangular markers labeled \"L=512, S=128\" shows a uniform decline from about 76% accuracy at 50 tokens/s to 72% at 100 tokens/s. The solid red line with circular markers labeled \"L=128, S=64\" starts at around 72% accuracy at 60 tokens/s and decreases to approximately 66% at 120 tokens/s. The solid red line with square markers labeled \"L=128, S=32\" begins at about 66% accuracy at 80 tokens/s and follows a downward trend to 65% at 160 tokens/s. The plot also includes a dotted gray line representing the \"LLaDA Pareto Front\" that connects around 73% accuracy at 20 tokens/s to 72% at 120 tokens/s, and a dashed gray line representing the \"Cache Pareto Front,\" which declines from 83% at 20 tokens/s to 65% at 160 tokens/s."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.15781v1",
      "paper_title": "dKV-Cache The Cache for Diffusion Language Models",
      "figure_filename": "046_2505.15781v1_Figure8.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/046_2505.15781v1_Figure8.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/046_2505.15781v1_dKV-Cache The Cache for Diffusion Language Models.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/046_2505.15781v1_dKV-Cache The Cache for Diffusion Language Models.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Impact of batch size on decoding speed. Evaluated on LLaDA with a single NVIDIA H20; prefill length fixed at 100 tokens.",
      "gpt_annotation": "The plot illustrates the relationship between batch size and token processing speed measured in tokens per second for two decoding methods with varying parameter values of \\( L \\). The x-axis, labeled \"Batch Size,\" uses a linear scale ranging from 0 to 130 with tick labels at intervals of 20. The y-axis, labeled \"Token/s,\" also follows a linear scale, spanning from 0 to 225 with tick labels at intervals of 25. For the decoding method represented by solid lines, the blue line with circle markers represents \\( L = 16 \\), starting at approximately 34.53 tokens/s at a batch size of 0 and reaching 173.17 tokens/s at a batch size of 120, peaking at 202.31 tokens/s. The gray line with square markers for \\( L = 32 \\) begins at about 34.93 tokens/s and trends upwards to 175.95 tokens/s and peaks at 189.74 tokens/s. The red line with triangle markers for \\( L = 64 \\) begins at 45.69 tokens/s, climbs steadily to 126.86 tokens/s, and ends at 130.58 tokens/s. Finally, the black line with diamond markers for \\( L = 128 \\), starting at 46.99 tokens/s and achieving a peak of 158.24 tokens/s. For the method represented by dashed lines, the blue line with circle markers (L = 16) starts at 34.53 tokens/s, reaching around 59.71 tokens/s. The gray line with square markers, representing \\( L = 32 \\), begins at about 34.93 tokens/s, and reaches approximately 59.63 tokens/s. The red line with triangle markers, representing \\( L = 64 \\), starts at 45.69 tokens/s and ends at 59.41 tokens/s. The dark blue line with diamond markers, representing \\( L = 128 \\), begins at 46.99 tokens/s and ultimately reaches about 81.48 tokens/s."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.16538v1",
      "paper_title": "Mechanistic Understanding and Mitigation of Language Confusion in   English-Centric Large Language Models",
      "figure_filename": "047_2505.16538v1_Figure3.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/047_2505.16538v1_Figure3.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/047_2505.16538v1_Mechanistic Understanding and Mitigation of Language Confusion in   English-Centric Large Language M.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/047_2505.16538v1_Mechanistic Understanding and Mitigation of Language Confusion in   English-Centric Large Language M.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Distribution of Important Neurons Associated with Confusion Points in Llama3-8B. (a) Distribution of the top 300 most important FFN neurons across layers for an individual Chinese prompt \u201cUTF8gbsn\u8bf7\u89e3\u91ca\u62c6\u4e1c\u5899\u8865\u897f\u5899\u7684\u610f\u601d\u3002(Please explain `UTF8gbsn\u62c6\u4e1c\u5899\u8865\u897f\u5899.')\u201d from Aya. (b) Aggregated distribution of important neuron scores across all Chinese test samples in Aya.",
      "gpt_annotation": "The set of plots illustrates the variation in scores across different layers for individual and aggregated neuron cases. In both plots, the x-axis is labeled \"layer\" and uses a linear scale, ranging from 0 to 35 with tick marks at intervals of 5 units. The y-axis in each plot is labeled \"count\" and also follows a linear scale, with a range from 0 to 150 in the first plot and up to 175 in the second plot, with tick marks at intervals of 25 units. \n\nIn the first plot, labeled \"(a) Individual Case,\" the line is blue with circle markers and a solid style. It begins near a score of 0 at the layer 0 mark and maintains a relatively low and steady count across layers, rising above 20 at layer 27, and then sharply increases to approximately 100 by the final layer. In the second plot, labeled \"(b) Aggregated Neuron Scores,\" the line is green with triangle markers and a solid style. It similarly starts near a count of 0 at layer 0, staying low until around layer 28, where it begins to rise sharply, ending near 150 by the last layer."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.16415v2",
      "paper_title": "Attributing Response to Context A Jensen-Shannon Divergence Driven   Mechanistic Study of Context Attribution in Retrieval-Augmented Generation",
      "figure_filename": "048_2505.16415v2_Figure5.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/048_2505.16415v2_Figure5.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/048_2505.16415v2_Attributing Response to Context A Jensen-Shannon Divergence Driven   Mechanistic Study of Context At.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/048_2505.16415v2_Attributing Response to Context A Jensen-Shannon Divergence Driven   Mechanistic Study of Context At.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Computational efficiency comparison between our ARC-JSD and Contextcite ($n=256$ calls) across all datasets and RAG models. Each bar represents the average seconds per sample for ARC-JSD and Contextcite with different RAG models and datasets.",
      "gpt_annotation": "The bar chart titled \"Computational Efficiency: ARC-JSD vs. Contextcite\" illustrates the computational efficiency, measured in seconds per sample, of different models when applied to various question-answering tasks. The horizontal axis represents the categories labeled as models: Qwen2-1.5B-IT, Qwen2-7B-IT, Gemma2-2B-IT, and Gemma2-9B-IT in a linear scale. The vertical axis, with a label \"Seconds/Sample,\" also follows a linear scale ranging from 0 to 100 with tick intervals at 20-unit marks. Bars are grouped for each model and depict performance across six labeled tasks, distinguished by a legend with specific colors: dark blue for ARC-JSD - TyDi QA, light blue for ARC-JSD - Hotpot QA, medium blue for ARC-JSD - MuSiQue, dark orange for Contextcite - TyDi QA, orange for Contextcite - Hotpot QA, and light orange for Contextcite - MuSiQue. In each model group, the bars consistently show variations in height amongst the tasks; the shortest bars, typically at approximately 5 seconds, are for the Contextcite - TyDi QA across most models, while the tallest, reaching close to 90 seconds, is for the Contextcite - MuSiQue in the Gemma2-9B-IT model. Bars are ordered according to their task categories, not sorted by height, and there is no specific visual emphasis or annotation differentiating any bars beyond the task-based color coding."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.23851v1",
      "paper_title": "ASyMOB Algebraic Symbolic Mathematical Operations Benchmark",
      "figure_filename": "052_2505.23851v1_Figure2.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/052_2505.23851v1_Figure2.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/052_2505.23851v1_ASyMOB Algebraic Symbolic Mathematical Operations Benchmark.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/052_2505.23851v1_ASyMOB Algebraic Symbolic Mathematical Operations Benchmark.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "belowskip=0pt : Effect of equivalence-type perturbations. Note the substantial drop in success rate for most models, even when performance on the seed set is high.",
      "gpt_annotation": "The bar chart illustrates the success rates of different models across various equivalence categories. The horizontal axis contains the categorical labels, representing different model names such as \"Gemini 2.5 Flash (Code)\" and \"Nemotron-Super-v1,\" displayed in a tilted fashion for readability. The vertical axis represents the \"Success Rate (%)\" with a linear scale ranging from 0 to 100, marked at intervals of 20 percent. The chart uses different shades of blue to denote different equivalence categories as indicated by the legend: 'Equivalence-One-Easy', 'Equivalence-One-Hard', 'Equivalence-All-Easy', and 'Equivalence-All-Hard'. Each category for a given model is represented by a bar with varying heights, the darkest blue being the tallest for categories like 'Equivalence-One-Easy' in models such as 'DeepSeek-ProverV2', reaching approximately 100%, while lighter shades represent lower values, such as 'Equivalence-All-Hard' for several models which appear much shorter. The bars are stacked for each model without any particular order or emphasis, allowing a clear comparative view within each category without a specific sorting pattern. The data points for the tallest bars within some models emphasize high success rates."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.07899v1",
      "paper_title": "DeltaEdit Enhancing Sequential Editing in Large Language Models by   Controlling Superimposed Noise",
      "figure_filename": "057_2505.07899v1_Figure1.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/057_2505.07899v1_Figure1.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/057_2505.07899v1_DeltaEdit Enhancing Sequential Editing in Large Language Models by   Controlling Superimposed Noise.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/057_2505.07899v1_DeltaEdit Enhancing Sequential Editing in Large Language Models by   Controlling Superimposed Noise.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The changes in Efficacytop (the rate of successfully maximizing the object's probability) and noise_E with the number of edits. The top figure displays results for GPT2-XL using AlphaEdit and MEMIT, while the bottom figure shows results for LLaMA3-8B using only AlphaEdit. MEMIT is excluded for LLaMA3-8B due to its excessively high noise_E, making visualization difficult.",
      "gpt_annotation": "This plot illustrates the comparison of efficacy and noise over a given range for two different methods: AlphaEdit and MEMIT in the context of GPT2-XL. The x-axis is labeled with an unspecified parameter, with a linear scale ranging from 0 to 3000, with tick marks shown every 500 units. The left y-axis is labeled \"Efficacy\u209c\u2092\u209a,\" also with a linear scale, ranging from 0 to 100, with tick marks every 10 units. The right y-axis is labeled \"noise\u2091,\" ranging from 0 to 160, also on a linear scale, with tick marks every 20 units.\n\nFor AlphaEdit, the efficacy is represented by a dashed red line with square markers. It starts at 100 at x=0 and decreases slightly to 90 by x=3000. The noise for AlphaEdit is shown as a solid red line with circle markers, starting at 30 at x=0 and remaining constant across to x=3000. For MEMIT, efficacy is depicted with a dashed light blue line and square markers, beginning at 90 at x=0 and decreasing steadily to 30 at x=3000. The noise of MEMIT is represented by a solid light blue line with circle markers, starting at 0 at x=0 and increasing markedly to 140 by x=3000."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.07899v1",
      "paper_title": "DeltaEdit Enhancing Sequential Editing in Large Language Models by   Controlling Superimposed Noise",
      "figure_filename": "057_2505.07899v1_Figure2.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/057_2505.07899v1_Figure2.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/057_2505.07899v1_DeltaEdit Enhancing Sequential Editing in Large Language Models by   Controlling Superimposed Noise.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/057_2505.07899v1_DeltaEdit Enhancing Sequential Editing in Large Language Models by   Controlling Superimposed Noise.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The changes in Efficacytop (the rate of successfully maximizing the object's probability) and noise_E with the number of edits. The top figure displays results for GPT2-XL using AlphaEdit and MEMIT, while the bottom figure shows results for LLaMA3-8B using only AlphaEdit. MEMIT is excluded for LLaMA3-8B due to its excessively high noise_E, making visualization difficult.",
      "gpt_annotation": "The plot illustrates a comparison between two metrics labeled as \"AlphaEdit Efficacy\" and \"AlphaEdit noise\" for a model referred to as Llama3-8B. The x-axis represents an unnamed variable with a linear scale, labeled with values ranging from 0 to 3000, with tick marks at intervals of 500 units. The left y-axis, labeled \"Efficacy,\" is also on a linear scale, ranging from 80 to 100, with tick marks at intervals of 5 units. The right y-axis, labeled \"noise,\" has a linear scale ranging from 0 to 6 with tick marks at intervals of 1 unit. The first line, representing \"AlphaEdit Efficacy,\" is displayed in red with square markers and a dashed style. It starts at approximately 97 at x=0, decreases steadily, and reaches about 80 at x=3000. The second line, representing \"AlphaEdit noise,\" is shown in red with circular markers and a solid line style. This line starts at approximately 0 at x=0, increases steadily, and reaches about 5.5 at x=3000."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.07899v1",
      "paper_title": "DeltaEdit Enhancing Sequential Editing in Large Language Models by   Controlling Superimposed Noise",
      "figure_filename": "057_2505.07899v1_Figure3.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/057_2505.07899v1_Figure3.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/057_2505.07899v1_DeltaEdit Enhancing Sequential Editing in Large Language Models by   Controlling Superimposed Noise.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/057_2505.07899v1_DeltaEdit Enhancing Sequential Editing in Large Language Models by   Controlling Superimposed Noise.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The variation curve of k^\u22a4\u03b2 for AlphaEdit and MEMIT as the number of edits increases, with GPT2-XL serving as the edited model. As the results of Llama3-8B are not suitable for display in a line chart, they are presented in Appendix <ref>.",
      "gpt_annotation": "The plot illustrates the performance or characteristics of two methods labeled \"AlphaEdit\" and \"MEMIT\" across a range of values related to the model GPT2-XL. The x-axis represents an undefined quantity with a linear scale, ranging from 0 to 3000 with tick labels at intervals of 500 units. The y-axis depicts \\(kT \\beta\\) on a linear scale, ranging from 0 to 0.012, with tick labels at intervals of 0.002 units. The line representing \"AlphaEdit\" is red with circular markers and a solid line style. This line begins at approximately \\(kT \\beta = 0.004\\) when the x-value is 0 and shows a decreasing trend, ending near \\(kT \\beta = 0.002\\) at an x-value of 3000, indicating a general decline. The \"MEMIT\" line is displayed in blue with circular markers and a solid line style. It starts at roughly \\(kT \\beta = 0.004\\) when the x-value is 0 and exhibits an upward trend, finishing at approximately \\(kT \\beta = 0.012\\) at an x-value of 3000, indicating a steady increase across the plot."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.07899v1",
      "paper_title": "DeltaEdit Enhancing Sequential Editing in Large Language Models by   Controlling Superimposed Noise",
      "figure_filename": "057_2505.07899v1_Figure4.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/057_2505.07899v1_Figure4.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/057_2505.07899v1_DeltaEdit Enhancing Sequential Editing in Large Language Models by   Controlling Superimposed Noise.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/057_2505.07899v1_DeltaEdit Enhancing Sequential Editing in Large Language Models by   Controlling Superimposed Noise.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": labelformat=parens, labelsep=period",
      "gpt_annotation": "The plot illustrates the comparison of Efficacy and noise levels for two different methods, AlphaEdit and DeltaEdit, over a range of values. The x-axis is labeled with a numeric scale ranging from 0 to 3000 with a regular tick interval of 500. The primary y-axis on the left is labeled \"Efficacy\\(_{top}\\)\" with a linear scale ranging from 86 to 100, and tick marks are labeled at intervals of 2. The secondary y-axis on the right is labeled \"noise\\(_{E}\\)\" with a linear scale ranging from 2 to 12, with tick intervals of 2. The plot contains four lines in total. The first line represents AlphaEdit Efficacy\\(_{top}\\) as a red dashed line with square markers, starting at approximately 86 at the lowest x-value and peaking near 100 at the highest x-value. The second line depicts AlphaEdit noise\\(_{E}\\) with a solid red line and circular markers, starting around 2 and increasing steadily to approximately 12. The third line illustrates DeltaEdit Efficacy\\(_{top}\\) as a blue dashed line with square markers, beginning near 100 and showing a decreasing trend down to about 96. The final line shows DeltaEdit noise\\(_{E}\\) with a solid purple line and circular markers, beginning around 2 and rising to approximately 11 by the end of the x-axis range."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17374v1",
      "paper_title": "Chart-to-Experience Benchmarking Multimodal LLMs for Predicting   Experiential Impact of Charts",
      "figure_filename": "058_2505.17374v1_Figure2.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/058_2505.17374v1_Figure2.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/058_2505.17374v1_Chart-to-Experience Benchmarking Multimodal LLMs for Predicting   Experiential Impact of Charts.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/058_2505.17374v1_Chart-to-Experience Benchmarking Multimodal LLMs for Predicting   Experiential Impact of Charts.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The task page for crowdsourced online study",
      "gpt_annotation": "The line plot illustrates the percentage of veterans as a share of the population over time. The x-axis is labeled with years, ranging linearly from 1960 to 2010, with tick marks at ten-year intervals. The y-axis represents the percentage of veterans, with a linear scale ranging from 0% to 15%, with tick marks at 5% intervals. The plot features a single red line with circular markers and a solid line style. This line starts at 12.8% in 1960, rises to a peak of 13.8% in 1980, and then declines progressively, reaching 7% by 2010. There is a noticeable decline between 2000 and 2010, where the percentage drops from 9.4% to 7%."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.12474v1",
      "paper_title": "What are they talking about Benchmarking Large Language Models for   Knowledge-Grounded Discussion Summarization",
      "figure_filename": "064_2505.12474v1_Figure2.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/064_2505.12474v1_Figure2.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/064_2505.12474v1_What are they talking about Benchmarking Large Language Models for   Knowledge-Grounded Discussion S.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/064_2505.12474v1_What are they talking about Benchmarking Large Language Models for   Knowledge-Grounded Discussion S.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Paragraph retrieval ratios 0.90$\\%$ of LLMs. The majority of LLMs can be classified as either conservative ($ratio<30\\scalebox{0.90}{$%$}$) or open retrievers ($ratio>38\\scalebox{0.90}{$%$}$).",
      "gpt_annotation": "The bar chart illustrates the retrieval ratio percentages for various models, including an expert benchmark, aiming to represent and compare their performance in this metric. The horizontal axis contains the category labels, which are names of different models along with an \"Expert\" category, arranged in an unspecified order and displayed in varying colors for each bar. The vertical axis is labeled \"Retrieval Ratio (%)\" and uses a linear scale ranging from 0 to 50, with tick marks at intervals of 10%. Each bar is individually labeled with its exact percentage value at the top. The \"Expert\" category, shown in black, has a retrieval ratio of 34.11%, whereas \"DeepSeek V3,\" shown in beige, stands out with the highest value of 46.94%. The \"Claude 3.0pus\" model, in blue, has a retrieval ratio of 43.57%, and \"GLM-4-Plus,\" shaded in pink, also has a notable value of 43.58%. The bars vary in color without any indication of emphasis beyond their length representing different retrieval ratios. There is no apparent sorting, such as alphabetical or numerical, and no additional annotations or emphases are visually marked beyond the provided percentage values."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.12474v1",
      "paper_title": "What are they talking about Benchmarking Large Language Models for   Knowledge-Grounded Discussion Summarization",
      "figure_filename": "064_2505.12474v1_Figure3.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/064_2505.12474v1_Figure3.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/064_2505.12474v1_What are they talking about Benchmarking Large Language Models for   Knowledge-Grounded Discussion S.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/064_2505.12474v1_What are they talking about Benchmarking Large Language Models for   Knowledge-Grounded Discussion S.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Expert win rates 0.90$\\%$ in EBS feedback judgment. Our annotations achieve an average win rate of $88.89$0.90$\\%$. All $12$ LLMs exceed an $82$0.90$\\%$ rate in preferring our EBS.",
      "gpt_annotation": "The bar chart illustrates the expert win rates (%) for a series of models. The horizontal axis contains the categories labeled as various model names: GPT-4o, GPT-4-turbo, GPT-4o-mini, Claude 3 Opus, Claude 3.5 Sonnet, Claude 3.5 Haiku, Gemini 1.5 Pro, Llama 3.140B5, Mistral Large, DeepSeek v3, Qwen-Max, and GLM-4-Plus, with an additional label for \"Average\" shown in red and italicized. The vertical axis represents the expert win rate percentage, with a linear scale ranging from 0% to 100%, marked at intervals of 20%. Each bar represents a model, colored distinctly, with approximate values as follows: GPT-4o at 88.89%, GPT-4-turbo at 84.38%, GPT-4o-mini at 91.67%, Claude 3 Opus at 97.56%, Claude 3.5 Sonnet at 83.12%, Claude 3.5 Haiku at 88.46%, Gemini 1.5 Pro at 86.25%, Llama 3.140B5 at 87.04%, Mistral Large at 97.14%, DeepSeek v3 at 91.14%, Qwen-Max at 82.26%, and GLM-4-Plus at 92.54%. The average value of 85.07% is highlighted. The bars are neither grouped nor stacked and appear to be sorted based on their respective categories. No specific bars are visually emphasized beyond the use of distinct colors and the highlighting of the \"Average\" category in red."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.12474v1",
      "paper_title": "What are they talking about Benchmarking Large Language Models for   Knowledge-Grounded Discussion Summarization",
      "figure_filename": "064_2505.12474v1_Figure7.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/064_2505.12474v1_Figure7.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/064_2505.12474v1_What are they talking about Benchmarking Large Language Models for   Knowledge-Grounded Discussion S.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/064_2505.12474v1_What are they talking about Benchmarking Large Language Models for   Knowledge-Grounded Discussion S.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Overall performance 0.90$\\%$ of all LLMs in the EBS-AOS pattern. The stratification is divided into three levels: Tier-1 ($\\text{OP}_{GM} \\in [72, 77]$), Tier-2 ($\\text{OP}_{GM} \\in [64, 68]$), and Tier-3 ($\\text{OP}_{GM} \\in [34, 46]$).",
      "gpt_annotation": "The bar chart illustrates overall performance percentages of unspecified categories. The vertical axis represents \"Overall Performance (%)\" with a linear scale ranging from 0 to 80, marked at intervals of 20. The horizontal axis contains categorical labels, but specific labels are not shown in the image. There are ten bars, each with a distinct color. The first bar, in green, has a value of 76.24%. The second bar, which is blue, shows 73.94%, while a teal bar follows with 34.24%. The fourth bar, pink in color, indicates a value of 72.09%. The fifth bar has a purplish hue with the highest value of 77.12%. The sixth bar, brown, shows 46.02%. A light blue bar follows with 76.71%. The eighth bar, light purple, shows 67.58%, and the ninth bar, a tan color, marks 63.63%. The last two bars have very similar heights, with the gray bar marking 66.50% and the peach-colored bar at 64.34%, and the final pastelly red bar at 67.55%. The chart appears to order the bars without a clear alphabetical or numerical sequence and does not visually emphasize any bar with unique shading or emphasis."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.12474v1",
      "paper_title": "What are they talking about Benchmarking Large Language Models for   Knowledge-Grounded Discussion Summarization",
      "figure_filename": "064_2505.12474v1_Figure8.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/064_2505.12474v1_Figure8.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/064_2505.12474v1_What are they talking about Benchmarking Large Language Models for   Knowledge-Grounded Discussion S.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/064_2505.12474v1_What are they talking about Benchmarking Large Language Models for   Knowledge-Grounded Discussion S.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Overall performance gap 0.90$\\%$ of LLMs between the two KGDS patterns. A larger gap indicates lower stability in cross-pattern performance.",
      "gpt_annotation": "The bar chart illustrates the performance gap percentage for various categories, with the vertical axis labeled as \"Performance Gap (%)\" and a linear scale ranging from 0 to 30, marked at intervals of 5. The horizontal axis contains unlabeled categories ordered from left to right. Colors differentiate the bars, each representing a distinct category. The chart features ten bars with the following approximate values and colors: the first bar is green with a value of 15.15%, the second is purple at 25.66%, the third is teal at 9.60%, the fourth is mauve at 11.37%, the fifth is blue-gray at 28.29%, the sixth is brown at 18.80%, the seventh is light blue at 21.88%, the eighth is lavender at 23.79%, the ninth is beige at 17.27%, and the tenth is gray at 22.41%. The eleventh and twelfth are light green and pink at 23.47% and 24.01%, respectively. The bars appear to be sorted neither alphabetically nor by height, and no specific bar is visually emphasized through techniques such as bolding, shading, or annotations."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.12474v1",
      "paper_title": "What are they talking about Benchmarking Large Language Models for   Knowledge-Grounded Discussion Summarization",
      "figure_filename": "064_2505.12474v1_Figure9.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/064_2505.12474v1_Figure9.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/064_2505.12474v1_What are they talking about Benchmarking Large Language Models for   Knowledge-Grounded Discussion S.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/064_2505.12474v1_What are they talking about Benchmarking Large Language Models for   Knowledge-Grounded Discussion S.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Overall performance 0.90$\\%$ of all LLMs in the ABS-AOS pattern. The stratification is divided into three levels: Tier-1 ($\\text{OP}_{GM} \\in [55, 61]$), Tier-2 ($\\text{OP}_{GM} \\in [41, 49]$), and Tier-3 ($\\text{OP}_{GM} \\in [25, 27]$).",
      "gpt_annotation": "The bar chart illustrates overall performance percentages across different categories. The horizontal axis presents the categories in sequential labels without a specified order. The vertical axis represents the overall performance percentage, ranging from 0 to 60% with a linear scale and tick labels at regular intervals of 10 percentage points. The bars are depicted in various colors, each associated with a specific category. The first bar is dark green with a value of 61.09%, and the second bar is dark blue, showing a value of 48.28%. The third bar is teal with a value of 24.64%, followed by a purple bar at 60.72%. The fifth category is dark blue with a value of 48.83%, with the sixth brown bar representing 27.22%. The subsequent bars are blue at 54.83%, light blue at 43.79%, beige at 46.36%, gray at 44.09%, light gray at 40.87%, and finally, a light pink bar at 43.54%. The tallest bar is the first, at 61.09%, and the shortest is the sixth, at 27.22%. The bars do not appear to be sorted in a specific order, nor are there visual emphases such as shading or outlines on particular bars."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.12474v1",
      "paper_title": "What are they talking about Benchmarking Large Language Models for   Knowledge-Grounded Discussion Summarization",
      "figure_filename": "064_2505.12474v1_Figure10.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/064_2505.12474v1_Figure10.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/064_2505.12474v1_What are they talking about Benchmarking Large Language Models for   Knowledge-Grounded Discussion S.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/064_2505.12474v1_What are they talking about Benchmarking Large Language Models for   Knowledge-Grounded Discussion S.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Average overall performance 0.90$\\%$ of all LLMs across both KGDS patterns. The stratification is divided into three levels: Tier-1 ($\\text{OP}_{\\text{avg}} \\in [61, 69]$), Tier-2 ($\\text{OP}_{\\text{avg}} \\in [53, 56]$), and Tier-3 ($\\text{OP}_{\\text{avg}} \\in [29, 37]$).",
      "gpt_annotation": "The bar chart illustrates the average performance represented in percentage points across various categories, with the performance data displayed vertically. The horizontal axis contains the categories, indicated by different colors for each bar, but without specific labels. The vertical axis is labeled \"Average Performance (%)\" and utilizes a linear scale ranging from 0 to 80 with intervals at every 20 units. The chart features individual bars, each differ in color and height, representing approximate values of average performance: the first bar, colored green, shows the highest value at 68.66%, while the shortest appears in dark blue at 29.44%. Other notable bars include one in pink at 66.41%, blue at 65.77%, and beige at 62.98%. The values for other bars range between 36.62% and 61.11% with no apparent specific sorting order or visual emphasis such as shading or outlining. Each bar is annotated at the top with its exact performance percentage, enhancing clarity and readability."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.10113v2",
      "paper_title": "What Does Neuro Mean to Cardio Investigating the Role of Clinical   Specialty Data in Medical LLMs",
      "figure_filename": "067_2505.10113v2_Figure2.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/067_2505.10113v2_Figure2.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/067_2505.10113v2_What Does Neuro Mean to Cardio Investigating the Role of Clinical   Specialty Data in Medical LLMs.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/067_2505.10113v2_What Does Neuro Mean to Cardio Investigating the Role of Clinical   Specialty Data in Medical LLMs.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Negative log-probabilities for clinically relevant tokens between baseline Mistral-v0.2 and the same model further fine-tuned on each specialty data. Each group represents tokens categorized into different clinical specialties. Each color means that the same model is further fine-tuned on each specialty data.",
      "gpt_annotation": "The bar chart illustrates the negative log probability across various medical specialties when the MistralV0.2-Base model is fine-tuned on different categories. The horizontal axis contains the categories labeled as \"Cardio,\" \"Gastro,\" \"Infect,\" \"Neuro,\" \"Obstetrics,\" and \"Pediatrics,\" arranged from left to right. The vertical axis represents the negative log probability, with values ranging from -27 to -22 on a linear scale, marked at intervals of 1 unit. Each group of bars represents a different specialty term and consists of bars for the base model and each fine-tuning category: \"Cardio,\" \"Gastro,\" \"Infect,\" \"Neuro,\" \"Obstetrics,\" and \"Pediatrics,\" with corresponding colors in shades of brown and a light blue for the base model. The bars within each category set show distinct heights. For example, the \"Cardio\" group features a light blue bar reaching approximately -26, with the other brown shades ranging higher around -22 to -23. No specific sorting method is apparent other than category grouping, and there are no visually emphasized bars beyond color-coding for distinction."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.18610v1",
      "paper_title": "PM-KVQ Progressive Mixed-precision KV Cache Quantization for Long-CoT   LLMs",
      "figure_filename": "068_2505.18610v1_Figure3.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/068_2505.18610v1_Figure3.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/068_2505.18610v1_PM-KVQ Progressive Mixed-precision KV Cache Quantization for Long-CoT   LLMs.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/068_2505.18610v1_PM-KVQ Progressive Mixed-precision KV Cache Quantization for Long-CoT   LLMs.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": DeepSeek-R1-Distill-LLaMA-8B",
      "gpt_annotation": "The bar chart illustrates sensitivity values across different block indices, with distinct values represented for two configurations: 8-Fbit and 4-Fbit. The horizontal axis, labeled \"Block Index,\" categorizes the data sequentially from 0 to 31 at regular intervals. The vertical axis is labeled \"Sensitivity\" and employs a logarithmic scale ranging from \\(10^{-6}\\) to \\(10^{-4}\\), with tick labels at each order of magnitude between these endpoints. Each block index has two associated bars: one blue, representing \"8-Fbit,\" and one green, representing \"4-Fbit.\" The blue bars, located primarily toward the right side of the chart, depict higher sensitivity values compared to the green bars on the left, which are more evenly distributed in height. The shortest green bar represents a sensitivity around \\(10^{-6}\\), while the tallest blue bar reaches a sensitivity close to \\(10^{-4}\\). This configuration is consistently applied without additional visual emphasis or sorting beyond the grouped structure, clarified by the legend in the upper left of the chart."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.18610v1",
      "paper_title": "PM-KVQ Progressive Mixed-precision KV Cache Quantization for Long-CoT   LLMs",
      "figure_filename": "068_2505.18610v1_Figure4.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/068_2505.18610v1_Figure4.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/068_2505.18610v1_PM-KVQ Progressive Mixed-precision KV Cache Quantization for Long-CoT   LLMs.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/068_2505.18610v1_PM-KVQ Progressive Mixed-precision KV Cache Quantization for Long-CoT   LLMs.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": DeepSeek-R1-Distill-Qwen-7B",
      "gpt_annotation": "The bar chart illustrates the sensitivity of different blocks, represented by their block index, under two conditions labeled as \"4-Fbit\" and \"2-Fbit.\" The horizontal axis contains the categories, labeled as \"Block Index,\" with indices ranging from 0 to 27. The vertical axis represents the \"Sensitivity,\" displayed on a logarithmic scale with a range from \\(10^{-5}\\) to \\(10^{-3}\\), with tick labels at \\(10^{-5}\\), \\(10^{-4}\\), and \\(10^{-3}\\). The chart features grouped bars with each group pertaining to a block index, showcasing two bars per group. The first bar in each group represents the \"4-Fbit\" data in blue, while the second bar represents the \"2-Fbit\" data in green. The approximate values vary, with the tallest \"4-Fbit\" bar at block index 27 reaching close to \\(10^{-3}\\), while the tallest \"2-Fbit\" bar is at block index 0, slightly under \\(10^{-4}\\). The bars are arranged in numerical order by block index, with no special visual emphasis provided to any bars. A legend is positioned in the upper left region of the chart to clarify the color coding for each condition."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.18610v1",
      "paper_title": "PM-KVQ Progressive Mixed-precision KV Cache Quantization for Long-CoT   LLMs",
      "figure_filename": "068_2505.18610v1_Figure5.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/068_2505.18610v1_Figure5.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/068_2505.18610v1_PM-KVQ Progressive Mixed-precision KV Cache Quantization for Long-CoT   LLMs.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/068_2505.18610v1_PM-KVQ Progressive Mixed-precision KV Cache Quantization for Long-CoT   LLMs.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": DeepSeek-R1-Distill-Qwen-14B",
      "gpt_annotation": "The bar chart illustrates the sensitivity of various blocks indexed along a timeline, contrasting two different conditions labeled as \"4-Fbit\" and \"2-Fbit.\" The categories are displayed along the horizontal axis labeled as \"Block Index,\" which is listed incrementally with index markers at positions 0, 11, 23, 35, and 47. The value axis, labeled \"Sensitivity,\" is oriented vertically and uses a logarithmic scale with values ranging from \\(10^{-5}\\) to \\(10^{-4}\\), marked by horizontal grid lines to assist with visualization. This chart displays two distinct sets of bars represented by a legend at the top left: green bars for \"2-Fbit\" and blue bars for \"4-Fbit.\" Bars are grouped per block index. The \"2-Fbit\" bars appear mostly in green with varying heights, the shortest being around \\(10^{-5}\\), prominently observable towards the start of the index, and gradually rising across the middle sections. The \"4-Fbit\" bars, colored in blue, begin from index 35 and consistently rise towards the end of the index axis, reaching heights near \\(10^{-4}\\). Bars are neither in alphabetical nor numerical order but are rather grouped by index, and there is no additional emphasis or annotation on any specific bar."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.18610v1",
      "paper_title": "PM-KVQ Progressive Mixed-precision KV Cache Quantization for Long-CoT   LLMs",
      "figure_filename": "068_2505.18610v1_Figure6.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/068_2505.18610v1_Figure6.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/068_2505.18610v1_PM-KVQ Progressive Mixed-precision KV Cache Quantization for Long-CoT   LLMs.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/068_2505.18610v1_PM-KVQ Progressive Mixed-precision KV Cache Quantization for Long-CoT   LLMs.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": DeepSeek-R1-Distill-Qwen-32B",
      "gpt_annotation": "The bar chart illustrates the sensitivity values for different block indices represented by two categories: \"4-Fbit\" and \"2-Fbit.\" The horizontal axis represents the Block Index, ranging from 0 to 63 with tick marks at intervals of 15. The vertical axis represents Sensitivity on a logarithmic scale, with values ranging from slightly below \\(10^{-5}\\) to above \\(10^{-4}\\), with tick marks at \\(10^{-5}\\) and \\(10^{-4}\\). There are two sets of bars at each block index: blue bars labeled \"4-Fbit\" and green bars labeled \"2-Fbit.\" The bars display varying heights, with \"4-Fbit\" bars generally appearing taller as the block index increases, particularly noticeable from indices 47 to 63. The \"2-Fbit\" bars mostly exhibit shorter heights throughout the range. The colors blue and green are used to differentiate between the two categories as explained in the legend. The bars are sorted by block indices in ascending order, and no additional visual emphasis such as shading or bold outlines is used to highlight any specific bars."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.18610v1",
      "paper_title": "PM-KVQ Progressive Mixed-precision KV Cache Quantization for Long-CoT   LLMs",
      "figure_filename": "068_2505.18610v1_Figure7.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/068_2505.18610v1_Figure7.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/068_2505.18610v1_PM-KVQ Progressive Mixed-precision KV Cache Quantization for Long-CoT   LLMs.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/068_2505.18610v1_PM-KVQ Progressive Mixed-precision KV Cache Quantization for Long-CoT   LLMs.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": DeepSeek-R1-Distill-QwQ-32B",
      "gpt_annotation": "The bar chart illustrates the sensitivity values for different block indices, depicting two categories labeled as \"4-Fbit\" and \"2-Fbit.\" The horizontal axis represents the \"Block Index,\" with the indices ordered sequentially from 0 to 63. The vertical axis represents \"Sensitivity,\" using a logarithmic scale ranging from \\(10^{-5}\\) to \\(10^{-4}\\), with grid lines marking these values at regular intervals. The chart includes two groups of bars: each block index contains either a single green bar representing \"2-Fbit\" or a single blue bar representing \"4-Fbit.\" The green bars, representing \"2-Fbit,\" appear in the lower half of the range with varying heights, generally shorter than the blue bars. The blue bars, representing \"4-Fbit,\" are taller and dominate the higher values in the range above \\(10^{-5}\\). There is no particular sorting visible among the bars besides the sequential order of the block indices. The colors are used consistently as per the legend, with no additional visual emphasis on any specific bar."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.18610v1",
      "paper_title": "PM-KVQ Progressive Mixed-precision KV Cache Quantization for Long-CoT   LLMs",
      "figure_filename": "068_2505.18610v1_Figure8.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/068_2505.18610v1_Figure8.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/068_2505.18610v1_PM-KVQ Progressive Mixed-precision KV Cache Quantization for Long-CoT   LLMs.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/068_2505.18610v1_PM-KVQ Progressive Mixed-precision KV Cache Quantization for Long-CoT   LLMs.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": DeepSeek-R1-Distill-LLaMA-70B",
      "gpt_annotation": "The bar chart illustrates the sensitivity of two different types of blocks labeled as \"4-Fbit\" and \"2-Fbit\" across varying block indices. The horizontal axis represents the block index, labeled as \"Block Index,\" with a range from 0 to approximately 79, marked at intervals of about 20 units. The vertical axis represents the sensitivity, labeled as \"Sensitivity,\" with a logarithmic scale ranging from 10^-6 to 10^-4, featuring major tick labels at 10^-6, 10^-5, and 10^-4. There are two distinct sets of bars, one colored blue representing \"4-Fbit\" and the other green representing \"2-Fbit.\" Each block index features a pair of bars, blue and green, indicating the sensitivity for \"4-Fbit\" and \"2-Fbit,\" respectively. Generally, for block indices around 0 to 59, the green \"2-Fbit\" bars are more prevalent, while from index 60 onwards, the blue \"4-Fbit\" bars are more prominent. The blue bars reach the highest sensitivity value close to 10^-4 at the highest block indices, whereas the green bars have sensitivity values mostly around 10^-6. There is no specific sort order visually emphasized for the bars, and they are primarily distinguished by color with no additional shading or outlining characteristics."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00871v1",
      "paper_title": "Towards Predicting Any Human Trajectory In Context",
      "figure_filename": "069_2506.00871v1_Figure3.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/069_2506.00871v1_Figure3.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance of random example selection and the proposed STES at varying numbers of in-context examples.",
      "gpt_annotation": "The plot illustrates the relationship between the number of examples and the minADE\u2088\u2080 (unitless) for two data sets labeled \"ST-ES\" and \"Random.\" The x-axis is labeled \"Number of Examples\" and has a linear scale ranging from 0 to 8 with tick marks at each integer. The y-axis is labeled \"minADE\u2082\u2080 (\u2192)\" and also uses a linear scale, ranging from 15.0 to 18.0, with tick marks at intervals of 0.5 units. The blue line, representing \"ST-ES,\" is marked with circular points and a solid line style. Initially at a value of around 17.0 at 0 examples, it decreases sharply to about 15.5 at 1 example before gradually declining to approximately 15.2 by 8 examples. The orange line, representing \"Random,\" similarly employs circular markers and a solid line style. Beginning at approximately 18.0 at 0 examples, it decreases to around 17.0 at 1 example and then moves slightly downward, staying near 16.7 from 2 to 8 examples. Both lines display distinct initial declines with subsequent steadier trends."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00871v1",
      "paper_title": "Towards Predicting Any Human Trajectory In Context",
      "figure_filename": "069_2506.00871v1_Figure4.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/069_2506.00871v1_Figure4.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance of random example selection and the proposed STES at varying numbers of in-context examples.",
      "gpt_annotation": "The plot illustrates a comparison of two methods based on the metric labeled as minFDE\\(_{20}\\) as a function of the number of examples. The x-axis is labeled \"Number of Examples\" and has a linear scale, ranging from 0 to 8 with tick marks at intervals of 1 unit. The y-axis is labeled \"minFDE\\(_{20}\\) (\u2190)\" and also has a linear scale, ranging from 16 to 24, with tick marks at intervals of 1 unit. There are two lines in the plot. The blue line, representing \"ST-ES,\" has circular markers and a solid line style. It starts at approximately 24 at 0 examples and decreases to about 17 at 8 examples, showing a noticeable decline, particularly sharp between 0 and 2 examples. The orange line, representing \"Random,\" also features circular markers and a solid line style. It begins at around 24 at 0 examples and decreases to approximately 21 at 8 examples, showing a more gradual decline compared to the blue line, with a relatively steady trend after 2 examples."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00871v1",
      "paper_title": "Towards Predicting Any Human Trajectory In Context",
      "figure_filename": "069_2506.00871v1_Figure5.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/069_2506.00871v1_Figure5.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance of random example selection and the proposed STES at varying numbers of in-context examples.",
      "gpt_annotation": "The plot illustrates the relationship between the number of examples and minADE\u2084\u2080. The x-axis, labeled as \"Number of Examples,\" has a linear scale ranging from 0 to 8 with tick marks at regular intervals of 1. The y-axis, labeled as \"minADE\u2082\u2080 (\u2190),\" also uses a linear scale with a value range from 0.59 to 0.66 and tick marks at intervals of 0.01. The plot features two lines. The blue line with circular markers and a solid style represents \"ST-ES.\" It starts at approximately 0.66 at zero examples and shows a sharp decline to about 0.60 by the first data point, followed by a slight fluctuation between 0.60 and 0.62 across the x-axis. The orange line with circular markers and a solid style represents \"Random.\" This line begins at around 0.64 and initially rises slightly, peaking near 0.645, then gradually descends to approximately 0.635, and exhibits minor fluctuations across the remainder of the x-axis."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00871v1",
      "paper_title": "Towards Predicting Any Human Trajectory In Context",
      "figure_filename": "069_2506.00871v1_Figure6.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/069_2506.00871v1_Figure6.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance of random example selection and the proposed STES at varying numbers of in-context examples.",
      "gpt_annotation": "The plot illustrates the changes in minFDE\u209a\u2080 values with respect to the number of examples for two different methods. The x-axis is labeled \"Number of Examples,\" using a linear scale, ranging from 0 to 8, with tick labels at intervals of 1. The y-axis is labeled \"minFDE\u2082\u2080 (\u2192),\" also using a linear scale, ranging from 0.85 to 1.05, with tick labels at intervals of 0.05. The first line, representing \"ST-ES,\" is in blue, marked with circle markers and a solid line style. It starts at approximately 1.05 when the number of examples is 0, then steadily decreases, reaching a value close to 0.88 at 8 examples. The second line, representing \"Random,\" is in orange, also with circle markers and a solid line style. It begins at a value slightly above 1.05 and decreases to around 0.98 with 1 example, then fluctuates slightly between values of 0.98 and 1.01 as the number of examples progresses to 8."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00871v1",
      "paper_title": "Towards Predicting Any Human Trajectory In Context",
      "figure_filename": "069_2506.00871v1_Figure9.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/069_2506.00871v1_Figure9.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance of random example selection and the proposed STES at varying numbers of in-context examples.",
      "gpt_annotation": "The plot illustrates the relationship between the number of examples and the minADE\u209a\u2080 metric across two methods. The x-axis is labeled \"Number of Examples,\" uses a linear scale ranging from 0 to 8, with tick marks at intervals of 1. The y-axis is labeled \"minADE\u2082\u2080 (\u2190),\" also on a linear scale, ranging from 21.00 to 24.00 with tick marks at intervals of 0.50. The blue line, representing the \"ST-ES\" method, uses circular markers and a solid line style. It starts at approximately 24.00 at 0 examples and decreases steadily to around 21.00 at 8 examples, with a notable downward trend. The orange line, representing the \"Random\" method, also uses circular markers and a solid line style, starting at about 24.00 at 0 examples and showing a slight decrease to near 23.50 at 8 examples, displaying a relatively stable trend with minor fluctuations."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00871v1",
      "paper_title": "Towards Predicting Any Human Trajectory In Context",
      "figure_filename": "069_2506.00871v1_Figure10.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/069_2506.00871v1_Figure10.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance of random example selection and the proposed STES at varying numbers of in-context examples.",
      "gpt_annotation": "This plot illustrates the relationship between the percentage of the in-context example pool and the minFDE\\(_{20}\\) values. The x-axis is labeled \"Percentage of In-Context Example Pool\" and has a linear scale ranging from 0.0 to 1.0, with tick labels at intervals of 0.2. The y-axis is labeled \"minFDE\\(_{20}\\) (\u2194)\" and also uses a linear scale, ranging from 20 to 30, with tick labels at intervals of 2. The plot features two lines: a blue line representing TrajICL and an orange line representing Full FT. The blue line, with circular markers and a solid style, starts just above 22 at 0.0 on the x-axis and gradually decreases to about 21 at 1.0, showing a general downward trend with minor fluctuations. The orange line, also with circular markers and a solid style, starts at approximately 28.5, decreases slightly, rises around 0.2 to 0.4, peaking close to 29, and then steadily declines to about 25 at 1.0."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00871v1",
      "paper_title": "Towards Predicting Any Human Trajectory In Context",
      "figure_filename": "069_2506.00871v1_Figure11.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/069_2506.00871v1_Figure11.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance of random example selection and the proposed STES at varying numbers of in-context examples.",
      "gpt_annotation": "The plot illustrates the relationship between the number of examples and the minADE\u2081\u2080 score. The x-axis represents the \"Number of Examples\" with a linear scale ranging from 0 to 8, with tick marks at every whole number interval. The y-axis represents \"minADE\u2082\u2080 (\u2193)\" with a linear scale, ranging from 8.80 to 10.20, with tick marks at intervals of 0.20. The plot contains two lines: a blue line with circular markers and a solid line style representing \"ST-ES,\" and an orange line with circular markers and a solid line style representing \"Random.\" The blue line for \"ST-ES\" starts at a y-value of approximately 10.20 at x=0 and decreases sharply to around 9.20 by x=2, then continues to decline more gradually to around 8.88 at x=8. The orange line for \"Random\" starts at approximately 9.80 at x=0, decreases slightly to around 9.40 by x=3, and then remains relatively stable, with a slight increase to about 9.60 at x=8."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00871v1",
      "paper_title": "Towards Predicting Any Human Trajectory In Context",
      "figure_filename": "069_2506.00871v1_Figure12.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/069_2506.00871v1_Figure12.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance of random example selection and the proposed STES at varying numbers of in-context examples.",
      "gpt_annotation": "The plot illustrates the relationship between the number of examples and the minimum final displacement error (\\( \\text{minFDE}_{20} \\)). The x-axis is labeled \"Number of Examples,\" with a linear scale ranging from 0 to 8, and tick labels at intervals of 1 unit. The y-axis is labeled \"\\(\\text{minFDE}_{20}\\) (\u2190),\" also on a linear scale, ranging from 15.00 to 19.00, with tick intervals of 0.50 units. The plot includes two lines: a blue line for \"ST-ES\" and an orange line for \"Random.\" The blue line, representing \"ST-ES,\" uses circular markers and is solid. It starts at approximately 19.00 when the number of examples is 0 and decreases steadily to about 15.50 at 8 examples, showing a declining trend. The orange line, representing \"Random,\" also uses circular markers and is solid. It starts at around 18.50 with 0 examples, declines to roughly 17.00 by 3 examples, and then fluctuates slightly, ending around 17.50 at 8 examples."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00871v1",
      "paper_title": "Towards Predicting Any Human Trajectory In Context",
      "figure_filename": "069_2506.00871v1_Figure13.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/069_2506.00871v1_Figure13.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance of random example selection and the proposed STES at varying numbers of in-context examples.",
      "gpt_annotation": "The plot illustrates the relationship between the number of examples and minADE\\(_{20}\\) values, comparing two methods labeled ST-ES and Random. The x-axis is labeled \"Number of Examples\" and follows a linear scale, ranging from 0 to 8 with tick labels at intervals of 1. The y-axis is labeled \"minADE\\(_{20}\\) (\u2193)\" and also uses a linear scale, ranging from 0.12 to 0.14 with tick labels at 0.01 intervals. The blue line, representing ST-ES, is marked with circles and is solid in style. It starts at approximately 0.14 at the x-value of 0, decreases to about 0.128 by x-value 2, fluctuates slightly, and ends at approximately 0.131 when x is 8. The orange line for Random is also marked with circles and is solid. It begins at around 0.141 at x-value 0, decreases to approximately 0.131 at x-value 1, then gradually increases, showing a peak at x=6 with a value near 0.136, and finally reaches around 0.137 at x-value 8."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00871v1",
      "paper_title": "Towards Predicting Any Human Trajectory In Context",
      "figure_filename": "069_2506.00871v1_Figure14.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/069_2506.00871v1_Figure14.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance of random example selection and the proposed STES at varying numbers of in-context examples.",
      "gpt_annotation": "The plot illustrates the relationship between the number of examples and the minFDE\\(_{20}\\) value for different methods. The x-axis is labeled \"Number of Examples,\" has a linear scale, ranges from 0 to 8, and has tick labels at intervals of 1. The y-axis is labeled \"minFDE\\(_{20}\\) (\u2192),\" also on a linear scale, ranging from 0.20 to 0.24, with tick labels at intervals of 0.01. The plot features two lines: the first line represents \"ST-ES,\" shown in blue with circular markers and a solid line style. It starts at approximately (0, 0.24), decreases to around (4, 0.21), and ends at (8, 0.21), showing a downward trend initially with a slight curve upwards at the end. The second line represents \"Random,\" depicted in orange with circular markers and a solid line style. It begins at about (0, 0.23), fluctuates slightly, reaching a dip near (5, 0.22), and rises to approximately (8, 0.23), maintaining a steady trend with minor oscillations."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00871v1",
      "paper_title": "Towards Predicting Any Human Trajectory In Context",
      "figure_filename": "069_2506.00871v1_Figure15.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/069_2506.00871v1_Figure15.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance of random example selection and the proposed STES at varying numbers of in-context examples.",
      "gpt_annotation": "The plot illustrates the relationship between the \"Number of Examples\" and \"minADE\u2081\u2080\" values for two different methods. The x-axis is labeled \"Number of Examples,\" has a linear scale, ranges from 0 to 8, and uses integer tick intervals. The y-axis is labeled \"minADE\u2082\u2080 (\u2190),\" also has a linear scale, ranges from 2.55 to 2.85, and uses tick intervals of 0.05. The plot contains two lines. The blue line, labeled \"ST-ES,\" has circular markers, a solid line style, and begins at approximately 2.85 at x=0, descends to around 2.58 at x=1, and then shows a slight upward trend to about 2.63 at x=8. The orange line, labeled \"Random,\" also has circular markers and a solid line style, starting at around 2.85 at x=0, decreasing to about 2.76 at x=2, and then fluctuating slightly between 2.75 and 2.81 until x=8."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00871v1",
      "paper_title": "Towards Predicting Any Human Trajectory In Context",
      "figure_filename": "069_2506.00871v1_Figure16.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/069_2506.00871v1_Figure16.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance of random example selection and the proposed STES at varying numbers of in-context examples with a larger number of examples",
      "gpt_annotation": "The plot illustrates the relationship between the number of examples and the minADE\\(_{20}\\) measure. The x-axis is labeled \"Number of Examples\" and employs a linear scale, ranging from 0 to 15 with tick labels at intervals of 5. The y-axis is labeled \"minADE\\(_{20}\\) (\u2194)\" and also uses a linear scale, ranging from 14.50 to 17.00, with tick labels at intervals of 0.50. There are two lines depicted on the plot. The first line is colored blue with circular markers and a solid line style, representing the \"ST-ES\" data. It starts at a y-value of 17.00 at the x-value of 0 and declines sharply to approximately 15.10 by x-value 2, then continues to gradually decrease and flattens towards the x-axis, reaching around 14.75 at x-value 15. The second line is orange with circular markers and a solid line style, representing the \"Random\" data. It starts at a y-value of around 16.50 at x-value 0, remains relatively steady with minor fluctuations, and ends at a similar value of approximately 16.50 at x-value 15."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00871v1",
      "paper_title": "Towards Predicting Any Human Trajectory In Context",
      "figure_filename": "069_2506.00871v1_Figure17.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/069_2506.00871v1_Figure17.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance of random example selection and the proposed STES at varying numbers of in-context examples with a larger number of examples",
      "gpt_annotation": "The plot illustrates the relationship between the number of examples and minFDE20 values for two methods, ST-ES and Random. The x-axis is labeled \"Number of Examples\" and uses a linear scale, ranging from 0 to 15, with tick marks at intervals of 5 units. The y-axis is labeled \"minFDE20 (\u2193)\" with a linear scale as well, ranging from 16.00 to 22.00, with tick marks at intervals of 1 unit. The plot includes two lines. The blue line with circular markers, representing ST-ES, starts with a value of 22.00 at 0 examples and rapidly decreases to around 17.50 at 2 examples, then gradually declines, reaching approximately 16.00 at 15 examples. The orange line with circular markers, representing Random, begins at approximately 21.50 at 1 example and shows a slight decreasing trend, finishing at around 20.50 at 15 examples, showing minor fluctuations. Both lines demonstrate their respective trends across the plot without intersecting."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00871v1",
      "paper_title": "Towards Predicting Any Human Trajectory In Context",
      "figure_filename": "069_2506.00871v1_Figure18.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/069_2506.00871v1_Figure18.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance of random example selection and the proposed STES at varying numbers of in-context examples with a larger number of examples",
      "gpt_annotation": "The plot illustrates the relationship between the number of examples and the minimum average displacement error (minADE) for two different methods. The x-axis is labeled \"Number of Examples\" and is on a linear scale, ranging from 0 to 15 with tick labels at intervals of 2. The y-axis is labeled \"minADE\u2080\u2082\u2080 (\u2190)\" and is also on a linear scale, ranging from 0.56 to 0.66 with tick labels at intervals of 0.02. The plot includes two lines. The blue line, representing \"ST-ES,\" uses circle markers and a solid line style. It starts at approximately (0, 0.66) and exhibits a rapid decline to about (2, 0.58), followed by fluctuations and a gradual decrease, ending near (15, 0.58). The orange line, representing \"Random,\" also uses circle markers and a solid line style. It begins around (0, 0.64) and remains relatively stable with minor fluctuations, ending near (15, 0.63)."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00871v1",
      "paper_title": "Towards Predicting Any Human Trajectory In Context",
      "figure_filename": "069_2506.00871v1_Figure19.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/069_2506.00871v1_Figure19.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance of random example selection and the proposed STES at varying numbers of in-context examples with a larger number of examples",
      "gpt_annotation": "The plot illustrates the relationship between the \"Number of Examples\" and \"minFDE\\(_{20}\\)\" for two different methods. The x-axis, labeled \"Number of Examples,\" uses a linear scale with values ranging from 0 to 15, marked at intervals of 1. The y-axis, labeled \"minFDE\\(_{20}\\) (\u2194),\" also follows a linear scale with values from 0.8 to 1.05, marked at intervals of 0.05. The plot features two lines. The first line, representing the \"ST-ES\" method, is depicted in blue with circular markers and a solid line style. It starts at a value of 1.05 at 0 examples and decreases steeply to about 0.85 at 5 examples, then shows a slight declining trend reaching approximately 0.83 by 15 examples. The second line, representing the \"Random\" method, is in orange with circular markers and a solid line style, starting at 1.00 at 0 examples, showing a slight decrease, reaching approximately 0.96 by 2 examples, and maintaining a steady trend with slight fluctuations around 0.97 towards 15 examples."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00871v1",
      "paper_title": "Towards Predicting Any Human Trajectory In Context",
      "figure_filename": "069_2506.00871v1_Figure20.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/069_2506.00871v1_Figure20.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance change brought by different sizes of the in-context pool.",
      "gpt_annotation": "The plot illustrates the relationship between different methods and their minADE\\(_{20}\\) performance metrics across various ratios. The x-axis is labeled \"Ratio\" with a linear scale, ranging from 0.0 to 1.0, with tick marks at intervals of 0.2. The y-axis is labeled \"minADE\\(_{20}\\) (\u2190)\" with a linear scale, ranging from 15.5 to 17.5, with tick marks at intervals of 0.5. The plot contains five lines. The blue line with circular markers and a solid style represents \"TrajCL,\" starting at approximately 16.8 at a ratio of 0.0 and decreasing to around 15.6 at a ratio of 1.0. The orange line with circular markers and a solid style represents \"Head Tuning,\" starting at roughly 17.3 and decreasing slightly to around 16.9. The green line, also with circular markers and a solid style, represents \"FT,\" starting at about 17.0 and decreasing to roughly 16.8 before dipping further to about 16.2 at 0.8, then slightly rising to 16.3. The red line with circular markers and a solid style represents \"LoRA 16,\" beginning at approximately 17.0 and decreasing smoothly to around 16.5. Finally, the purple line with circular markers and a solid style represents \"LoRA 64,\" starting at approximately 17.4 and decreasing gradually to around 16.6. Each line displays a general downward trend, with individual variations in the degree of decline."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00871v1",
      "paper_title": "Towards Predicting Any Human Trajectory In Context",
      "figure_filename": "069_2506.00871v1_Figure21.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/069_2506.00871v1_Figure21.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance change brought by different sizes of the in-context pool.",
      "gpt_annotation": "The plot illustrates the relationship between \"Ratio\" and \"minFDE20\" for different methods. The x-axis is labeled \"Ratio\" and has a linear scale, with a value range from 0.0 to 1.0, and tick marks labeled at intervals of 0.2. The y-axis is labeled \"minFDE20\" with a linear scale and a range from 18 to 23, with tick labels at intervals of 1. The blue solid line with circle markers represents \"TrajICL,\" starting at approximately 22.5 and decreasing steadily to around 18 at the endpoint, displaying a consistent downward trend. The orange solid line with square markers for \"Head Tuning\" starts just above 23 and remains relatively stable with minor fluctuations, ending near 23. The green solid line with triangle markers for \"FT\" begins around 23, dips toward 21, and ends slightly below its initial value. The red solid line with cross markers, labeled \"LoRA 16,\" begins near 23 and gradually declines, finishing slightly above 21. The purple solid line with diamond markers for \"LoRA 64\" starts near 23, dips towards 21, and ends around 22."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00871v1",
      "paper_title": "Towards Predicting Any Human Trajectory In Context",
      "figure_filename": "069_2506.00871v1_Figure22.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/069_2506.00871v1_Figure22.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance change brought by different sizes of the in-context pool.",
      "gpt_annotation": "The plot illustrates the relationship between the ratio and minADE20. The x-axis is labeled \"Ratio,\" with a linear scale ranging from 0.0 to 1.0 and tick intervals at 0.2. The y-axis is labeled \"minADE20,\" with a linear scale from 0.55 to 0.75 and tick intervals at 0.05. There are five lines in the plot: the dark blue line uses circular markers and a solid style, starting at 0.60 and ending at 0.60 with a slight dip around 0.6; the orange line has triangular markers and a solid style, starting at 0.75 and ending at 0.70, showing a gradual decline; the green line with circular markers and a solid style starts at 0.65 and ends at 0.55, indicating a sharper decline; the red line has circular markers and a solid style, starting at 0.70 and ending at 0.60 with a slight dip around 0.8; the purple line with circular markers and a solid style starts at 0.70 and ends at 0.60, showing a consistent decline across the x-axis."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00871v1",
      "paper_title": "Towards Predicting Any Human Trajectory In Context",
      "figure_filename": "069_2506.00871v1_Figure23.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/069_2506.00871v1_Figure23.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/069_2506.00871v1_Towards Predicting Any Human Trajectory In Context.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance change brought by different sizes of the in-context pool.",
      "gpt_annotation": "The line plot illustrates how the minFDE20 metric changes with different ratios for various methods. The x-axis is labeled \"Ratio,\" has a linear scale ranging from 0 to 1, with tick labels at 0.0, 0.2, 0.4, 0.6, 0.8, and 1.0. The y-axis is labeled \"minFDE20 (\u2193),\" also on a linear scale, with values ranging from 0.8 to 1.3 and tick labels at 0.8, 0.9, 1.0, 1.1, 1.2, and 1.3. The plot includes five lines: a blue line with circular markers representing \"TrajICL,\" which trends downward from approximately 0.92 at a ratio of 0.0 to about 0.82 at a ratio of 1.0; an orange line with circular markers for \"Head Tuning,\" which decreases from about 1.28 to 0.92; a green line with circular markers for \"FT,\" showing a decline from around 1.04 to 0.84; a red line with circular markers for \"LoRA 16,\" decreasing from approximately 1.18 to 0.88; and a purple line with circular markers for \"LoRA 64,\" which starts at about 1.22 and ends at 0.96. All lines show a general downward trend across the x-axis, with no fluctuations observed."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.12973v1",
      "paper_title": "Fast, Not Fancy Rethinking G2P with Rich Data and Rule-Based Models",
      "figure_filename": "072_2505.12973v1_Figure1.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/072_2505.12973v1_Figure1.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/072_2505.12973v1_Fast, Not Fancy Rethinking G2P with Rich Data and Rule-Based Models.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/072_2505.12973v1_Fast, Not Fancy Rethinking G2P with Rich Data and Rule-Based Models.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": Phase 1 (5 epochs)",
      "gpt_annotation": "The plot illustrates the changes in training and validation loss over a series of training steps. The x-axis is labeled \"Steps\" and is on a linear scale, ranging from 0 to 30 with tick marks at intervals of 5. The y-axis is labeled \"Loss,\" also using a linear scale, ranging from 0.025 to 0.200 with tick marks at intervals of 0.025. The blue line represents the training loss and is characterized by circular markers and a solid line style. It begins at a value of approximately 0.200 at step 0 and decreases sharply, then more gradually, leveling off around 0.050 by step 30. The orange line depicts the validation loss, featuring circular markers and a solid line style as well. Starting near 0.075 at step 0, it follows a downward trend as well, stabilizing around 0.025 by step 30. Both lines show a general decline with diminishing returns over the course of the steps."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.12973v1",
      "paper_title": "Fast, Not Fancy Rethinking G2P with Rich Data and Rule-Based Models",
      "figure_filename": "072_2505.12973v1_Figure2.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/072_2505.12973v1_Figure2.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/072_2505.12973v1_Fast, Not Fancy Rethinking G2P with Rich Data and Rule-Based Models.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/072_2505.12973v1_Fast, Not Fancy Rethinking G2P with Rich Data and Rule-Based Models.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": Phase 2 (20 epochs)",
      "gpt_annotation": "The plot illustrates the changes in training and validation loss over a series of steps. The x-axis is labeled \"Steps,\" with a linear scale ranging from 0 to 250, with tick intervals at 0, 50, 100, 150, 200, and 250, and no units specified. The y-axis is labeled \"Loss,\" also with a linear scale, ranging from 0.01 to 0.07, with tick intervals at 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, and 0.07. The plot features two lines: a blue line with circular markers and a solid style representing the training loss, which starts at a value of approximately 0.07 at step 0 and decreases, showing a generally declining trend with minor fluctuations, ending around 0.02 at step 250. The orange line with circular markers and a solid style represents the validation loss, beginning at about 0.04 at step 0, also decreasing and showing some fluctuations, and stabilizing around 0.02 towards step 250. Both lines demonstrate a downward trend from their starting points."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.12973v1",
      "paper_title": "Fast, Not Fancy Rethinking G2P with Rich Data and Rule-Based Models",
      "figure_filename": "072_2505.12973v1_Figure3.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/072_2505.12973v1_Figure3.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/072_2505.12973v1_Fast, Not Fancy Rethinking G2P with Rich Data and Rule-Based Models.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/072_2505.12973v1_Fast, Not Fancy Rethinking G2P with Rich Data and Rule-Based Models.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": Phase 3 (50 epochs)",
      "gpt_annotation": "The plot illustrates the training and validation loss over a series of steps, showcasing how these losses evolve across different iterations. The x-axis is labeled \"Steps\" and is linear, ranging from 0 to 500, with tick labels at intervals of 100. The y-axis is labeled \"Loss\" and also uses a linear scale, with a range from 0.00 to 0.10 and tick labels at intervals of 0.02. The blue line represents the training loss, marked with circles and a solid line style. It starts at a value slightly above 0.10 at 0 steps, showing a steep decline before gradually leveling off to approximately 0.02 at 500 steps. The orange line illustrates the validation loss, also marked with circles and a solid line style. It begins slightly above 0.06 at 0 steps, decreasing steadily with minor fluctuations to reach about 0.02 at 500 steps. Both lines demonstrate a downward trend with diminishing loss as the number of steps increases."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.10402v1",
      "paper_title": "Rethinking Repetition Problems of LLMs in Code Generation",
      "figure_filename": "073_2505.10402v1_Figure3.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/073_2505.10402v1_Figure3.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/073_2505.10402v1_Rethinking Repetition Problems of LLMs in Code Generation.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/073_2505.10402v1_Rethinking Repetition Problems of LLMs in Code Generation.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The performance of RPG applied to LLMs of different sizes. This result is the average value across three scenarios on CodeRepetEval dataset.",
      "gpt_annotation": "The bar chart illustrates the performance of different algorithms based on specific metrics. The horizontal axis categorizes metrics labeled as \"EGP,\" \"TR-N,\" \"TR-S,\" and \"CCP,\" with arrows indicating upward or downward trends. The vertical axis represents \"Performance\" on a linear scale ranging from 0.0 to 1.0, with tick intervals at 0.2. The chart contains four bars for each metric, represented by different colors: blue for \"Greedy (CodeLlama-7B),\" yellow for \"RPG (CodeLlama-7B),\" green for \"Greedy (CodeLlama-34B),\" and orange for \"RPG (CodeLlama-34B).\" For the \"EGP\" metric, the yellow bar reaches approximately 0.9, and the orange bar slightly exceeds this value, while the blue and green bars are significantly shorter. In the \"TR-N\" category, the blue bar is tallest, around 0.7, while the others are lower. For \"TR-S,\" the green bar leads at approximately 0.5, with the blue bar slightly behind, and the yellow and orange bars are shorter. Lastly, in the \"CCP\" metric, the orange bar reaches the highest point close to 1.0, followed by the yellow and green bars, while the blue bar is the shortest. There is no specific sorting order or visual emphasis among the bars beyond the use of distinct colors for differentiation."
    }
  },
  {
    "data": {
      "arxiv_id": "2506.00363v1",
      "paper_title": "Adapting General-Purpose Embedding Models to Private Datasets Using   Keyword-based Retrieval",
      "figure_filename": "076_2506.00363v1_Figure3.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/076_2506.00363v1_Figure3.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/076_2506.00363v1_Adapting General-Purpose Embedding Models to Private Datasets Using   Keyword-based Retrieval.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/076_2506.00363v1_Adapting General-Purpose Embedding Models to Private Datasets Using   Keyword-based Retrieval.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Retrieval performance of MAP@10 for different $m$ and sampling strategies.",
      "gpt_annotation": "The bar chart illustrates the MAP@10 performance for two sampling strategies, Fine-to-Coarse and Uniform, across different values of the variable \"m.\" The horizontal axis represents categories labeled with values of \"m\" ranging from 6 to 10, and it is linearly spaced. The vertical axis signifies MAP@10, with numerical values ranging from 23.0 to 27.0, also linearly spaced, with tick labels at intervals of 0.5 units. Two sets of bars are present for each m-value; the Fine-to-Coarse strategy bars are purple, and the Uniform strategy bars are yellow. The purple bars range approximately from 26.0 to 26.7, with the tallest at m=7, and the shortest at m=6. The yellow bars range approximately from 24.4 to 25.7, with the tallest at m=8, and the shortest at m=6. The bars are grouped by m-values without specific sorting by height, and no bars are visually emphasized beyond color differentiation for strategy labeling."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06843v2",
      "paper_title": "Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely   Breaks Safety",
      "figure_filename": "080_2505.06843v2_Figure6.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/080_2505.06843v2_Figure6.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/080_2505.06843v2_Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely   Breaks Safety.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/080_2505.06843v2_Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely   Breaks Safety.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": Cross-Architecture.",
      "gpt_annotation": "The bar chart illustrates the transferability across different model architectures, focusing on their harmfulness scores. The horizontal axis contains categories representing various models, labeled as Qwen2-7B, Gemma2-9B, Ministral-8B, and Llama3-8B, in that order. The vertical axis is labeled \"Harmfulness Score\" with a linear scale ranging from 0 to 5, with tick labels marked at intervals of 1 unit. Each category has two bars: the \"Original Model\" in blue and the \"Fine-tuned Model\" in orange, as indicated by the legend. For Qwen2-7B, the blue bar reaches approximately 0.1, while the orange bar is around 2. For Gemma2-9B, the blue bar is around 0.2, and the orange bar is approximately 2.5. Ministral-8B shows the blue bar at about 0.1, and the orange bar reaches 4.5. Llama3-8B has the blue bar near 0.2, with the orange bar also at 4.5. The grouping is visually arranged with original models alongside fine-tuned models for each architecture. The bars do not appear to be sorted in any particular order, and there is no visual emphasis on any specific bar beyond the color distinction for the models."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06843v2",
      "paper_title": "Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely   Breaks Safety",
      "figure_filename": "080_2505.06843v2_Figure7.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/080_2505.06843v2_Figure7.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/080_2505.06843v2_Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely   Breaks Safety.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/080_2505.06843v2_Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely   Breaks Safety.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": Transferability of the harmfulness to (a) other architectures and (b) stronger models.",
      "gpt_annotation": "The bar chart illustrates the transferability of models to stronger versions, comparing harmfulness scores between original and fine-tuned models. The horizontal axis categorizes the models, labeled as \"llama2-13b\" and \"llama2-70b,\" while the vertical axis represents the \"Harmfulness Score,\" using a linear scale ranging from 1 to 5 with tick intervals of 1. Two bars for each category are presented: one for the \"Original Model\" in blue and another for the \"Fine-tuned Model\" in red. For \"llama2-13b,\" the blue bar reaches a score of approximately 1.5, and the red bar reaches approximately 4. For \"llama2-70b,\" the blue bar is slightly above 1, whereas the red bar is around 3.5. The bars are grouped by model category but not sorted in a specific order, and no particular visual emphasis is applied beyond the distinct colors for each model type."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06843v2",
      "paper_title": "Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely   Breaks Safety",
      "figure_filename": "080_2505.06843v2_Figure10.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/080_2505.06843v2_Figure10.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/080_2505.06843v2_Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely   Breaks Safety.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/080_2505.06843v2_Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely   Breaks Safety.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": Defense with data augmentation strategies.",
      "gpt_annotation": "The plot illustrates the change in harmfulness scores when augmented with safety samples for two different categories, represented by lines. The x-axis is labeled as \"# Safety Samples,\" using a linear scale ranging from 0 to 40, with tick labels at intervals of 5 samples. The y-axis is labeled \"Harmfulness Score,\" also on a linear scale, ranging from 1 to 5, with tick labels at intervals of 1. There are two lines in the plot: a blue line representing \"Bianchi\" with circular markers, and a red line representing \"BeaverTails\" with square markers. The blue line is solid and starts at a harmfulness score of 4 at 0 safety samples, declines to 2 at 5 samples, experiences a minimal further decline to end at a score of 1 at 40 samples. The red line is also solid, beginning at the same starting score of 4, decreases to a score of 3 at 5 samples, and generally maintains a steady trend to end at the same score of about 3 at 40 samples."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06843v2",
      "paper_title": "Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely   Breaks Safety",
      "figure_filename": "080_2505.06843v2_Figure11.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/080_2505.06843v2_Figure11.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/080_2505.06843v2_Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely   Breaks Safety.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/080_2505.06843v2_Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely   Breaks Safety.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": Harmfulness of the LLM when different defense strategies are adopted.",
      "gpt_annotation": "The bar chart illustrates the harmfulness score of fine-tuning large language models (LLMs) using a method labeled as Lisa with various alignment/fine-tuning steps. The horizontal axis contains the categories, which are labeled as alignment/fine-tuning steps represented by \"18/2,\" \"14/6,\" \"10/10,\" \"6/14,\" and \"2/18.\" The vertical axis represents the harmfulness score, with a linear scale ranging from 0 to 5, and tick intervals of 1 unit. Each bar is colored light blue, with the approximate values of the harmfulness scores being 3 for \"18/2,\" 2.5 for \"14/6,\" 4 for \"10/10,\" 3 for \"6/14,\" and 4 for \"2/18.\" A red dashed line labeled \"w/o defense\" at a score of 4 is present on the chart, indicating a reference point for comparison. The bars are organized in the order as listed on the horizontal axis without additional sorting methods applied. No bars are visually emphasized beyond their coloring."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.06843v2",
      "paper_title": "Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely   Breaks Safety",
      "figure_filename": "080_2505.06843v2_Figure16.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/080_2505.06843v2_Figure16.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/080_2505.06843v2_Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely   Breaks Safety.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/080_2505.06843v2_Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely   Breaks Safety.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Harmfulness of LLMs after fine-tuning over the data poisoned with Self-Inf-N.",
      "gpt_annotation": "The plot illustrates the change in a certain metric with different batch sizes under two different poi_rate conditions. The x-axis is labeled \"bsz\" and is on a categorical scale with values \"bsz=2,\" \"bsz=4,\" \"bsz=6,\" \"bsz=8,\" \"bsz=10,\" \"bsz=20,\" and \"bsz=32.\" The y-axis is labeled with an unlabeled metric, likely on a linear scale, ranging from 1.0 to 3.5, with tick intervals of 0.5 units. The green line represents the \"poi_rate=0\" condition, marked with round shapes and a solid line style. Starting at 2.9 at \"bsz=2,\" it shows a slight decrease to 2.75 at \"bsz=4,\" remains steady through \"bsz=8,\" then declines to about 1.4 at \"bsz=32.\" The orange line corresponds to the \"poi_rate=1%\" condition, also using round markers and a solid line style. Beginning at 3.0 at \"bsz=2,\" it dips slightly and then rises peaking around \"bsz=6\" at approximately 3.0, before showing a steady decline to 2.0 at \"bsz=32.\""
    }
  },
  {
    "data": {
      "arxiv_id": "2505.07293v1",
      "paper_title": "AttentionInfluence Adopting Attention Head Influence for Weak-to-Strong   Pretraining Data Selection",
      "figure_filename": "082_2505.07293v1_Figure4.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/082_2505.07293v1_Figure4.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/082_2505.07293v1_AttentionInfluence Adopting Attention Head Influence for Weak-to-Strong   Pretraining Data Selection.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/082_2505.07293v1_AttentionInfluence Adopting Attention Head Influence for Weak-to-Strong   Pretraining Data Selection.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The statistics of clustering. The left is the clustering result of AttentionInfluence, the right part is that of FineWeb-Edu Classifier.",
      "gpt_annotation": "The image features two pie charts comparing the distribution of categories for \"AttentionInfluence\" and \"FineWeb-Edu Classifier\" purposes. The \"AttentionInfluence\" chart includes 16 slices with each labeled and visually color-coded. The categories are: \"Policy\" (blue, 1%), \"Manage\" (purple, 2%), \"Materials\" (brown, 3%), \"Emerging tech\" (purple, 4%), \"Infor tech\" (pink, 5%), \"Events & people\" (gold, 6%), \"Society & culture\" (orange, 10%), \"Historical review\" (tan, 7%), \"Health & Medicine\" (blue, 22%), \"Medical\" (light blue, 6%), \"Animal & Plant\" (pink, 13%), \"Science\" (red, 15%), \"Nutrition\" (teal, 5%), \"Diseases\" (light blue, 11%), \"Teaching & resources\" (green, 15%), and \"Science & technology education\" (light green, 10%). A legend is not included, and all slices are labeled inside the sections. The \"FineWeb-Edu Classifier\" chart is composed of 17 slices with similar labeling and coloring, including additional categories such as \"Healthy education\" (green, 2%) and \"Teaching & resources\" (green, 17%). The largest portion is \"Education\" (green, 38%). There is no separate legend, but percentages are marked on the slices, and there is no visual emphasis on any slice except for proportional representation, arranged in descending size order."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.23493v1",
      "paper_title": "R2I-Bench Benchmarking Reasoning-Driven Text-to-Image Generation",
      "figure_filename": "083_2505.23493v1_Figure4.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/083_2505.23493v1_Figure4.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/083_2505.23493v1_R2I-Bench Benchmarking Reasoning-Driven Text-to-Image Generation.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/083_2505.23493v1_R2I-Bench Benchmarking Reasoning-Driven Text-to-Image Generation.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Distribution of Errors of Emu3, SD3-medium, Show-o+PARM, gpt-image-1.",
      "gpt_annotation": "The image contains four pie charts that represent the distribution of error types for different models or systems labeled as SD3.5-medium, Emu3, Show-o+PARM, and gpt-image-1. Each chart is divided into three slices: Reasoning Errors, Image Quality Degradation, and Visual Element Errors. In the SD3.5-medium chart, Reasoning Errors occupy 90.8% and are colored light blue, Visual Element Errors are 6.4% in pink, and Image Quality Degradation is 2.8% in teal. The Emu3 chart shows Reasoning Errors at 90.3% (light blue), Visual Element Errors at 6.2% (pink), and Image Quality Degradation at 3.5% (teal). In the Show-o+PARM chart, Reasoning Errors constitute 83.6% (light blue), Visual Element Errors 10.0% (pink), and Image Quality Degradation 6.4% (teal). Lastly, the gpt-image-1 chart displays Reasoning Errors at 93.7% (light blue), Visual Element Errors at 4.7% (pink), and Image Quality Degradation at 1.7% (teal). All charts include percentage labels within the slices, and the segments appear to be ordered by size in descending order. A legend is not present, and no slice is visually emphasized."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.23493v1",
      "paper_title": "R2I-Bench Benchmarking Reasoning-Driven Text-to-Image Generation",
      "figure_filename": "083_2505.23493v1_Figure14.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/083_2505.23493v1_Figure14.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/083_2505.23493v1_R2I-Bench Benchmarking Reasoning-Driven Text-to-Image Generation.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/083_2505.23493v1_R2I-Bench Benchmarking Reasoning-Driven Text-to-Image Generation.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Distribution of Quantifiers and Operations in Categorical, Approximate Number Generation, Disjunctive Reasoning, and Conjunctive Reasoning.",
      "gpt_annotation": "The set of three pie charts represents different categories or concepts: \"Categorical,\" \"Approximate Number Generation,\" and \"Disjunctive & Conjunctive.\" The \"Categorical\" chart contains four slices. These include \"Existential Quantifiers\" in green at 15.12%, \"Limiter\" in pink at 27.91%, \"Negation\" in orange at 27.91%, and \"Universal Quantifiers\" in light blue at 29.07%. The \"Approximate Number Generation\" chart has three slices, consisting of \"Absolute Absence\" in light purple at 18.71%, \"Comparative Relationships\" in light blue at 36.26%, and \"Approximate Quantities\" in beige at 45.03%. The \"Disjunctive & Conjunctive\" chart has two slices: \"Disjunctive\" in light blue at 49.51%, and \"Conjunctive\" in light pink at 50.49%. Each slice is labeled with percentages inside the slices. There is no legend provided, and none of the slices are visually emphasized or offset. The segments do not appear to follow a specific order based on size."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.13652v1",
      "paper_title": "Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineering Agents",
      "figure_filename": "086_2505.13652v1_Figure1.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/086_2505.13652v1_Figure1.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The comparison of two evaluation protocols for a GPT-4o-based agent: Pass$\\boldsymbol{@N}$ and random sampling. The x-axis shows the number of attempts, the y-axis shows the average success rate (i.e. the percentage of correct solutions).",
      "gpt_annotation": "The plot illustrates the comparison of success rates between two methods, \"Pass@N\" and \"Random,\" as a function of N. The x-axis is labeled \"N,\" with a linear scale ranging from 1 to 5, marked at integer intervals. The y-axis is labeled \"Success rate, %\" with a linear scale ranging from 20 to 50, marked at intervals of 5. The \"Pass@N\" method is represented by a red dashed line with circular markers. It starts at a success rate of 20% for N=1 and increases steadily to 45% for N=5, with similar increments between each point, such as reaching approximately 30% at N=3. The \"Random\" method is depicted by a blue dashed line with circular markers. This line remains constant across all values of N, maintaining a success rate of 20% from N=1 to N=5."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.13652v1",
      "paper_title": "Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineering Agents",
      "figure_filename": "086_2505.13652v1_Figure2.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/086_2505.13652v1_Figure2.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Per-instance success rate of Qwen-based policy computed over $15$ runs.",
      "gpt_annotation": "The bar chart illustrates the success rate per instance, comparing two policies: \"Qwen-based policy\" and \"Qwen-based policy + 1-step lookahead.\" The horizontal axis contains the categories labeled as \"Instance,\" numbered sequentially from 0 to 30. The vertical axis represents the \"Success rate per instance,\" labeled in percentage (%), with a range from 0% to 100% and tick marks at intervals of 10%. Each instance has two bars; the blue bars represent the \"Qwen-based policy,\" and the orange bars represent the \"Qwen-based policy + 1-step lookahead.\" The heights of the bars vary, with some instances showing similar success rates for both policies, while others exhibit differences. For instance, at the lowest instance number, both policies reach near 100%, while at instance 11, the orange bar reaches approximately 70%, and the blue bar is around 40%. The bars are grouped by instance and displayed in numerical order. No bars are visually emphasized, and the chart does not present any annotations for emphasis beyond the default coloration and differentiation in the legend."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.13652v1",
      "paper_title": "Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineering Agents",
      "figure_filename": "086_2505.13652v1_Figure3.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/086_2505.13652v1_Figure3.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The dependency between success rate and the number of candidates $N$ in trajectory selection.",
      "gpt_annotation": "The plot illustrates the success rate as a percentage for different policies as a function of N. The x-axis is labeled \"N\" and uses a linear scale with values ranging from 1 to 10, marked at intervals of 1. The y-axis is labeled \"Success rate, %\" and also uses a linear scale with values ranging from 20 to 55, marked at intervals of 5. The red dashed line with circle markers represents the \"Pass@N: Qwen-based policy + 1-step lookahead,\" starting at approximately 37% at N=1 and rising to about 52% at N=10, showing a steady upward trend. The pink dashed line with circle markers, labeled \"Pass@N: Qwen-based policy,\" starts at around 30% at N=1 and increases to roughly 42% at N=10. The green dashed line with circle markers, representing \"Trajectory selection: Qwen-based policy + 1-step lookahead,\" begins at about 28% at N=1 and reaches close to 37% at N=10, showing a gradual rise. The blue dashed line with circle markers, indicating \"Random: Qwen-based policy + 1-step lookahead,\" remains relatively constant at approximately 31% across all values of N. Lastly, the light blue dashed line with circle markers represents \"Random: Qwen-based policy,\" starting at around 22% at N=1 and remaining stable at close to 24% by N=10."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.13652v1",
      "paper_title": "Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineering Agents",
      "figure_filename": "086_2505.13652v1_Figure4.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/086_2505.13652v1_Figure4.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": The dependency between $1$-step lookahead SR and $\\lambda$.",
      "gpt_annotation": "The plot illustrates the variation in success rate, expressed as a percentage, with different values of the variable \u03bb. The x-axis represents \u03bb, with a linear scale ranging from 0.0 to 1.0 and tick labels at intervals of 0.2. The y-axis indicates the success rate, also on a linear scale, ranging from 27% to 34%, with tick labels at one-percentage-point intervals. The graph features a green dashed line with circular markers, indicating the success rate changes across the \u03bb values. The line shows an increase from 28% at \u03bb = 0.0 to a peak of approximately 33% at \u03bb = 0.6, followed by a decline to about 29% at \u03bb = 1.0. Error bars are present for each marker, showing variability in the data."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.13652v1",
      "paper_title": "Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineering Agents",
      "figure_filename": "086_2505.13652v1_Figure5.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/086_2505.13652v1_Figure5.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": The dependency between success rate and $\\gamma$ for MC.",
      "gpt_annotation": "The plot illustrates the success rate percentage in relation to the variable \\(\\gamma\\). The x-axis is labeled as \\(\\gamma\\), follows a linear scale, and ranges from 0.86 to 1.00 with non-uniform intervals. The y-axis is labeled as \"Success rate, %\" and also uses a linear scale, ranging from 25 to 32, with tick labels at every unit. The plot features a single green line, distinguished by circular markers and a dashed style. At \\(\\gamma = 0.86\\), the success rate begins at approximately 27.5%. The line decreases to around 26% at \\(\\gamma = 0.90\\), then demonstrates an upward trend, reaching close to 31% at \\(\\gamma = 1.00\\). Error bars are present for each data point, indicating variability in the measurements."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.13652v1",
      "paper_title": "Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineering Agents",
      "figure_filename": "086_2505.13652v1_Figure6.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/086_2505.13652v1_Figure6.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": The dependency between trajectory length and $\\gamma$ for MC.",
      "gpt_annotation": "The plot illustrates the trajectory length as a function of the variable \\(\\gamma\\). The x-axis represents \\(\\gamma\\) on a linear scale, ranging from 0.86 to 1.00, with tick labels regularly spaced at intervals of 0.02. The y-axis represents trajectory length, also on a linear scale, ranging from 10 to 22, with tick labels at intervals of 2. The plot contains three lines: a red dashed line with circle markers representing \"Unsuccessful\" trajectories, starting at approximately (0.86, 18) and ending at (1.00, 22), showing an increasing trend. A blue dashed line with circle markers represents \"All\" trajectories, beginning at around (0.86, 16) and ending near (1.00, 20), also increasing steadily. Lastly, a green dashed line with circle markers denotes \"Successful\" trajectories, starting at (0.86, 10), showing a gradual rise and reaching about (1.00, 14), indicating a consistent upward trend."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.13652v1",
      "paper_title": "Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineering Agents",
      "figure_filename": "086_2505.13652v1_Figure7.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/086_2505.13652v1_Figure7.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Critic learns to distinguish successfull and unsucessfull trajectories produced by Qwen-based policy.",
      "gpt_annotation": "The plot illustrates the average critic score over relative time for different policies involving a Qwen-based approach. The x-axis is labeled \"Time (relative)\" and is on a linear scale ranging from 0.0 to 1.0, with tick labels at intervals of 0.1. The y-axis, labeled \"Average critic score,\" also uses a linear scale, ranging from 0.2 to 0.9, with tick labels at intervals of 0.1. The green dashed line represents the \"Qwen-based policy + 1-step lookahead, successful\" scenario, starting at approximately 0.3 and increasing steadily to about 0.75 towards the end. The green solid line denotes the \"Qwen-based policy, successful\" condition, beginning near 0.3 and rising progressively to approximately 0.65. The red dashed line shows the \"Qwen-based policy + 1-step lookahead, unsuccessful\" outcome, starting around 0.3 and increasing slightly to about 0.45 by the end. Lastly, the red solid line indicates the \"Qwen-based policy, unsuccessful\" scenario, which starts at about 0.3, remains relatively stable, and slightly decreases to approximately 0.27 at the end."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.13652v1",
      "paper_title": "Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineering Agents",
      "figure_filename": "086_2505.13652v1_Figure8.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/086_2505.13652v1_Figure8.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Success rate of a TD($0.8$) critic with varying $K$ and $T$.",
      "gpt_annotation": "The plot illustrates the relationship between the variable K and the success rate, showing how different values of T affect the trend. The x-axis is labeled \"K\" and uses a linear scale ranging from 1 to 16, with tick marks at intervals of 1. The y-axis is labeled \"Success rate, %\" and also employs a linear scale ranging from 20 to 32, with tick marks at intervals of 2. There are five lines representing different T values. The blue line with circle markers and a dashed style represents T = 0.1, starting at approximately 21% at K = 1, peaking near 24% at K = 8, and declining to about 23% at K = 16. The cyan line with circle markers and a dashed style represents T = 0.5, starting around 22% at K = 1 and increasing steadily to approximately 27% at K = 16. The green line with circle markers and a dashed style represents T = 0.9, beginning at about 20% at K = 1, peaking at 31% at K = 8, and slightly decreasing to 30% at K = 16. The orange line with circle markers and a dashed style represents T = 1.2, starting near 22% at K = 1, peaking at approximately 31% at K = 8, and ending at 30% at K = 16. The red line with circle markers and a dashed style represents T = 1.5, beginning at 21% at K = 1, peaking at 32% at K = 8, and slightly decreasing to 31% at K = 16. Each line exhibits varying upward trends with specific peaks and slight declines toward the higher values of K."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.13652v1",
      "paper_title": "Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineering Agents",
      "figure_filename": "086_2505.13652v1_Figure9.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/086_2505.13652v1_Figure9.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": $1$-step lookahead improves success rate per instance for GPT-4o base policy, adding new solved issues.",
      "gpt_annotation": "The bar chart illustrates the success rate per instance across different approaches, specifically comparing \"GPT-4o policy\" and \"GPT-4o policy + 1-step lookahead.\" The horizontal axis represents the categories labeled as \"Instance,\" with each instance numbered sequentially without a designated scale type, while the vertical axis indicates the value labeled as \"Success rate per instance, %\" on a linear scale, ranging from 0% to 100% with tick intervals of 20%. The bars for each instance are grouped, with \"GPT-4o policy\" represented in blue and \"GPT-4o policy + 1-step lookahead\" in orange. Each group of bars illustrates the success rate for each instance, with blue and orange bars showing different height patterns. For example, instance 0 shows both approaches reaching a 100% success rate, whereas instance 1's rates differ with blue nearing 90% and orange slightly lower. Instances like 11 and 17 have notably high orange bars compared to blue, where instance 11 reaches about 100% in orange. No specific sorting order is applied, and no bars are visually emphasized beyond the color differentiation in the legend."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.13652v1",
      "paper_title": "Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineering Agents",
      "figure_filename": "086_2505.13652v1_Figure10.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/086_2505.13652v1_Figure10.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": The dependency between the success rate and the number of candidates $N$ in trajectory selection for GPT-4o.",
      "gpt_annotation": "The plot illustrates the success rate as a function of different values of \\( N \\) for various conditions related to the GPT-4o policy. The x-axis is labeled \"N\" and uses a linear scale, ranging from 1 to 5 with integer tick labels at each unit interval. The y-axis is labeled \"Success rate, %\" and also uses a linear scale, ranging from 20 to 50 with tick labels at intervals of 5%. There are five lines shown in the plot. The solid blue line with circle markers represents \"Random: GPT-4o policy,\" which remains constant at a success rate of 30% across all values of \\( N \\). The dashed red line with circle markers, labeled \"Pass@N: GPT-4o policy,\" starts at approximately 30% for \\( N=1 \\) and increases to about 44% at \\( N=5 \\). The dashed pink line with circle markers, representing \"Pass@N: GPT-4o policy + 1-step lookahead,\" begins at around 32% and rises progressively to approximately 46% by \\( N=5 \\). A dashed green line with circle markers labeled \"Trajectory selection: GPT-4o policy\" starts near 23% and increases steadily, reaching about 38% at \\( N=5 \\). The dashed green line with dark shade and solid circle markers, indicating \"Trajectory selection: GPT-4o policy + 1-step lookahead,\" begins at roughly 27% and shows an upward trajectory, ending at about 41% when \\( N=5 \\)."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.13652v1",
      "paper_title": "Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineering Agents",
      "figure_filename": "086_2505.13652v1_Figure11.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/086_2505.13652v1_Figure11.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": Critic learns to distinguish successful and unsuccessful trajectories for GPT-4o.",
      "gpt_annotation": "The plot illustrates the average critic score of different GPT-4o policy configurations over relative time. The x-axis is labeled \"Time (relative)\" with a linear scale ranging from 0.0 to 1.0, and tick labels marked at intervals of 0.2. The y-axis is labeled \"Average critic score\" with a linear scale ranging from 0.2 to 0.8, with tick labels marked at intervals of 0.1. Four lines are shown on the plot. The solid green line represents a successful GPT-4o policy, starting at approximately 0.3 and ending near 0.6 with a gradual upward trend throughout. The solid red line indicates an unsuccessful GPT-4o policy, beginning around 0.3 and remaining relatively flat, slightly above 0.3, with minimal variation. The dashed green line signifies a successful GPT-4o policy with a 1-step lookahead, starting near 0.3 and showing an upward trend with notable increases, reaching approximately 0.75 at the end. The dashed red line represents an unsuccessful GPT-4o policy with a 1-step lookahead, beginning slightly above 0.3 and displaying a slow upward trend, closing near 0.4."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.13652v1",
      "paper_title": "Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineering Agents",
      "figure_filename": "086_2505.13652v1_Figure12.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/086_2505.13652v1_Figure12.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": ": The average number of retries it takes to generate a trajectory that ends with \u201csubmit\u201d for a given fraction of the test set using the Qwen-based policy.",
      "gpt_annotation": "The bar chart illustrates the relationship between the number of retries before submission and the fraction of submitted instances. The horizontal axis represents the \"Number of retries before submitted,\" categorically numbered from 0 to 10. The vertical axis signifies the \"Fraction of submitted instances,\" using a linear scale ranging from 0.0 to 1.0 with intervals of 0.1. Each bar is uniformly colored blue and corresponds to a specific number of retries. The bar for 0 retries indicates a fraction slightly above 0.5, while the bar for 10 retries shows a fraction of 1.0. As the number of retries increases, the bars progressively rise, indicating increasing fractional values. The bars are sorted in ascending order based on the number of retries, and there is no visual emphasis placed on any specific bars."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.13652v1",
      "paper_title": "Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineering Agents",
      "figure_filename": "086_2505.13652v1_Figure13.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/086_2505.13652v1_Figure13.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/086_2505.13652v1_Guided Search Strategies in Non-Serializable Environments with   Applications to Software Engineerin.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "The effects of varying training dataset size for critic on performance of Qwen-based policy + $1$-step lookahead, Verified-$50$, default regime.",
      "gpt_annotation": "The plot illustrates the relationship between critic dataset size and success rate, both expressed in percentages. The x-axis is labeled \"Critic dataset size, %\" and uses a linear scale, ranging from 30% to 100% with regular tick intervals at 10%. The y-axis is labeled \"Success rate, %\" with a linear scale, ranging from 18% to 28% and tick marks at 2% intervals. The plot features a single green line with circular markers and a dashed style. Starting at approximately 20% success rate at 30% dataset size, the line rises to about 22.5% at 50%, dips slightly at 70%, and peaks at 27% by 100% dataset size. Error bars are present for each marker denoting variability."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.23856v1",
      "paper_title": "OMNIGUARD An Efficient Approach for AI Safety Moderation Across   Modalities",
      "figure_filename": "088_2505.23856v1_Figure2.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/088_2505.23856v1_Figure2.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/088_2505.23856v1_OMNIGUARD An Efficient Approach for AI Safety Moderation Across   Modalities.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/088_2505.23856v1_OMNIGUARD An Efficient Approach for AI Safety Moderation Across   Modalities.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Accuracy of detecting harmful prompts in a few-shot setting. As few-shot examples are provided, quickly achieves near-perfect accuracy, despite the attacks being quite different from its training data (e.g. without any few-shot examples, 's accuracy is close to 50% ). In contrast, the guard model baselines improve their accuracy slowly in a few-shot setting, despite sometimes having seen similar code attacks in their training data. Accuracies are averaged over 50 random sets of few-shot examples; error bars show the standard error of the mean.",
      "gpt_annotation": "The line plot illustrates the accuracy of different guards against code attacks on Python lists as a function of the number of few-shot examples. The x-axis is labeled \"Number of Few-Shot Examples\" and has a linear scale ranging from 0 to 5, with tick labels at each integer value. The y-axis, labeled \"Accuracy,\" also uses a linear scale, running from 45 to 100 with tick labels at intervals of 10 units. The plot includes six distinct lines. The black line with circular markers and a solid style represents \"OmniGuard,\" which starts at an accuracy of approximately 50 at 0 examples and increases steadily to reach about 95 at 5 examples. AegisGuard (P), shown in a blue line with plus markers and a solid style, starts at around 55 and rises sharply to over 80 at 1 example before declining steadily to around 65 by 5 examples. The red line with x markers and a solid style for AegisGuard (D) starts near 60 and decreases gradually to about 50 across the x-axis range. LlamaGuard 1, represented by a green line with circular markers and solid style, remains around 50 throughout the x-axis values. LlamaGuard 2 is depicted with an orange line, also with circular markers and a solid style, starting at approximately 50 and climbing steadily to around 90 at 5 examples. DuoGuard, illustrated with a purple line with star markers and a solid style, begins at about 55, drops slightly to 50 at 1 example, increases to around 70 by 2 examples, and then descends gradually to end at about 50 at 5 examples."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.23856v1",
      "paper_title": "OMNIGUARD An Efficient Approach for AI Safety Moderation Across   Modalities",
      "figure_filename": "088_2505.23856v1_Figure3.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/088_2505.23856v1_Figure3.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/088_2505.23856v1_OMNIGUARD An Efficient Approach for AI Safety Moderation Across   Modalities.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/088_2505.23856v1_OMNIGUARD An Efficient Approach for AI Safety Moderation Across   Modalities.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Accuracy of detecting harmful prompts in a few-shot setting. As few-shot examples are provided, quickly achieves near-perfect accuracy, despite the attacks being quite different from its training data (e.g. without any few-shot examples, 's accuracy is close to 50% ). In contrast, the guard model baselines improve their accuracy slowly in a few-shot setting, despite sometimes having seen similar code attacks in their training data. Accuracies are averaged over 50 random sets of few-shot examples; error bars show the standard error of the mean.",
      "gpt_annotation": "The plot illustrates the accuracy of different models in the context of a code attack on a Python stack, as a function of the number of few-shot examples. The x-axis is labeled \"Number of Few-Shot Examples\" with a linear scale ranging from 0 to 5, with tick labels at each integer. The y-axis represents \"Accuracy,\" also on a linear scale, ranging from 40 to 100 with tick labels spaced at intervals of 10. The plot includes multiple lines: the black line represents \"OmniGuard\" with circle markers and a solid style, beginning at an accuracy of 50 at x=0, peaking over 90 by x=5. The blue line represents \"AegisGuard (P)\" with triangle markers and a solid style, starting just below 50 at x=0, reaching around 70 at x=2 before decreasing to about 60 by x=5. The red line for \"AegisGuard (D)\" uses cross markers and a solid style, beginning above 50 at x=0 and remaining steady just above 50 throughout. \"LlamaGuard 1\" is indicated by a green square-dotted line staying constant at 50 across all x-values. The yellow line for \"LlamaGuard 2,\" marked with star shapes and a solid line, begins at approximately 60 at x=0, slightly increases and stabilizes at about 65 by x=5. The purple line, \"DuoGuard,\" with diamond markers and a solid line, starts exactly at 50 and remains steady across the x-axis."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.23856v1",
      "paper_title": "OMNIGUARD An Efficient Approach for AI Safety Moderation Across   Modalities",
      "figure_filename": "088_2505.23856v1_Figure4.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/088_2505.23856v1_Figure4.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/088_2505.23856v1_OMNIGUARD An Efficient Approach for AI Safety Moderation Across   Modalities.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/088_2505.23856v1_OMNIGUARD An Efficient Approach for AI Safety Moderation Across   Modalities.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Accuracy of detecting harmful prompts in a few-shot setting. As few-shot examples are provided, quickly achieves near-perfect accuracy, despite the attacks being quite different from its training data (e.g. without any few-shot examples, 's accuracy is close to 50% ). In contrast, the guard model baselines improve their accuracy slowly in a few-shot setting, despite sometimes having seen similar code attacks in their training data. Accuracies are averaged over 50 random sets of few-shot examples; error bars show the standard error of the mean.",
      "gpt_annotation": "The plot illustrates the accuracy of different models under a \"Code Attack: Python String\" scenario, based on the number of few-shot examples. The x-axis is labeled \"Number of Few-Shot Examples\" and uses a linear scale with integer values ranging from 0 to 5, with tick labels at every whole number. The y-axis is labeled \"Accuracy\" and also uses a linear scale, with values ranging from 0 to 100 and tick labels at intervals of 10 units. The plot contains six lines representing different models. The black line with circular markers and a solid style represents OmniGuard, starting at approximately 50 at 0 examples and rising to about 98 at 5 examples. The purple line with cross markers and solid style represents AegisGuard (P), beginning at around 81 at 0 examples and decreasing to roughly 70 at 5 examples. The red line with plus markers and solid style represents AegisGuard (D), which starts at approximately 84 at 0 examples, decreases to around 62 at 1 example, and remains steady until 5 examples. The green line with square markers and solid style represents LlamaGuard 1, starting at about 65 at 0 examples, peaking at 83 at 1 example, and then declining to around 62 at 5 examples. The orange line with diamond markers and solid style represents LlamaGuard 2, starting at around 90 at 0 examples and increasing slightly to about 95 at 5 examples. Lastly, the dark blue line with triangle markers and solid style represents DuoGuard, starting and staying at approximately 50 across all examples."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.21523v2",
      "paper_title": "More Thinking, Less Seeing Assessing Amplified Hallucination in   Multimodal Reasoning Models",
      "figure_filename": "089_2505.21523v2_Figure3.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/089_2505.21523v2_Figure3.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/089_2505.21523v2_More Thinking, Less Seeing Assessing Amplified Hallucination in   Multimodal Reasoning Models.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/089_2505.21523v2_More Thinking, Less Seeing Assessing Amplified Hallucination in   Multimodal Reasoning Models.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance across four perception benchmarks comparing Base, RL, and SFT+RL.",
      "gpt_annotation": "The bar charts illustrate the performance metrics in percentages for different models labeled \"MMV2t (Fix & Perception),\" \"MMEval-Pro (Fix & Perception),\" \"UNITERBase (Fix & Perception),\" and \"MMIBA-L (Fix & Perception).\" Each chart contains bars corresponding to three categories labeled on the horizontal axis as \"Base,\" \"RL,\" and \"SFT+RL.\" The vertical axis for all charts is labeled \"Accuracy (%)\" with a linear scale, displaying values ranging from specific minimums to maximums adapted to each dataset: 65 to 41 for MMV2t, 70 to 77 for MMEval-Pro, 72.5 to 85 for UNITERBase, and 30 to 37 for MMIBA-L. Each chart also includes a legend indicating the color-coding of bars, with \"Base\" in light blue, \"RL\" in medium blue, and \"SFT+RL\" in green. \n\nIn the \"MMV2t (Fix & Perception)\" chart, bars are light blue for \"Base\" (40.69), medium blue for \"RL\" (45.49), and green for \"SFT+RL\" (47.33). In the \"MMEval-Pro (Fix & Perception)\" chart, the values are 75.39 for \"Base,\" 71.10 for \"RL,\" and 76.85 for \"SFT+RL.\" The \"UNITERBase (Fix & Perception)\" chart shows values as 82.50 for \"Base,\" 83.70 for \"RL,\" and 75.90 for \"SFT+RL,\" while the \"MMIBA-L (Fix & Perception)\" chart depicts them as 3.50 for \"Base,\" 9.40 for \"RL,\" and 3.10 for \"SFT+RL.\" The bars in each chart do not appear to be sorted in a specific order beyond the consistent ordering of categories, and they do not appear to have any visual emphasis through shading, bolding, or additional annotations."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17067v2",
      "paper_title": "Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive   Impairment Detection via Contrastive Learning",
      "figure_filename": "096_2505.17067v2_Figure2.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/096_2505.17067v2_Figure2.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/096_2505.17067v2_Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive   Impairment Detection via Contras.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/096_2505.17067v2_Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive   Impairment Detection via Contras.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance impact of contrastive learning on different models. A-B UAR and F1 scores for linguistic models (BERT, RoBERTa, XLM) with and without CL. C-D UAR and F1 scores for speech models (Whisper, HuBERT, Wav2Vec2) with and without CL.",
      "gpt_annotation": "The charts illustrate the performance of different models with and without certain conditions applied. Chart A displays the Unweighted Average Recall (UAR) percentages, with categories represented on the horizontal axis labeled as BERT, RoBERTa, and XLM, and a vertical linear scale from 65% to 90% with tick marks every 5%. The blue bar represents \"Without CL,\" and the orange bar represents \"Add CL\" for each model. In Chart A, the BERT model shows UAR values of approximately 68% for Without CL and 72% for Add CL, RoBERTa displays around 69% and 74%, and XLM presents roughly 67% and 71%. Chart B shows the F1 Score percentages, with the same horizontal axis categories and a vertical linear scale ranging from 65% to 90%, with tick marks every 5%. In this chart, BERT has F1 Scores of about 80% for Without CL and 83% for Add CL, RoBERTa shows 82% and 85%, and XLM shows 78% and 86%. The bars are visually grouped by model with each group containing a blue and orange bar, distinguished by a color-coded legend indicating the two conditions compared. Bars do not appear to be sorted in any specific order beyond the categorical arrangement of the models, and there is no additional visual emphasis applied to any specific bar."
    }
  },
  {
    "data": {
      "arxiv_id": "2505.17067v2",
      "paper_title": "Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive   Impairment Detection via Contrastive Learning",
      "figure_filename": "096_2505.17067v2_Figure3.png",
      "figure_image": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/figures/096_2505.17067v2_Figure3.png",
      "paper_pdf": "https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/096_2505.17067v2_Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive   Impairment Detection via Contras.pdf",
      "html_pdf_button": "<a href='https://banahene.github.io/FigureSense-Data-Bank/Batch-1/downloaded_pdfs/096_2505.17067v2_Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive   Impairment Detection via Contras.pdf' title='Right-click \u2192 Open in new tab' target='_blank' style='display:inline-block;padding:10px 20px;font-size:16px;color:white;background-color:#007BFF;border-radius:5px;text-decoration:none;'>\ud83d\udcc4 View Full PDF</a>",
      "caption": "Performance impact of contrastive learning on different models. A-B UAR and F1 scores for linguistic models (BERT, RoBERTa, XLM) with and without CL. C-D UAR and F1 scores for speech models (Whisper, HuBERT, Wav2Vec2) with and without CL.",
      "gpt_annotation": "The bar charts illustrate the performance metrics for three models: Whisper, HuBERT, and Wav2Vec2, comparing results with and without the addition of CL. In the left chart (C), the horizontal axis represents categorical labels for the three models, while the vertical axis indicates UAR (%) with a linear scale ranging from 65% to 90% at 5% intervals. Each model has two bars: one for \"Without CL\" in blue and one for \"Add CL\" in orange. Approximate values for Whisper are 70% and 67%, for HuBERT both values are about 72%, and for Wav2Vec2 both values are about 68%. Bar heights are consistent for HuBERT and Wav2Vec2, while Whisper shows a slight variation between the bars. The right chart (D) displays F1 Score (%) on the vertical axis with a range from 65% to 90% at 5% intervals. Similar color coding is used for the bars: \"Without CL\" in blue and \"Add CL\" in orange. For Whisper, bars reach approximately 81% and 82%; for HuBERT, around 86% and 88%; and for Wav2Vec2, approximately 70% and 72%. Whisper and HuBERT show close values between \"Without CL\" and \"Add CL,\" while Wav2Vec2 exhibits greater change. Bars are not explicitly sorted, and none are visually emphasized beyond the use of color differentiation. Grouping is consistent across both charts, with each model having paired bars to indicate performance variations with and without CL."
    }
  }
]