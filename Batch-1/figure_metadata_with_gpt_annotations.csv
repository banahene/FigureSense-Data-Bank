pdf_filename,figure_filename,caption,plot_type,generated_annotation
001_2503.15664v1_Enhancing Pancreatic Cancer Staging with Large Language Models The Role   of Retrieval-Augmented Gen.pdf,001_2503.15664v1_Figure2.png,"TNM classification performance of NotebookLM with REK, Gemini 2.0 Flash with REK, and Gemini 2.0 Flash without REK in the experiment using 100 fictional pancreatic cancer cases. The TNM classification was deemed correct only if all T, N, and M factors were accurately identified. Additionally, the classification accuracy for each T, N, and M factor was compared across the three groups. REK=reliable external knowledge",Bar Chart,"The bar chart illustrates classification accuracy (%) across four different categories using three methodologies. On the horizontal axis, the categories are labeled as ""TNM classification,"" ""T factor,"" ""N factor,"" and ""M factor."" The vertical axis represents ""Classification accuracy (%)"" with a linear scale ranging from 0 to 100, marked at intervals of 20. Each category contains three bars representing different methods, identified by color and described in the legend as ""NotebookLM with REK"" (blue), ""Gemini with REK"" (orange), and ""Gemini without REK"" (green). In the ""TNM classification"" category, the ""NotebookLM with REK"" bar is approximately 80% in height, the ""Gemini with REK"" bar is around 55%, and the ""Gemini without REK"" bar is 50%. In the ""T factor"" category, the ""NotebookLM with REK"" bar reaches 85%, ""Gemini with REK"" is at 74%, and ""Gemini without REK"" is at 66%. For the ""N factor,"" the heights are approximately 94% for ""NotebookLM with REK,"" 75% for ""Gemini with REK,"" and 79% for ""Gemini without REK."" Lastly, in the ""M factor"" category, ""NotebookLM with REK"" stands at 97%, ""Gemini with REK"" at 95%, and ""Gemini without REK"" at 92%. The bars do not appear to be sorted in any particular order and none are visually emphasized beyond their coloring."
001_2503.15664v1_Enhancing Pancreatic Cancer Staging with Large Language Models The Role   of Retrieval-Augmented Gen.pdf,001_2503.15664v1_Figure3.png,"The performance of NotebookLM with REK, Gemini 2.0 Flash with REK, and Gemini 2.0 Flash without REK in determining local invasion factors and resectability classification in the experiment using 100 fictional pancreatic cancer cases. REK=reliable external knowledge",Bar Chart,"The bar chart illustrates classification accuracy across different models and conditions for two evaluation categories: ""Local invasion"" and ""Resectability."" The horizontal axis represents the categories labeled ""Local invasion"" and ""Resectability,"" with bars grouped under each category. The vertical axis is labeled ""Classification accuracy (%)"" and is a linear scale ranging from 0 to 100, with tick marks in intervals of 20%. For each category, three bars represent different models: ""NotebookLM with REK"" (in blue), ""Gemini with REK"" (in orange), and ""Gemini without REK"" (in green). In ""Local invasion,"" the blue bar reaches approximately 93%, the orange bar 83%, and the green bar 70%. In ""Resectability,"" the blue bar reaches approximately 86%, the orange bar 80%, and the green bar 83%. The bars are not sorted in a particular order beyond their grouping by category, and none are visually emphasized with special shading or annotation effects."
002_2503.13844v1_Spotting Persuasion A Low-cost Model for Persuasion Detection in   Political Ads on Social Media.pdf,002_2503.13844v1_Figure2.png,Model Performance across Different Thresholds.,Line Plot,"The plot illustrates the variation of different F1 scores as a function of a threshold value. The x-axis is labeled ""Threshold,"" uses a linear scale, and ranges from 0.3 to 0.7 with tick labels at intervals of 0.1. The y-axis is labeled ""Score,"" also employs a linear scale, and spans from 0.10 to 0.40 with tick labels at intervals of 0.05. The plot includes four lines. The first line, representing ""Dev F1 Micro,"" is blue with solid lines and circular markers. It begins at approximately 0.35 at a threshold of 0.3, peaks around 0.39 near a threshold of 0.5, and ends near 0.34 at 0.7. The second line, ""Dev F1 Macro,"" is cyan with solid lines and circular markers. It begins at about 0.15, shows slight fluctuation with a minimal downward trend, and ends just below 0.15. The third line, ""Test F1 Micro,"" is red with dashed lines and square markers. It starts around 0.34, increases to a peak near 0.39 at a threshold close to 0.5, and decreases to about 0.33 at 0.7. The fourth line, ""Test F1 Macro,"" is orange with dashed lines and square markers. It starts at approximately 0.25 and shows a slight downward trend, ending near 0.22 at a threshold of 0.7."
"003_2504.21625v3_Ask, Fail, Repeat Meeseeks, an Iterative Feedback Benchmark for LLMs'   Multi-turn Instruction-Follo.pdf",003_2504.21625v3_Figure2.png,Utility rate and Meeseeks Score gain over turns,Bar Chart,"The bar chart illustrates the comparison of scores for different systems across three utility and Meeseeks score measures. The categories are displayed on the vertical axis, labeled with system names (""Claude-3.7-Sonnet,"" ""Claude-3.7-Sonnet-Thinking,"" ""DeepSeek-V3-Chat-20241226,"" and ""DeepSeek-R1""), ordered from top to bottom. The chart utilizes a horizontal value axis labeled ""Score,"" with a linear scale ranging from 0.00 to 1.00. The tick labels are regularly spaced, indicating numeric intervals. Each system has associated grouped bars, differentiated by color corresponding to a legend that denotes ""Meeseeks Score Turn 1"" and ""Utility Rate Turn 1"" among others. The bars are colored red and shades of blue, representing different utility rates and Meeseeks scores across turns. For ""Claude-3.7-Sonnet,"" the ""Meeseeks Score Turn 3 Gain"" is red with an approximate score of 0.874, while ""Utility Rate Turn 1"" is the darkest blue with a score of 0.661. The grouping visually arranges these as stacked bars, with red segments consistently appearing longer. The shortest bar segments are often the blue segments for ""Utility Rate Turn 3 Gain."" The bars do not appear to be sorted in any particular order, nor are any bars visually emphasized beyond their colors."
008_2504.19675v1_Annif at SemEval-2025 Task 5 Traditional XMTC augmented by LLMs.pdf,008_2504.19675v1_Figure3.png,Effect of adding synthetic training data for Omikuji Bonsai models. The models were evaluated against the development set.,Line Plot,"The plot illustrates the nDCG@10 scores for different proportions of original and synthetic training data across four data categories. The x-axis is labeled ""Proportions of original + synthetic training data"" with a linear scale and categorical values, specifically ""1+0,"" ""1+1,"" ""1+2,"" and ""1+3."" The y-axis is labeled ""nDCG@10,"" also on a linear scale, ranging from 0.45 to 0.60 with tick marks at intervals of 0.05. The plot features four lines, each representing a different category. The blue line with circular markers represents the ""all-subjects / de"" category and shows a solid line style that starts at approximately 0.49 at ""1+0"" and rises to about 0.52 at ""1+3."" The red line with square markers, labeled ""all-subjects / en,"" is solid and begins around 0.53 at ""1+0,"" ending near 0.55 at ""1+3."" The purple line with triangular markers, representing ""tib-core-subjects / de,"" is a solid line that starts just above 0.48 at ""1+0"" and increases consistently to approximately 0.51 at ""1+3."" Lastly, the green line with diamond markers, labeled ""tib-core-subjects / en,"" exhibits a solid line style, commencing at about 0.55 at ""1+0"" and reaching around 0.58 at ""1+3."""
011_2503.21735v1_GateLens A Reasoning-Enhanced LLM Agent for Automotive Software Release   Analytics.pdf,011_2503.21735v1_Figure1.png,: The first benchmark with annotated difficulty levels.,Bar Chart,"The bar chart illustrates the F1 scores as percentages for two approaches, ""GateLens"" and ""Without RA,"" across four levels. The horizontal axis contains the categories labeled as ""Level,"" ordered numerically from 1 to 4, while the vertical axis represents the value labeled ""F1 (%)"" with a linear scale ranging from 70 to 100, with tick marks at intervals of 5. Each level contains two bars: one in blue representing ""GateLens"" and another in orange representing ""Without RA."" At Level 1, the blue bar's value is 100.0, and the orange bar is approximately 96.67. Similarly, at Level 2, the same values are represented. At Level 3, both bars have values of 100.0. At Level 4, the blue bar remains at 100.0, while the orange bar drops to approximately 72.73. The bars are grouped by level and are not sorted beyond the numerical order of levels. A legend clearly identifies the color coding for ""GateLens"" and ""Without RA."" There is no additional visual emphasis such as shading or annotation beyond the numerical value labels atop each bar."
011_2503.21735v1_GateLens A Reasoning-Enhanced LLM Agent for Automotive Software Release   Analytics.pdf,011_2503.21735v1_Figure2.png,: The second benchmark with real-world user queries,Bar Chart,"The bar chart illustrates the F1 percentages for two methods, ""GateLens"" and ""without RA,"" across different categories. The horizontal axis contains the categories labeled as ""Column Ops,"" ""Complex Multi,"" ""Conditional Cal,"" ""Data Filtering,"" ""Duplicate Re,"" ""Grouping,"" ""Metadata,"" and ""Table Gen,"" arranged in a specific order. The vertical axis represents the F1 score in percentage, with a linear scale ranging from 50% to 100%, marked at intervals of 10%. Each group consists of two bars: blue for ""GateLens"" and orange for ""without RA."" In ""Column Ops,"" both bars are approximately equal at 64.7%. For ""Complex Multi,"" the blue bar reaches about 84.0%, while the orange bar is approximately 78.3%. In ""Conditional Cal,"" both bars reach 93.3%. ""Data Filtering"" shows the blue bar at about 85.2% and the orange at 82.0%. In ""Duplicate Re,"" the blue bar is around 84.8% and the orange at 83.4%. Both bars in ""Grouping"" are at 80.0%. In the ""Metadata"" category, the blue bar shows 88.0% and the orange the same at 88.0%. For ""Table Gen,"" the blue bar is at 88.9%, while the orange is approximately 77.8%. The bars are grouped side by side within each category, and there are no specific visual emphases such as shading or outlining noted in the chart."
011_2503.21735v1_GateLens A Reasoning-Enhanced LLM Agent for Automotive Software Release   Analytics.pdf,011_2503.21735v1_Figure3.png,Performance of GateLens and GoNoGo with varying numbers of few-shot examples.,Line Plot,"The plot illustrates the F1 score percentage change as a function of the few-shot number, comparing two different approaches labeled ""GoNoGo"" and ""GateLens."" The x-axis is labeled ""Few-shot Num,"" with a linear scale ranging from 0 to 50, marked at intervals of 10. The y-axis is labeled ""F1 (%)"" with a linear scale ranging from 0 to 100, also marked at intervals of 10. There are two lines depicted in the plot. The first line, representing ""GoNoGo,"" is blue with circular markers and a dashed style. It begins at an F1 score of 0% at the 0 few-shot mark, rises to 42% at the 2 few-shot mark, continues to 60% at the 10 few-shot mark, and reaches 84.57% at the 50 few-shot mark, showing a strong upward trend across the x-axis. The second line, representing ""GateLens,"" is orange with circular markers and a solid style. It starts at an F1 score of 95.92% at the 0 few-shot mark, maintains a slight fluctuation around 95.92% between the 0 and 10 few-shot marks, before stabilizing to 100% by the 50 few-shot mark, depicting minimal fluctuation overall."
013_2505.16869v1_MPO Multilingual Safety Alignment via Reward Gap Optimization.pdf,013_2505.16869v1_Figure2.png,The results of replacing the dominant language reward gap with a fixed value on multilingual safety and general utility performance.,Line Plot,"The plot demonstrates the performance of two models, MultiJail and MT-Bench, in terms of multilingual safety and utility. The x-axis is labeled with scale type and represents numeric values ranging from 0.1 to 20.0 with regularly spaced intervals. The left y-axis, labeled ""Multilingual Safety Performance,"" has a linear scale ranging from 0 to 50, with major ticks every 10 units. The right y-axis, labeled ""Multilingual Utility Performance,"" has a linear scale ranging from 4.2 to 5.2 with major ticks every 0.2 units. The line representing MultiJail is maroon with circular markers and a solid line style, starting at a performance score near 50 at x = 0.1 and declining sharply to below 10 by x = 1.0, then remaining relatively steady around that value up to x = 20.0. The line representing MT-Bench is blue with circular markers and a solid line style, starting slightly above 40 at x = 0.1, exhibiting fluctuations with peaks above 40 and dips below 20 across the x-axis, eventually ending near 30 at x = 20.0."
013_2505.16869v1_MPO Multilingual Safety Alignment via Reward Gap Optimization.pdf,013_2505.16869v1_Figure3.png,: Impact of Data Quality.,Line Plot,"The plot illustrates multilingual safety performance across different models labeled as NLLB-600M, NLLB-1.3B, and NLLB-3.3B, with a comparison to LLaMA-3.1-8B-Instruct. The x-axis is labeled with model names and is categorical. The y-axis is labeled ""Multilingual Safety Performance,"" with a linear scale ranging from 5 to 35, marked at intervals of 5 units. The plot features six lines: a maroon line with circle markers representing MPO remains steady at a performance level of around 5; a yellow line with square markers representing DPO peaks at 30 at NLLB-1.3B and decreases to near 25 at NLLB-3.3B; a blue line with triangle markers representing IPO consistently increases slightly from 15 to around 17; a green line with star markers representing KTO peaks at approximately 27 at NLLB-1.3B before declining to near 20; an orange line with diamond markers representing ORPO shows a slight decline from 10 to about 7; and a light green line with inverted triangle markers representing SimPO starts at around 27, peaks to about 30, and decreases to about 24 by NLLB-3.3B. The dashed line at a performance level of 30 represents the benchmark LLaMA-3.1-8B-Instruct performance."
013_2505.16869v1_MPO Multilingual Safety Alignment via Reward Gap Optimization.pdf,013_2505.16869v1_Figure4.png,: Impact of the preference data. (a) Multilingual safety performance on MultiJail with varied data quality. (b) Multilingual safety performance on MultiJail with varied data size.,Line Plot,"The plot illustrates the multilingual safety performance of various models over different iterations. The x-axis is labeled as numeric with a linear scale, ranging from 100 to 400, with tick labels at intervals of 100. The y-axis is labeled ""Multilingual Safety Performance,"" also using a linear scale, ranging from 5 to 45, with tick labels at intervals of 5. There are six lines in the plot, each representing a different model or method. The maroon line with circle markers is solid and represents MPO, staying steady with slight fluctuation around 5. The yellow line with square markers is solid, representing DPO, and shows a slight increase from around 10 to just over 15. The blue line with triangle markers is solid, labeled IPO, showing an initial decline from around 14 to 10, then back up to approximately 15. The green line with diamond markers is solid and represents KTO, dipping from around 25 to 10 between 100 and 200, then increasing to end near 20. The orange line with circle markers is solid, denoting ORPO, maintaining a relatively steady trend around 10. The light green line with star markers is solid, labeled SimPO, rising consistently across the x-axis from approximately 25 to over 40 by the end of the range. A horizontal dashed line labeled ""LLaMA-3.1-8B-Instruct"" is drawn at y=32."
013_2505.16869v1_MPO Multilingual Safety Alignment via Reward Gap Optimization.pdf,013_2505.16869v1_Figure5.png,Reward gap across languages for the original backbone and those safety aligned by MPO and DPO.,Bar Chart,"The bar chart illustrates the ""Reward Gap"" across different language categories with a focus on specific models or approaches. The horizontal axis represents the language categories, labeled as ""En,"" ""Zh,"" ""Ar,"" ""Ko,"" ""Bn,"" and ""Sw."" These are arranged in no specific order. The vertical axis is labeled ""Reward Gap,"" with a linear scale ranging from 0.00 to 1.75 and tick intervals of 0.25. The chart contains grouped bars for each language category, differentiated by color: gray for ""LLAMA-3.1,"" yellow for ""DPO,"" and maroon for ""MPO."" In the ""En"" category, the gray bar indicates an approximate value of 1.00, the yellow bar is around 1.75, and the maroon bar is around 1.50. For ""Zh,"" the values are approximately 0.50 (gray), 1.25 (yellow), and 1.00 (maroon). The ""Ar"" category shows values around 1.25 (gray), 1.50 (yellow), and 1.50 (maroon). In ""Ko,"" the bars are around 0.50 (gray), 1.25 (yellow), and 1.50 (maroon). The ""Bn"" category shows values of approximately 0.50 (gray), 0.75 (yellow), and 1.00 (maroon). For ""Sw,"" the values are approximately 0.00 (gray), 0.75 (yellow), and 1.00 (maroon). The bars are not sorted in a particular sequence, and no bars are visually emphasized with shading or annotations. A legend at the upper right identifies the color coding for each model."
016_2505.00147v1_AdaptMI Adaptive Skill-based In-context Math Instruction for Small   Language Models.pdf,016_2505.00147v1_Figure5.png,"Accuracy and average output length of  on questions of Level 1–5 defined in the  dataset. Compared to <Ref>, the performance gap between fixed and skill-based examples is unnoticeable across all levels.",Bar Chart,"The bar chart presents two separate side-by-side plots that illustrate ""Accuracy (%)"" and ""Average Output Length (Token)"" across different ""MATH levels."" In both plots, the horizontal axis represents the value, labeled as ""Accuracy (%)"" on the left and ""Average Output Length (Token)"" on the right, with linear scales ranging from 0 to 100 and 300 to 900, respectively. The vertical axis on both plots displays ""MATH Level,"" ranging from 1 to 5, with levels decreasing from top to bottom. Each MATH level contains two bars representing ""Fixed Examples"" and ""Skill-based Examples,"" colored in blue and red, respectively. The bars compare different attributes at each MATH level for two example types, with specific accuracy percentages or token lengths labeled on top of each bar. In the Accuracy (%) chart, MATH Level 1 shows the highest accuracy for both example types, with ""Fixed Examples"" at 92.5% and ""Skill-based Examples"" at 91.3%. MATH Level 5 displays the lowest accuracy values, with ""Fixed Examples"" at 42.1% and ""Skill-based Examples"" close at 42.5%. In terms of Average Output Length, MATH Level 5 has the longest values, with ""Fixed Examples"" and ""Skill-based Examples"" at 842 and 867 tokens, respectively, while Level 1 has the shortest, at 355 and 378 tokens. The order of MATH levels is consistent across both charts and no specific sorting method is applied to the bars, nor are any bars visually emphasized beyond the color differentiation."
018_2505.00022v2_Aleph-Alpha-GermanWeb Improving German-language LLM pre-training with   model-based data curation an.pdf,018_2505.00022v2_Figure1.png,"Average accuracies (MMMLU, ARC-Easy, HellaSwag; single- & five-shot) of 1B Llama-style models trained on $\sim$ 84 billion tokens from different datasets. All subsets of  – marked with an asterisk (*) – outperform FineWeb2. For comparison, we also include EPFML High. Here, 1,000 training steps equates to approximately 2.1 billion tokens. Individual benchmark results are provided in App.  <ref>.",Line Plot,"The plot illustrates the relationship between training steps and average accuracy across different categories or datasets. The x-axis is labeled ""Training steps,"" with a linear scale ranging from 5,000 to 40,000, with tick labels at intervals of 5,000. The y-axis is labeled ""Average Accuracy,"" also on a linear scale, ranging from 0.28 to 0.36, with tick labels at intervals of 0.02. There are five lines representing different data categories. The blue line with circular markers and a solid style begins at an accuracy of approximately 0.285 at 5,000 steps and ends near 0.315 at 40,000 steps, showing a generally upward trend with fluctuations. The green line, also with circular markers and a solid style, starts slightly above 0.29 and ends close to 0.32, maintaining a relatively steady increase. The brown line with circular markers and a solid style begins at about 0.275, rises steeply to around 0.295 by 15,000 steps, and then levels out to finish near 0.305. The gray line, with circular markers and a solid style, shows a prominent increase from around 0.29 at the start to over 0.355 at the end, with a notable rise around 10,000 steps. The cyan line, featuring circular markers and a solid style, starts at approximately 0.285, experiences notable fluctuations, and ends around 0.33. The plot highlights variations in accuracy trends across these datasets or categories."
018_2505.00022v2_Aleph-Alpha-GermanWeb Improving German-language LLM pre-training with   model-based data curation an.pdf,018_2505.00022v2_Figure2.png,"Accuracies of 8B HAT models trained on different datasets. On average, and for all but one individual benchmark comparison,  outperforms FineWeb2. (Left) The HAT model was trained for 25,000 training steps on an English-language web-derived dataset, equating to $\sim$ 75 billion English words. Afterwards the model was further trained for 20,000 steps ($\sim$ 60 billion words) with German language data, either random FineWeb2 data augmented with high quality datasets like Wikipedia (FineWeb2:HQ Curated) or our generated synthetic data (Synthetic). Both datasets amount to 60 billion German words each. (Middle) We trained for 21,000 steps ($\sim$ 63 billion words) on either random FineWeb2 data or on our synthetic German data. (Right) The FineWeb2 training run was continued to 50,0000 steps and is compared to a training of 50,000 steps on data from our Filtered CC pipeline. Both runs each equate to $\sim$ 150 billion words.",Bar Chart,"The bar chart illustrates the accuracy levels of different models on four datasets, divided into categories associated with Synthetic and FineWeb2:HQ Curated. The horizontal axis contains the categories labeled as MMLU, ARC, TruthfulQA, and HellaSwag, ordered in this sequence. The vertical axis, labeled ""Accuracy,"" employs a linear scale ranging from 0.0 to 0.5, with tick labels at intervals of 0.1. Each category features a pair of bars; the Synthetic bars are gray, while the FineWeb2:HQ Curated bars are brown. The approximate accuracy values for the gray bars are 0.4 for MMLU, 0.35 for ARC, 0.3 for TruthfulQA, and 0.5 for HellaSwag. The brown bars have approximate accuracy values of 0.25 for MMLU, 0.3 for ARC, 0.25 for TruthfulQA, and 0.45 for HellaSwag. The bars are not sorted in any specific numerical order, and there are no additional visual emphases or annotations on specific bars. Grouping is executed such that each dataset has a pair consisting of one gray and one brown bar for direct visual contrast within the same dataset."
018_2505.00022v2_Aleph-Alpha-GermanWeb Improving German-language LLM pre-training with   model-based data curation an.pdf,018_2505.00022v2_Figure3.png,"Accuracies of 8B HAT models trained on different datasets. On average, and for all but one individual benchmark comparison,  outperforms FineWeb2. (Left) The HAT model was trained for 25,000 training steps on an English-language web-derived dataset, equating to $\sim$ 75 billion English words. Afterwards the model was further trained for 20,000 steps ($\sim$ 60 billion words) with German language data, either random FineWeb2 data augmented with high quality datasets like Wikipedia (FineWeb2:HQ Curated) or our generated synthetic data (Synthetic). Both datasets amount to 60 billion German words each. (Middle) We trained for 21,000 steps ($\sim$ 63 billion words) on either random FineWeb2 data or on our synthetic German data. (Right) The FineWeb2 training run was continued to 50,0000 steps and is compared to a training of 50,000 steps on data from our Filtered CC pipeline. Both runs each equate to $\sim$ 150 billion words.",Bar Chart,"The bar chart illustrates the accuracy of two models, Synthetic and FineWeb2, across four different datasets: MMLU, ARC, TruthfulQA, and HellaSwag. The horizontal axis represents the dataset categories, labeled at an angle, with MMLU, ARC, TruthfulQA, and HellaSwag in that order. The vertical axis indicates the accuracy, labeled as such, with a linear scale ranging from 0.0 to 0.5, marked at intervals of 0.1. Each dataset category has two bars: one gray bar for Synthetic and one brown bar for FineWeb2. The approximate accuracy values are as follows: for MMLU, Synthetic is around 0.32, and FineWeb2 is approximately 0.27; for ARC, both Synthetic and FineWeb2 are approximately 0.31; for TruthfulQA, Synthetic is about 0.27, and FineWeb2 stands at roughly 0.25; for HellaSwag, Synthetic reaches around 0.44, and FineWeb2 is close to 0.48. The chart employs grouped bars within each category. The bars are not sorted in any specific order like descending or alphabetical, and no visual emphasis such as shading or annotation is applied to any of the bars."
018_2505.00022v2_Aleph-Alpha-GermanWeb Improving German-language LLM pre-training with   model-based data curation an.pdf,018_2505.00022v2_Figure4.png,"Accuracies of 8B HAT models trained on different datasets. On average, and for all but one individual benchmark comparison,  outperforms FineWeb2. (Left) The HAT model was trained for 25,000 training steps on an English-language web-derived dataset, equating to $\sim$ 75 billion English words. Afterwards the model was further trained for 20,000 steps ($\sim$ 60 billion words) with German language data, either random FineWeb2 data augmented with high quality datasets like Wikipedia (FineWeb2:HQ Curated) or our generated synthetic data (Synthetic). Both datasets amount to 60 billion German words each. (Middle) We trained for 21,000 steps ($\sim$ 63 billion words) on either random FineWeb2 data or on our synthetic German data. (Right) The FineWeb2 training run was continued to 50,0000 steps and is compared to a training of 50,000 steps on data from our Filtered CC pipeline. Both runs each equate to $\sim$ 150 billion words.",Bar Chart,"The bar chart illustrates the accuracy of two data sources, Filtered CC and FineWeb2, across four categories labeled MMMLU, ARC, TruthfulQA, and HellaSwag. The horizontal axis categorizes these four distinct groups in the order mentioned, while the vertical axis is labeled ""Accuracy"" and appears on a linear scale ranging from 0.0 to 0.6, with tick marks at intervals of 0.1. Each category consists of two grouped bars: a green bar representing Filtered CC and a brown bar representing FineWeb2. Approximate accuracy values for Filtered CC are as follows: MMMLU at 0.25, ARC at 0.35, TruthfulQA at 0.3, and HellaSwag at 0.55. For FineWeb2, values are approximately MMMLU at 0.2, ARC at 0.3, TruthfulQA at 0.2, and HellaSwag at 0.45. The chart legend in the top left corner clarifies the color representation for each data source. Bars are presented in the same order for each category without any specific sorting by value or additional visual emphasis such as shading or bolding."
019_2503.20914v1_D4R -- Exploring and Querying Relational Graphs Using Natural Language   and Large Language Models -.pdf,019_2503.20914v1_Figure1.png,: Node types distribution,Bar Chart,"The bar chart illustrates the count of various categorized elements. The categories are represented on the vertical axis with labels: ""Person,"" ""Religious,"" ""Concept,"" ""Place,"" ""Judicial,"" ""Unknown,"" and ""Org,"" ordered from top to bottom. The horizontal axis represents the count labeled as ""Count"" and uses a linear scale ranging from 0 to 350, with tick intervals of 50 units. Bars are colored in blue, with the approximate values indicated by labels on or near each bar. The bar labeled ""Person"" is the tallest at 308, followed by ""Religious"" at 172 and ""Concept"" at 126. The ""Place"" bar measures approximately 64, ""Judicial"" at 50, ""Unknown"" at 33, and ""Org"" as the shortest at 26. The bars are sorted in descending order based on their count values, and no bars are visually emphasized beyond the use of color and labels."
019_2503.20914v1_D4R -- Exploring and Querying Relational Graphs Using Natural Language   and Large Language Models -.pdf,019_2503.20914v1_Figure2.png,: Left: Distribution of node types in the historical dataset by category. Right: Distribution of relationship types in the historical dataset.,Bar Chart,"The bar chart illustrates the frequency of various activities or themes labeled on the vertical axis, including ""Communication,"" ""Assertions and Claims,"" and others, while the horizontal axis represents the frequency of occurrence, labeled as ""Frequency."" The frequency axis uses a linear scale ranging from 0 to approximately 550, with tick intervals of 100 units. Each bar is labeled and displayed in dark blue, with the respective frequency indicated within or adjacent to the bar. The longest bar represents ""Communication"" with a frequency of 537, while the shortest is ""Accusations and Charges"" with a frequency of 111. All bars are uniformly colored and show a descending order based on their frequency values. No specific bars are visually emphasized beyond uniform color coding and labeling."
020_2504.21024v1_WebEvolver Enhancing Web Agent Self-Improvement with Coevolving World   Model.pdf,020_2504.21024v1_Figure3.png,: WebVoyager,Line Plot,"The plot represents the changes across different models over several iterations labeled as ""Self-improve Iterations."" The x-axis is labeled ""Self-improve Iterations,"" with a linear scale ranging from 0 to 3, marked at integer intervals. The y-axis does not have a label, but it uses a linear scale with a range from 30 to 55, marked at intervals of 5 units. The plot contains four lines. The first line, labeled ""OpenWebVoyager,"" is depicted in blue with circular markers and a solid line style, starting at a value around 32 at iteration 0 and ending just above 40 at iteration 3, showing a steady upward trend. The second line, labeled ""WebEvolver,"" is shown in blue with square markers and a solid line style, beginning slightly below 35 at iteration 0 and reaching around 42 at iteration 3, displaying a gradual increase. The third line, labeled ""OpenWebVoyager+WMLH,"" is illustrated in red with triangular markers and a dashed line style, commencing at approximately 42 at iteration 0 and remaining constant at that value through iteration 3. The fourth line, labeled ""WebEvolver+WMLH,"" is orange with diamond markers and a dashed line, starting around 37 at iteration 0 and climbing to approximately 50 at iteration 3, with a notable peak at iteration 2."
020_2504.21024v1_WebEvolver Enhancing Web Agent Self-Improvement with Coevolving World   Model.pdf,020_2504.21024v1_Figure4.png,: Visual illustration of overall success rate evolving on WebVoyager and Mind2Web-Live.,Line Plot,"The line plot illustrates the performance of different models over a series of self-improve iterations. The x-axis is labeled ""Self-improve Iterations,"" with a linear scale ranging from 0 to 3, with tick labels at intervals of 1. The y-axis does not have a label in the image provided, but it features a linear scale ranging from 15.0 to 27.5, with tick labels at intervals of 2.5. The first line, representing ""OpenWebVoyager,"" is colored blue and marked with circular markers, following a solid line style. It starts at approximately 20.0 at iteration 0, decreases to around 17.0 at iteration 1, and then increases, ending near 20.0 at iteration 3. The second line, representing ""WebEvolver,"" is colored cyan with square markers and a solid line style, starting at approximately 17.5, dipping to about 16.0 at iteration 1, and rising to end at around 19.0. The ""OpenWebVoyager+WMLH"" line is red, incorporating triangular markers and a dashed line style, beginning at around 20.5, peaking at approximately 22.5 at iteration 1, and maintaining a roughly steady trend thereafter. The ""WebEvolver+WMLH"" line is shown in orange with diamond markers and a solid line style, starting near 27.0, peaking at about 28.0 at iteration 1, then decreasing to finish at approximately 26.0 at iteration 3. The plot includes a legend that clearly identifies each line according to its respective model."
021_2505.19294v1_Towards Reliable Large Audio Language Model.pdf,021_2505.19294v1_Figure4.png,"Rejection Rate(%), Reliability(%), and Reliability Gain Index (RGI) performance for different LoRA alpha weights trained on speech modality.",Bar Chart,"The bar chart illustrates the rejection rates associated with two categories, ""Sound"" and ""Music,"" across different values of ""LoRA alpha."" The horizontal axis displays the categories labeled as ""LoRA alpha"" with the values 8, 16, and 32, ordered numerically. The vertical axis represents the ""Rejection Rate (%)"" using a linear scale, ranging from 0 to 100 with tick marks at intervals of 20. The bars for each category are color-coded, with ""Sound"" represented in blue and ""Music"" in orange. At LoRA alpha value 8, the ""Sound"" category has a short blue bar with an approximate rejection rate of 5%, while the ""Music"" category has an even shorter orange bar near 0%. At LoRA alpha value 16, the ""Sound"" and ""Music"" categories have bars of similar height, with approximate rejection rates of 10%. At LoRA alpha value 32, both categories have the tallest bars, with ""Sound"" reaching approximately 92% and ""Music"" slightly shorter at around 90%. The bars are grouped by the LoRA alpha values, with color differentiating the categories, and there is no visual emphasis or specific sorting beyond the numerical order of LoRA alpha values."
021_2505.19294v1_Towards Reliable Large Audio Language Model.pdf,021_2505.19294v1_Figure5.png,"Rejection Rate(%), Reliability(%), and Reliability Gain Index (RGI) performance for different LoRA alpha weights trained on speech modality.",Bar Chart,"The bar chart illustrates the reliability percentage of two categories, labeled as ""Sound"" and ""Music,"" across different LoRA alpha values. The horizontal axis, which contains the categories, is labeled ""LoRA alpha"" and includes discrete values: 8, 16, and 32. The vertical axis represents ""Reliability (%)"" with a linear scale ranging from 0 to 80, marked at intervals of 10. The bars are grouped by LoRA alpha values, with colors indicating the categories: blue for ""Sound"" and orange for ""Music."" For the LoRA alpha value of 8, the ""Sound"" category has a height of approximately 70%, while ""Music"" is around 60%. At the alpha value of 16, both categories show the same approximate values as at 8, with ""Sound"" slightly taller. At an alpha of 32, the ""Sound"" bar is shorter than at previous values, at about 15%, and the ""Music"" bar is around 10%. The chart arranges the bars in sets of two for each alpha value, with ""Sound"" consistently being slightly taller than ""Music"" within each group. There is no specific emphasis on any of the bars, and they are sorted by increasing LoRA alpha values."
021_2505.19294v1_Towards Reliable Large Audio Language Model.pdf,021_2505.19294v1_Figure6.png,"Rejection Rate(%), Reliability(%), and Reliability Gain Index (RGI) performance for different LoRA alpha weights trained on speech modality.",Bar Chart,"The bar chart illustrates the Relative Gain Index (RGI) associated with different LoRA alpha values for two categories: Sound and Music. The horizontal axis contains the categories labeled as LoRA alpha, with values of 8, 16, and 32 in linear order. The vertical axis represents the RGI, with a linear scale ranging from -0.1 to 0.5. The tick labels are evenly spaced at intervals of 0.1. Each LoRA alpha group consists of two bars: one for Sound, shown in blue, and one for Music, shown in orange. At LoRA alpha 8, both Sound and Music have RGI values slightly above 0.4. For alpha 16, the Sound bar reaches about 0.35, while Music is slightly lower, around 0.25. At alpha 32, Sound has a negative RGI value near -0.1, and Music maintains a small positive value above zero. The grouped bars are not sorted or visually emphasized in any specific way beyond the color differentiation indicated by the legend."
021_2505.19294v1_Towards Reliable Large Audio Language Model.pdf,021_2505.19294v1_Figure7.png,IDK percentage for constructing IDK dataset with different $K$@$5$ threshold.,Line Plot,"The plot illustrates the percentage of IDK responses across various categories over different values of K@5. The x-axis is labeled ""K@5,"" with a linear scale ranging from 0 to 5, and tick labels are at intervals of 1 unit. The y-axis is labeled ""IDK Percentage (%)"" and also uses a linear scale, with values ranging from 0% to 60%, with tick intervals of 10%. The blue line represents the ""Sound (%)"" category, marked with circular markers and a solid line style. It starts at an IDK percentage of 0% at K=0, rises to about 50% at K=1, and slightly increases to approximately 56% by K=5. The orange line depicts ""Music (%)"" with circular markers and a solid line style, beginning at 0% at K=0, rising to about 45% at K=1, and reaching around 53% by K=5. The green line shows ""Speech (%)"" with circular markers and a solid line style, starting at 0% at K=0, increasing to approximately 54% at K=1, and ending at roughly 60% by K=5. The red line represents ""Total (%)"" with circular markers and a solid line style, beginning at 0% at K=0, climbing to around 51% at K=1, and slightly increasing to around 57% at K=5."
022_2505.07233v2_DynamicRAG Leveraging Outputs of Large Language Model as Feedback for   Dynamic Reranking in Retriev.pdf,022_2505.07233v2_Figure4.png,: Performance on NQ.,Bar Chart,"The bar chart illustrates the performance of different retrieval models, as indicated by the ""NQ EM"" label, comparing ""VanillaRAG"" and ""DynamicRAG"" across three categories: DPR, Contriever, and MonoT5. The categories are represented on the horizontal axis, which is ordered from left to right as DPR, Contriever, and MonoT5. The vertical axis represents the performance metric, ranging from 0 to 60, with tick marks at intervals of 10 units, indicating a linear scale. Each category is displayed with two bars: a light green bar representing VanillaRAG and a yellow bar representing DynamicRAG. For DPR, the VanillaRAG performance is approximately 40, while DynamicRAG is around 50. In Contriever, VanillaRAG is approximately 42, and DynamicRAG is about 53. For MonoT5, VanillaRAG is approximately 43, with DynamicRAG near 55. The bars are grouped by category with consistent coloring, as specified in the legend, and are not sorted in any particular way beyond the categorical grouping. No bars are visually emphasized beyond their color coding."
022_2505.07233v2_DynamicRAG Leveraging Outputs of Large Language Model as Feedback for   Dynamic Reranking in Retriev.pdf,022_2505.07233v2_Figure5.png,: Performance on HotpotQA.,Bar Chart,"The bar chart illustrates the performance measured by HotpotQA Exact Match (EM) for three models: DPR, Contriever, and MonoT5, using two different approaches, VanillaRAG and DynamicRAG. The horizontal axis contains the categories labeled as DPR, Contriever, and MonoT5, while the vertical axis represents the performance values labeled as ""HotpotQA EM,"" with a linear scale ranging from 0 to 40 in increments of 5. Each category has two bars, one for VanillaRAG in green and one for DynamicRAG in yellow. For the DPR model, the VanillaRAG bar reaches approximately 30, and the DynamicRAG bar is slightly taller, just above 30. The Contriever model has a VanillaRAG bar around 30 and a DynamicRAG bar slightly above it. The MonoT5 model shows a VanillaRAG bar slightly above 30, with the DynamicRAG bar reaching close to 40, making it the tallest. The bars are not sorted in any particular order, and none are visually emphasized through shading or annotation."
022_2505.07233v2_DynamicRAG Leveraging Outputs of Large Language Model as Feedback for   Dynamic Reranking in Retriev.pdf,022_2505.07233v2_Figure6.png,: Performance on ASQA.,Bar Chart,"The bar chart illustrates the ASQA EM performance comparison between different retrieval methods, represented as VanillaRAG and DynamicRAG. The horizontal axis features the categories labeled DPR, Contriever, and MonoT5, while the vertical axis represents the performance score, labeled from 0 to 60 at intervals of 10, on a linear scale. Each category contains two bars, where the bar in light green represents VanillaRAG and the bar in yellow represents DynamicRAG. In the DPR category, the VanillaRAG bar reaches approximately 40, and the DynamicRAG bar is slightly taller, reaching around 50. For the Contriever category, the VanillaRAG bar stands at about 45, while the DynamicRAG bar reaches approximately 55. In the MonoT5 category, the VanillaRAG bar reaches around 45, and the DynamicRAG bar extends higher to around 55. The bars are grouped by category but not specifically sorted, and no visual emphasis is given to any specific bar within the chart."
022_2505.07233v2_DynamicRAG Leveraging Outputs of Large Language Model as Feedback for   Dynamic Reranking in Retriev.pdf,022_2505.07233v2_Figure8.png,"Distribution of reranked document numbers (k) on NQ and HotpotQA before and after RL training. $k$ is truncated at 15 to ensure a fair comparison, as we restrict $k \leq 15$ during both training and sampling.",Line Plot,"The plot illustrates the distribution of a variable influenced by a factor labeled ""RL"" across different datasets, specifically NQ and HotpotQA, both before and after the application of RL. The x-axis is labeled ""Number of K,"" utilizes a linear scale, and ranges from 0 to 15 with tick marks at intervals of 1. The y-axis does not have a label but also employs a linear scale, ranging from 0 to 2500 with tick labels at intervals of 500. Four lines are depicted in the plot: a blue solid line with no markers represents ""NQ After RL,"" starting near 0, remaining low until approximately K=10, and peaking around K=13 at just over 1500. A red solid line represents ""HotpotQA After RL,"" begins near 0, quickly rises to peak at K=13 with a value slightly below 2500, and then declines. A green solid line without markers stands for ""NQ Before RL,"" starts near 0, gradually increases with a peak at K=13 around 1000, and decreases thereafter. An orange solid line with no markers indicates ""HotpotQA Before RL,"" starting near 0, rising smoothly to peak close to 1500 around K=13, and then falling."
022_2505.07233v2_DynamicRAG Leveraging Outputs of Large Language Model as Feedback for   Dynamic Reranking in Retriev.pdf,022_2505.07233v2_Figure9.png,"The impact of varying the number of Top-$N$ documents (Top-50, Top-100, Top-150, Top-200, Top-300, and Top-500) used for reranking on DynamicRAG performance across different benchmarks (NQ, HotpotQA, ASQA). We use Exact Match as the metric.",Line Plot,"The plot illustrates the performance comparison of three models, labeled as NQ, HotpotQA, and ASQA, across different conditions on the x-axis, which is labeled with categories: Zero-Shot, Top-50, Top-100, Top-150, Top-200, Top-300, and Top-500. The y-axis is linear, ranging from 25 to 60, with tick intervals of 5, and represents the performance metric without specified units. The ""NQ"" line is green with circular markers and solid style, starting at approximately 26 at Zero-Shot and increasing to about 53 at Top-50 before gradually decreasing to around 46 at Top-500. The ""HotpotQA"" line is orange with diamond markers and solid style, starting at about 26, peaking at approximately 36 at Top-50, and then remaining relatively stable around 35 across the rest of the x-axis. The ""ASQA"" line is blue with cross markers and solid style, starting at about 29 at Zero-Shot, peaking near 57 at Top-50, and slightly declining to approximately 55 by Top-500."
022_2505.07233v2_DynamicRAG Leveraging Outputs of Large Language Model as Feedback for   Dynamic Reranking in Retriev.pdf,022_2505.07233v2_Figure14.png,"Distribution of reranked document numbers (k) on NQ and HotpotQA before and after RL training. $k$ is truncated at 15 to ensure a fair comparison, as we restrict $k \leq 15$ during both training and sampling.",Line Plot,"The plot illustrates the behavior of different datasets in relation to a variable measured as ""Number of K."" The x-axis is labeled ""Number of K,"" with a linear scale ranging from 0 to 16, marked at intervals of 2. The y-axis is not labeled with specific units but has a linear scale ranging from 0 to 2500, also marked at intervals of 500. The plot features four lines: a blue line representing ""NQ After RL,"" a red line for ""HotpotQA After RL,"" a green line for ""NQ Before RL,"" and an orange line for ""HotpotQA Before RL."" The blue line begins near the x-axis, rises around K=10, reaching approximately 2200 at K=14, then declines. The red line starts similarly low, peaking sharply around 2400 at K=13, before decreasing. The green line shows a slight upward trend starting near the baseline, peaking modestly near 1500 at K=14, and then dropping off. The orange line rises gradually from the low starting point, reaching around 2500 at K=14 before declining. Each line displays unique inflection points and variations across the x-axis."
022_2505.07233v2_DynamicRAG Leveraging Outputs of Large Language Model as Feedback for   Dynamic Reranking in Retriev.pdf,022_2505.07233v2_Figure15.png,: Performance on NQ.,Bar Chart,"The bar chart illustrates the Exact Match (EM) performance across different models labeled as DPR, Contriever, and MonoT5, comparing the VanillaRAG and DynamicRAG configurations. The horizontal axis categorizes the models, ordered as DPR, Contriever, and MonoT5, while the vertical axis represents the EM score labeled ""NQ EM,"" with a linear scale ranging from 0 to 60 in increments of 10. Each model category contains two grouped bars: one for VanillaRAG in green and one for DynamicRAG in yellow. In the DPR category, the VanillaRAG bar reaches approximately 35, while the DynamicRAG bar is slightly taller at around 50. For the Contriever category, the VanillaRAG bar is at approximately 35, and the DynamicRAG bar extends to about 55. In the MonoT5 category, the VanillaRAG score is slightly above 30, with the DynamicRAG bar reaching about 50. The bars are not sorted by height but grouped by model type, with no specific visual emphasis like shading or outlines noted."
022_2505.07233v2_DynamicRAG Leveraging Outputs of Large Language Model as Feedback for   Dynamic Reranking in Retriev.pdf,022_2505.07233v2_Figure16.png,: Performance on HotpotQA.,Bar Chart,"The bar chart illustrates the performance of different models on the HotpotQA EM metric, showing a comparison between two variations, VanillaRAG and DynamicRAG, across three categories. The horizontal axis contains the categories labeled as DPR, Contriever, and MonoT5. The vertical axis represents the performance score, labeled with values from 0 to 40, using a linear scale with tick intervals of 5 units. Each category includes two bars: one for VanillaRAG, colored in green, and one for DynamicRAG, colored in yellow. The bars for VanillaRAG have approximate values of 33 for DPR, 28 for Contriever, and 30 for MonoT5. The bars for DynamicRAG show higher values at approximately 35 for DPR, 30 for Contriever, and 38 for MonoT5. The bars are grouped side by side within each category, and there is no specific sorting applied across categories. No bars are visually emphasized in terms of shading, bolding, outlining, or annotation."
022_2505.07233v2_DynamicRAG Leveraging Outputs of Large Language Model as Feedback for   Dynamic Reranking in Retriev.pdf,022_2505.07233v2_Figure17.png,: Performance on ASQA.,Bar Chart,"The bar chart illustrates the ASQA EM scores for different models, comparing VanillaRAG and DynamicRAG. The horizontal axis contains the categories, labeled as DPR, Contriever, and MonoT5, with no specific sorting apparent. The vertical axis represents the ASQA EM scores, ranging from 0 to 60 in a linear scale with tick intervals of 10 units. Each category includes two bars: VanillaRAG is depicted in green, and DynamicRAG is shown in yellow. In the DPR category, the VanillaRAG bar reaches approximately 43, while the DynamicRAG bar is taller, reaching around 53. The Contriever category shows the VanillaRAG bar at approximately 38, with the DynamicRAG bar at 49. For MonoT5, the VanillaRAG bar is around 40, and the DynamicRAG bar is slightly higher at about 50. The bars are grouped by category, with each pair of bars visually representing the two different models, and there is no visual emphasis applied to any specific bar."
022_2505.07233v2_DynamicRAG Leveraging Outputs of Large Language Model as Feedback for   Dynamic Reranking in Retriev.pdf,022_2505.07233v2_Figure18.png,"The impact of varying the number of Top-$N$ documents (Top-50, Top-100, Top-150, Top-200, Top-300, and Top-500) used for reranking on DynamicRAG performance across different benchmarks (NQ, HotpotQA, ASQA). We use Exact Match as the metric.",Line Plot,"The plot illustrates the performance metrics across different configurations for three datasets: NQ, HotpotQA, and ASQA. The x-axis is labeled with categorical values, representing ""Zero-Shot,"" ""Top-50,"" ""Top-100,"" ""Top-150,"" ""Top-200,"" ""Top-300,"" and ""Top-500,"" indicating different experimental setups. The y-axis is labeled with numerical values, with a linear scale ranging from 25 to 60, and tick labels at intervals of 5, denoting performance scores. The plot contains three lines. The first line, representing ""NQ,"" is colored green with circular markers and a solid line style. It starts at approximately 25 for ""Zero-Shot,"" peaks at about 50 by ""Top-50,"" and then shows a slight decline towards ""Top-500,"" ending just under 45. The second line, labeled ""HotpotQA,"" is orange with diamond-shaped markers and a solid line style. It begins near 25 at ""Zero-Shot,"" rises to around 35 by ""Top-50,"" and remains steady through to ""Top-500."" The third line, ""ASQA,"" is displayed in blue with x-shaped markers and a solid line style, starting at about 28 at ""Zero-Shot,"" sharply rising to around 55 by ""Top-50,"" and then showing a minor decline, ending slightly above 50 at ""Top-500."""
024_2503.15235v1_Exploring Large Language Models for Word GamesWho is the Spy.pdf,024_2503.15235v1_Figure7.png,Experiment Ticket Statistics,Pie Chart,"The image displays two pie charts representing Experiment Ticket Statistics for different combinations of categories, labeled ""JC & DC"" and ""JC & DC & SC."" The chart on the left, titled ""JC & DC,"" consists of four slices. The largest slice is labeled ""[3,0,0,0]"" and is blue, comprising 75% of the pie. The second largest, in green, is labeled ""[2,1,1,0],"" accounting for 21%. An orange slice labeled ""[2,2]"" represents 3%, and the smallest slice, in red, labeled ""[1,1,1,1]"" accounts for 1%. The chart is accompanied by percentage labels inside each slice. On the right, the pie chart titled ""JC & DC & SC"" has the same four labels. The largest slice, blue and labeled ""[3,0,0,0],"" makes up 59% of the chart. A green slice labeled ""[2,1,1,0]"" comprises 38%. The orange slice, labeled ""[2,2],"" accounts for 3%, and there is no visible red slice in this chart. The orange slice is slightly offset, emphasizing its position. Each segment includes a percentage label, and the slices appear ordered by size."
026_2505.20322v1_Beyond Prompt Engineering Robust Behavior Control in LLMs via Steering   Target Atoms.pdf,026_2505.20322v1_Figure3.png,The impact of data size on the detoxification performance of the steering vector on Gemma-2-9B-pt. “Real” is an abbreviation for RealToxicPrompts dataset.,Line Plot,"The line plot illustrates the relationship between data size for steering vectors and the defense rate. The x-axis, labeled ""Data Size for Steering Vector,"" uses a logarithmic scale with values ranging from \(2^2\) to \(2^{12}\), with tick labels at increasing powers of two. The y-axis is labeled ""Defense Rate"" and utilizes a linear scale ranging from 50 to 100, with tick labels placed every 10 units. Four lines are depicted in the plot. The blue line with circular markers, representing ""STA on SafeEdit,"" has a solid style and begins at approximately 80 at \(2^2\), ending around 90 at \(2^{12}\), showing a general upward trend. The orange line, denoting ""STA on Real,"" has circular markers and a solid style, starting around 75 at \(2^2\) and finishing close to 85 at \(2^{12}\), demonstrating a steady increase. The green line with diamond markers, indicating ""CAA on SafeEdit,"" is dashed and begins near 70 at \(2^2\), rising to about 87 at \(2^{12}\), with small fluctuations. The red line, representing ""CAA on Real"" with diamond markers in a dashed style, initiates at about 68 at \(2^2\) and ends slightly above 80 at \(2^{12}\), indicating a gradual upward trend with minor oscillations."
026_2505.20322v1_Beyond Prompt Engineering Robust Behavior Control in LLMs via Steering   Target Atoms.pdf,026_2505.20322v1_Figure4.png,: The boundary of steering.,Line Plot,"The plot illustrates the relationship between ""Multiplier"" and ""Defense Rate"" and ""Fluency"" using multiple lines for different data sets. The x-axis is labeled ""Multiplier,"" uses a linear scale, and covers a range from -10 to 10 with tick marks at intervals of 2. The primary y-axis is labeled ""Defense Rate,"" also using a linear scale, ranging from 0 to 100, with tick marks at intervals of 20. There is a secondary y-axis labeled ""Fluency,"" ranging from 0 to 6, with unspecified intervals. There are five lines in total. 

The first line, representing the ""Defense Rate of STA,"" is depicted in blue with solid circles. It begins around 50 at a multiplier of -10 and ends at approximately 80 at a multiplier of 10, showing fluctuations with a peak near a multiplier of 5. The second line, the ""Defense Rate of CAA,"" is shown in green with dashed lines and diamond markers. It starts at about 90, decreases to a low of 40 around a multiplier of -7, rises to 90 again by 10. The third line, for the ""Defense Rate of Gemma-2-9B-it,"" is gray and dot-dash styled with no markers, starting and ending slightly above 80 across the entire x-axis range, remaining relatively stable. The fourth line, ""Fluency of STA,"" is orange with a solid line and circles. It starts around 4 at -10, peaks near 5 at 0, and decreases to nearly 2 at 10. Finally, the ""Fluency of CAA"" line is red with a dashed line and diamond markers. It starts near 3 at -10, peaks near 4.5 at 0, and falls to below 1 at 10."
026_2505.20322v1_Beyond Prompt Engineering Robust Behavior Control in LLMs via Steering   Target Atoms.pdf,026_2505.20322v1_Figure5.png,: The boundary of prompting.,Line Plot,"The plot illustrates the relationship between ""n_shot"" and both ""Defense Rate"" and ""Fluency."" The x-axis is labeled ""n_shot,"" with a logarithmic scale ranging from \(2^1\) to \(2^4\), and tick labels appearing at double intervals (\(2^1\), \(2^2\), \(2^3\), \(2^4\)). The left y-axis represents ""Defense Rate"" with a linear scale from 0 to 100, with tick labels at intervals of 20. The right y-axis represents ""Fluency"" with a linear scale from 0 to 6 and tick labels at intervals of 1. The red solid line, marked with circles, represents the ""Defense Rate Of Positive Control,"" starting from approximately 80 at \(2^1\), peaking slightly above 80 at \(2^2\), decreasing back to near 80 at \(2^3\), and rising slightly above 80 at \(2^4\). The orange solid line with circles depicts the ""Defense Rate Of Negative Control,"" beginning around 60 at \(2^1\), rising to just above 80 at \(2^2\), decreasing to about 75 at \(2^3\), and then falling to approximately 50 at \(2^4\). The gray dashed line shows ""Defense Rate of Gemma-2-9B-it"" staying constant at 64 across all x values. The green dashed line with diamonds represents ""Fluency Of Positive Control,"" which remains constant at 4 across the x-axis. Lastly, the blue dashed line with diamonds indicates ""Fluency Of Negative Control,"" consistently positioned at 5 throughout the range of \(n_shot\)."
027_2505.15695v1_Can Large Language Models be Effective Online Opinion Miners.pdf,027_2505.15695v1_Figure6.png,Comparison of the OIG task performance between content-only input and input with gold tuples. Automated metric performance by model (Left) and human evaluation of outputs (Right). Detailed human evaluation is presented in Appendix <ref>.,Bar Chart,"The bar chart illustrates the impact of structured opinions on summary performance using two different metrics: A3CU and Human-Evaluated Score. The horizontal axis categorizes different models and evaluation aspects with labels such as ""GPT-4o mini,"" ""GPT-4o,"" ""Claude Haiku,"" ""Claude Sonnet,"" ""Gemma2 27B,"" and ""Llama3 70B"" for the models, and ""Faith.,"" ""Covg.,"" ""Specif.,"" ""Insght.,"" ""Intnt.,"" and ""Flcy."" for the human-evaluated scores. The vertical axis represents the A3CU score for the left graph and Human-Evaluated Score for the right, with a linear scale. The A3CU score ranges from 25 to 50 with intervals of 5, while the Human-Evaluated Score ranges from 3.2 to 4.4 with intervals of 0.2. Bars are grouped into two series, ""Content Only"" in gray and ""Content with Gold Tuple"" in blue. For each model or evaluation aspect, the bars vary in height, with the tallest A3CU bar reaching approximately 45.3 (""GPT-4o mini"" with Gold Tuple) and the shortest around 31.8 (""Claude Haiku"" with Content Only). Similarly, for the Human-Evaluated Score, the highest bar is approximately 4.4 (""Flcy."" with Content Only) and the lowest around 3.7 (""Insght."" with Content Only). Bars are not sorted in a specific order and no additional visual emphasis is applied to any of the bars."
029_2505.23015v1_Detecting Stealthy Backdoor Samples based on Intra-class Distance for   Large Language Models.pdf,029_2505.23015v1_Figure4.png,Memory requirements of clustering algorithms at different sample sizes.,Line Plot,"The plot illustrates the relationship between memory usage and the number of samples. The x-axis is labeled ""Number of Samples,"" with a linear scale ranging from 0 to 200,000, marked at intervals of 25,000. The y-axis is labeled ""Memory Usage (GB),"" also using a linear scale, ranging from 0 to 750, with tick marks at intervals of 50 GB. The plot features a single red solid line with circle markers, which indicates an increasing trend of memory usage as the number of samples increases. The line begins at a point near (0, 0), progresses through notable points like (50,000, 100), (100,000, 200), and (150,000, 400), eventually reaching around (200,000, 750), showing a consistent and upward curving trend across the range."
029_2505.23015v1_Detecting Stealthy Backdoor Samples based on Intra-class Distance for   Large Language Models.pdf,029_2505.23015v1_Figure14.png,: IWSLT2017-zh-en (Word),Bar Chart,"The bar chart illustrates the performance metrics TPR, FPR, and F1 across different categories labeled as \( p_1 \), \( p_2 \), \( p_3 \), and BLEU-4. The horizontal axis contains these category labels, presented from left to right in the order of \( p_1 \), \( p_2 \), \( p_3 \), and BLEU-4. The vertical axis represents the values of the performance metrics, labeled with a range from 0.0 to 1.0 with a linear scale, and intervals are marked at every 0.2 units. Each category has three vertically arranged bars, representing TPR (green), FPR (red), and F1 (orange). In all categories, the TPR values are uniformly the highest, reaching approximately 1.0. The FPR values are noticeably smaller, consistently near the value of 0.0 across all categories. The F1 values vary, with the bar heights around 0.6 for \( p_1 \), decreasing for \( p_2 \), \( p_3 \), and BLEU-4, where they remain relatively similar and lower than the TPR values. There is no specific sorting of the bars by height, and no bars are visually emphasized beyond their distinct colors. The grouping of bars indicates three performance metrics measured uniformly across the four given categories, as clarified by the legend."
029_2505.23015v1_Detecting Stealthy Backdoor Samples based on Intra-class Distance for   Large Language Models.pdf,029_2505.23015v1_Figure15.png,: IWSLT2017-zh-en (Syntactic),Bar Chart,"The bar chart illustrates the performance measures for various metrics labeled \(p_1\), \(p_2\), \(p_3\), and BLEU-4, with each category represented on the horizontal axis. The horizontal axis features categorical labels for the metrics, ordered as \(p_1\), \(p_2\), \(p_3\), and BLEU-4. The vertical axis represents values with the label ranging from 0.0 to 1.0, with a linear scale and tick marks at intervals of 0.2. Each set of bars within a category is grouped, containing three distinct bars for TPR, FPR, and F1 colored green, red, and orange, respectively. For \(p_1\), the green TPR bar reaches a value close to 1.0, the red FPR bar is very short, slightly above 0.0, and the orange F1 bar is approximately 0.8. In \(p_2\), the green bar also reaches nearly 1.0, with a similarly short red bar and an orange bar around 0.75. Under \(p_3\), the green bar again reaches near 1.0, the red bar is short, and the orange bar is about 0.25. For BLEU-4, the green bar remains near 1.0, with a short red bar and an orange bar around 0.45. The bars are not sorted in any particular order, and no specific bars are visually emphasized beyond color distinctions provided by the legend, which indicates the color-coding scheme for TPR, FPR, and F1."
029_2505.23015v1_Detecting Stealthy Backdoor Samples based on Intra-class Distance for   Large Language Models.pdf,029_2505.23015v1_Figure16.png,: WMT18-zh-en (Combination),Bar Chart,"The bar chart illustrates the performance metrics for different models or configurations labeled as \( p_1 \), \( p_2 \), \( p_3 \), and BLEU-4. The horizontal axis contains the categories, with each category labeled accordingly from left to right. The vertical axis represents the value of the metrics, labeled without units, ranging from 0.0 to 1.0 with a linear scale and regular tick intervals of 0.2. The bars for each category are grouped and color-coded based on the legend: TPR is shown in green, FPR in red, and F1 in orange. In category \( p_1 \), the TPR bar is approximately 0.7, FPR is near 0.0, and F1 is around 0.8. For \( p_2 \), the TPR bar reaches the maximum value of 1.0, FPR is slightly above 0.0, and F1 is approximately 0.5. In \( p_3 \), the TPR bar again reaches 1.0, FPR is below 0.2, and F1 is slightly above 0.0. In the BLEU-4 category, the TPR remains at 1.0, FPR is just above 0.0, and F1 is approximately 0.3. The bars do not appear to be sorted in any specific order, and none are visually emphasized beyond the color-coding used to denote the different metrics."
029_2505.23015v1_Detecting Stealthy Backdoor Samples based on Intra-class Distance for   Large Language Models.pdf,029_2505.23015v1_Figure17.png,: CoQA (Word),Bar Chart,"The bar chart illustrates different metrics represented by the labels TPR, FPR, and F1 for categories labeled \( p_1 \), \( p_2 \), \( p_3 \), and BLEU-4. The horizontal axis contains the categories, labeled at intervals with these four names, while the vertical axis represents the values of the metrics, marked with a linear scale ranging from 0.0 to 1.0 with tick marks at intervals of 0.2. Each category contains three stacked bars, where the green bar represents TPR, the red bar represents FPR, and the orange bar represents F1. In terms of color and value for \( p_1 \), the TPR bar is green and reaches a height of 1.0, the FPR bar is red and reaches an approximate height of 0.05, and the F1 bar is orange, reaching approximately 0.8. For \( p_2 \), the TPR bar is green and again reaches a height of 1.0, while the F1 bar is orange, reaching a height of approximately 0.6, and the FPR bar is red with a height of about 0.05. The same green bars for TPR are consistent across \( p_3 \) and BLEU-4, each reaching 1.0. For \( p_3 \), the F1 bar is slightly lower than in \( p_2 \), and for BLEU-4, the orange F1 bar is smaller still. The FPR bars across all categories remain red and comparatively shorter than their corresponding green and orange bars. No particular sorting or visual emphasis such as bolding or annotation is applied to any specific category or set of bars."
029_2505.23015v1_Detecting Stealthy Backdoor Samples based on Intra-class Distance for   Large Language Models.pdf,029_2505.23015v1_Figure18.png,: CoQA (Syntactic),Bar Chart,"The bar chart illustrates the values of three metrics: TPR, FPR, and F1 for four different categories labeled \(p_1\), \(p_2\), \(p_3\), and BLEU-4. The horizontal axis contains these categories in a left-to-right order, while the vertical axis represents the value of the metrics with a scale ranging from 0.0 to 1.0. The vertical axis is labeled with evenly spaced intervals at 0.2 from 0.0 to 1.0. Each category contains three grouped bars representing the metrics: TPR, FPR, and F1. TPR is depicted in green, FPR in red, and F1 in orange. For the category \(p_1\), the green TPR bar reaches a value of approximately 1.0, the red FPR bar is around 0.1, and the orange F1 bar is close to 0.8. Similar patterns are observed across categories \(p_2\) and \(p_3\), with the green bars consistently reaching the top of the scale, red bars remaining low, and orange bars generally higher than the red but lower than the green bars. For the BLEU-4 category, TPR remains high in green, FPR is lower in red, and F1 is moderate in orange. The bars are not sorted in any specific order beyond the categorical labels, and none of the bars in particular appear visually emphasized beyond their standard coloring."
029_2505.23015v1_Detecting Stealthy Backdoor Samples based on Intra-class Distance for   Large Language Models.pdf,029_2505.23015v1_Figure19.png,: Caption not extracted,Bar Chart,"The bar chart illustrates three performance metrics (TPR, FPR, F1) for four different processes or conditions, labeled as \( p_1 \), \( p_2 \), \( p_3 \), and BLEU-4. The horizontal axis contains these category labels, arranged from left to right with a linear scale. The vertical axis represents the value of the metrics, labeled with a numeric scale ranging from 0.0 to 1.0, with tick marks at intervals of 0.2. Each set of bars is color-coded based on a legend: TPR is green, FPR is red, and F1 is orange. For each category \( p_1 \), \( p_2 \), \( p_3 \), and BLEU-4, the green bar (TPR) consistently reaches the maximum value of approximately 1.0, while the red bar (FPR) is shortest across categories, and the orange bar (F1) is of intermediate height. The bars within each category are not sorted by height. There is no indication of emphasis or annotation on any individual bar within the sets, and each bar within a group appears distinctly colored as per the legend, without overlapping."
029_2505.23015v1_Detecting Stealthy Backdoor Samples based on Intra-class Distance for   Large Language Models.pdf,029_2505.23015v1_Figure20.png,: Caption not extracted,Bar Chart,"The bar chart illustrates various performance metrics across four categories labeled \( p_1 \), \( p_2 \), \( p_3 \), and BLEU-4. The horizontal axis represents these four categories, each of which contains three bars. The vertical axis is labeled with numeric values in a linear scale ranging from 0.0 to 1.0, marked at intervals of 0.2. The chart includes three distinct metrics represented in grouped bars: TPR, FPR, and F1, specifically colored in green, red, and orange, respectively. In each category, the green bars representing TPR consistently reach an approximate value of 1.0, indicating they are the tallest. The red bars, representing FPR, have lower values approximately ranging from around 0.2 to 0.3. The orange bars, representing F1, vary more, reaching values slightly above 0.5 in the first two categories and slightly below in the last two. The bars are grouped by category, and no specific sorting is applied to the categories on the horizontal axis. The chart does not visually emphasize any bars, as they are uniformly presented without bolding, shading, or additional annotations. The legend located in the upper-left corner distinctly identifies each metric by color."
029_2505.23015v1_Detecting Stealthy Backdoor Samples based on Intra-class Distance for   Large Language Models.pdf,029_2505.23015v1_Figure21.png,: Caption not extracted,Bar Chart,"The bar chart illustrates several performance metrics for different predictions labeled as \(p_1\), \(p_2\), \(p_3\), and BLEU-4. The horizontal axis contains these categories, and the vertical axis represents the values of the metrics, labeled from 0.0 to 1.0, using a linear scale with a tick label interval of 0.2. Each category contains a grouped set of three bars, each representing a different metric: TPR (green), FPR (red), and F1 (orange), as indicated by the legend. These bars exhibit various heights, with TPR consistently reaching the maximum value of 1.0 across all categories. The FPR bars show lower values, approximately around 0.2 for \(p_1\) and BLEU-4, and slightly higher for \(p_2\) and \(p_3\). The F1 bars are consistently shorter than the TPR bars but taller than the FPR bars, with values slightly below the TPR values in each category. The bars are not sorted in any particular order and there are no additional visual emphases such as shading or bolding on any specific bars."
029_2505.23015v1_Detecting Stealthy Backdoor Samples based on Intra-class Distance for   Large Language Models.pdf,029_2505.23015v1_Figure22.png,: Caption not extracted,Bar Chart,"The bar chart illustrates a comparison of three metrics (TPR, FPR, and F1) across four different categories labeled \(p_1\), \(p_2\), \(p_3\), and BLEU-4. The categories are represented on the horizontal axis, while the vertical axis represents the metric values, ranging from 0.0 to 1.0 with a linear scale and tick intervals of 0.2. Each category contains a set of three stacked bars, where TPR is shown in green, FPR in red, and F1 in orange. In the \(p_1\) category, the TPR bar reaches a height of 1.0, FPR is approximately 0.1, and F1 is around 0.4. Categories \(p_2\) and \(p_3\) display similar patterns with TPR at 1.0, FPR near 0.2, and F1 hovering slightly above 0.4. For BLEU-4, TPR also reaches 1.0, FPR is slightly below 0.2, and F1 maintains a similar height to the other categories. Bars are not specifically sorted in any apparent order, and there is no visual emphasis on any particular bar apart from the color coding indicated by the legend."
029_2505.23015v1_Detecting Stealthy Backdoor Samples based on Intra-class Distance for   Large Language Models.pdf,029_2505.23015v1_Figure25.png,: WMT18-zh-en (Combination),Bar Chart,"The bar chart illustrates the frequency distribution of sample confidence for two different categories, labeled as ""Clean Sample"" and ""Backdoor Sample."" The horizontal axis contains the categories and is labeled as ""Sample Confidence,"" with values ranging from 0 to 100 on a linear scale, marked at intervals of 10. The vertical axis represents the ""Frequency,"" presented on a logarithmic scale, with values ranging from 10^-5 to 10^0. The bars are colored green for the ""Clean Sample"" and red for the ""Backdoor Sample,"" as indicated in the legend. In terms of arrangement, the bars are juxtaposed for each confidence value, demonstrating the distribution for both sample types simultaneously. The shortest bars are at 10^-5, while the tallest reaches approximately 10^0. The bars are not explicitly sorted in any particular order, aside from the natural sequence of confidence values. There are no visually emphasized bars evident in the chart through shading or outlining."
029_2505.23015v1_Detecting Stealthy Backdoor Samples based on Intra-class Distance for   Large Language Models.pdf,029_2505.23015v1_Figure27.png,: CoQA (Syntactic),Bar Chart,"The bar chart illustrates the frequency distribution of sample confidence for two categories: Clean Sample and Backdoor Sample. The horizontal axis contains the categories labeled ""Sample Confidence,"" ranging from 0 to 100, with a linear scale. The vertical axis represents ""Frequency"" and uses a logarithmic scale, ranging from 10^-4 to 10^-1 with markings for 10^-4, 10^-3, 10^-2, and 10^-1. Each bar represents the frequency of samples within a specific confidence interval. The bars for Clean Sample are colored green, and the Backdoor Sample bars are colored red. Clean Sample bars are shown for the entire range, with heights decreasing as the confidence increases towards 100. Clean Sample bars start around a frequency of 10^-1 and decrease to around 10^-4. The Backdoor Sample bars are concentrated at the lower confidence values, with the tallest bars appearing close to zero and decreasing rapidly. There is no specific sorting pattern for the bars, and they are not visually emphasized through shading, bolding, or outlining. Each group of bars within the same confidence interval is placed adjacent to each other, with Clean Sample bars generally being taller across the confidence range. There are no specific annotations indicating further emphasis or interpretation."
029_2505.23015v1_Detecting Stealthy Backdoor Samples based on Intra-class Distance for   Large Language Models.pdf,029_2505.23015v1_Figure29.png,: Caption not extracted,Bar Chart,"The bar chart illustrates the distribution of sample confidence levels comparing two types of samples: Clean Sample and Backdoor Sample. The horizontal axis represents the sample confidence, labeled as ""Sample Confidence,"" and spans values from 0 to 100 with irregular intervals. The vertical axis indicates the frequency of each confidence level, labeled ""Frequency,"" with a logarithmic scale ranging from \(10^{-4}\) to \(10^{0}\). The chart contains two sets of bars distinguished by color: green for Clean Samples and red for Backdoor Samples. Each bar represents the frequency of a specific confidence level, with bars extending vertically from the baseline. The presented values show that Clean Samples have higher frequencies at specific confidence levels such as 0, 20, and 100, while Backdoor Samples have higher frequencies at lower confidence levels around 0 to 20. The legend at the top specifies the colors representing each sample type. There is no indication of sorting order for the bars, and no additional emphasis, such as shading or bolding, is applied to any specific bar or group."
029_2505.23015v1_Detecting Stealthy Backdoor Samples based on Intra-class Distance for   Large Language Models.pdf,029_2505.23015v1_Figure30.png,: Caption not extracted,Bar Chart,"The bar chart illustrates the frequency of samples with varying confidence levels, differentiated between clean and backdoor samples. The horizontal axis represents ""Sample Confidence,"" labeled from 0 to 100 with a linear scale, indicating different confidence intervals without specific tick marks. The vertical axis, labeled ""Frequency,"" uses a logarithmic scale ranging from \(10^{-4}\) to \(10^{0}\), with tick labels at regular intervals corresponding to powers of ten. Bars are colored either green for ""Clean Sample"" or red for ""Backdoor Sample,"" as indicated by the legend. Each bar represents a range of sample confidence, with the frequency shown by the relative height of the bar on the logarithmic scale. For clean samples, the green bars have varying heights across the confidence range but tend to be taller at extreme ends of the scale, particularly near confidence levels of 0 and 100. Conversely, red bars for backdoor samples are clustered towards the lower confidence range, notably peaking between the 0 to 20 confidence interval. No particular visual emphasis, such as shading or annotation, is given to any bar, and the bars display the data unsorted beyond the arrangement by the horizontal confidence axis."
037_2505.17098v1_TACO Enhancing Multimodal In-context Learning via Task Mapping-Guided   Sequence Configuration.pdf,037_2505.17098v1_Figure10.png,Visualization of the in-context lens for different methods under the Harder-Mapping setting.,Pie Chart,"The chart displays the distribution of different categories across layers for three models: IQPR, Lever-LM, and Ta-ICL. Each model is represented by four pie charts, corresponding to layers 12, 18, 24, and 30. Each pie chart is divided into four slices labeled as Deep, Shallow, Correct, and Wrong, depicted in red, orange, blue, and green respectively. The charts provide specific percentage values for each category. The IQPR model at layer 12 shows proportions of 18.5% Deep, 62.8% Shallow, 8.3% Correct, and 10.4% Wrong. As the layers progress to 18, 24, and 30, the distributions shift slightly, with layer 30 showing a noticeable increase in the Correct category to 62.4%. For the Lever-LM model, layer 12 has 16.9% Deep, 73.6% Shallow, 7.2% Correct, and 2.3% Wrong. By layer 30, the Correct category increases significantly to 66.7%. The Ta-ICL model also shows a similar trend where at layer 12, the categories are distributed as 19.3% Deep, 67.4% Shallow, 8.6% Correct, and 4.7% Wrong, while by layer 30, Correct reaches 70.9%. Each slice includes percentage labels and the segments are ordered roughly according to their size. A legend is provided below the charts to clarify the color coding of each category."
038_2505.00030v1_Can Language Models Represent the Past without Anachronism.pdf,038_2505.00030v1_Figure1.png,"The stylistic center of mass of prompted and period-pretrained models. Kernel density plots for ground truth passages drawn from books, 1905–1914, as well as continuations of those passages by GPT-4o (1-shot and 20-shot) and GPT-1914 (trained only on text 1880–1914).",Line Plot,"The plot illustrates the density distribution of passage continuations' publication years as perceived by a RoBERTa model trained on COHA. The x-axis represents the publication year, marked as ""Publication year of passage continuations, as perceived by a RoBERTa model trained on COHA,"" with a linear scale ranging from 1825 to 2025 and tick labels at intervals of 25 years. The y-axis is labeled ""Density,"" with a linear scale ranging from 0 to 0.025, with tick labels at intervals of 0.005. The plot contains four lines. The maroon line, labeled ""GPT-4o 1-shot,"" is represented by a dashed style and shows density mostly rising from near zero around 1825, peaking slightly above 0.020 around the year 2000, then decreasing sharply. The light blue line, ""GPT-4o 20-shot,"" also dashed, follows a similar trend, starting near zero, peaking just above 0.025 around 2000, and subsequently declining. The orange dotted line, ""GPT-1914,"" begins at a low density near 1825, peaks near 0.020 between 1900 and 1925, and then descends. The green solid line, ""Ground Truth,"" starts near zero, rises to a peak density below 0.015 around 1925, and gradually decreases thereafter."
038_2505.00030v1_Can Language Models Represent the Past without Anachronism.pdf,038_2505.00030v1_Figure2.png,"The stylistic center of mass of fine-tuned and period-pretrained models. Kernel density plots for ground truth passages drawn from books 1905–1914, as well as continuations of those passages by GPT-1914 and by a version of GPT-4o-mini fine-tuned on passages 1905–1914.",Line Plot,"The plot illustrates the density of publication years of passage continuations as perceived by a RoBERTa model trained on COHA, represented by three distinct lines. The x-axis is labeled ""Publication year of passage continuations, as perceived by a RoBERTa model trained on COHA"" and has a linear scale ranging from 1825 to 2025, with tick marks spaced at intervals of 25 years. The y-axis is labeled ""Density,"" also on a linear scale, with values ranging from 0.0000 to 0.0175, marked at intervals of 0.0025. The purple dashed line represents ""GPT-4o-mini fine-tuned,"" starting at approximately 0.0000 in 1825, peaking around 0.0125 between 1880 and 1925, then decreasing towards 0.0000 by 2025. The orange dotted line represents ""GPT-1914,"" beginning at approximately 0.0000 in 1825, peaking at about 0.0175 around 1900, then decreasing to near 0.0000 by 2025. The green solid line signifies ""Ground Truth,"" starting around 0.0000 in 1825, peaking near 0.0150 around the year 1900, and declining smoothly to approximately 0.0000 by 2025."
043_2505.13534v1_InterFeat An Automated Pipeline for Finding Interesting Hypotheses in   Structured Biomedical Data.pdf,043_2505.13534v1_Figure2.png,"Hierarchical clustering of recurring features across 8 disease targets. Features marked as ""interesting"" by the LLM annotation models were grouped into semantic categories. The number in each section is the number of times features in that group were judged as interesting by models across targets",Pie Chart,"The pie chart represents the distribution of various factors related to a study or dataset, encompassing a total of nine categories. The segments are labeled as follows: ""Blood/Urine Biomarkers,"" shown in purple, constitutes the largest portion at 178; ""Genetic Risk,"" in light blue, follows with 151; ""Disease Diagnoses,"" in violet, accounts for 148; ""Lifestyle & Environment,"" in pink, makes up 92; ""Mental Health,"" in light pink, encompasses 58; ""Medication & Interventions,"" in light green, includes 49; ""Clinical Measurements,"" in teal, comprises 26; ""Body Composition,"" in orange-red, and ""Socio-Demographics,"" in peach, both are 24; and ""Family History,"" in light orange, is the smallest at 13. Each slice is labeled with its category name and number directly inside. The chart does not include additional features such as a legend, slice emphasis, or an ordered arrangement based on size."
043_2505.13534v1_InterFeat An Automated Pipeline for Finding Interesting Hypotheses in   Structured Biomedical Data.pdf,043_2505.13534v1_Figure3.png,"A two-level hierarchical sunburst plot of the recurring features, in semantic clusters. The inner ring represents broad categories (e.g., Genetic Risk, Metabolic Markers, Disease Diagnoses), while the outer ring refines these into more specific subgroups. Features marked as ""interesting"" by the LLM annotation models were grouped into semantic categories. The number in each section indicates the count of times features in that group were judged as interesting by models across disease targets.",Pie Chart,"The pie chart represents various categories related to health factors. There are a total of 31 slices. The main slices and their approximate proportions are: ""Blood/Urine Biomarkers"" (dark green, 178), ""Genetic Risk"" (red, 151), ""Disease Diagnoses"" (green, 148), ""Lifestyles & Environment"" (orange, 92), ""Lipids & Cholesterol"" (light green, 93), ""Mental Health"" (blue, 58), ""Metabolites"" (lightest green, 60), ""Musculoskeletal"" (teal, 60), ""Cardiometabolic"" (peach, 40), ""Diet"" (cream, 45), ""Physical Environment"" (light orange, 38), and others continuing in various colors with smaller proportions such as ""Immune"" (red, 26) and ""Medications"" (purple, 29). Some slices have labels inside the segments, and the chart includes a legend correlating to the colors and categories. The segments appear to be ordered in descending size, but none of the slices are visually emphasized such as being exploded or offset."
044_2504.15784v1_Automated Creativity Evaluation for Large Language Models A   Reference-Based Approach.pdf,044_2504.15784v1_Figure1.png,Distribution of TTCW test scores across different models. The dashed lines indicate the average number of tests passed by each model.,Bar Chart,"The bar charts represent the number of stories that passed a certain threshold, labeled as ""#TTCW passed,"" for different categories. Each sub-chart is aligned horizontally, with the category axis, labeled as ""#TTCW passed,"" displayed horizontally, ranging from 0 to 14 with integer increments. The vertical value axis is labeled ""# Stories,"" ranging linearly from 0 to 12 with integer tick intervals. In the first chart, the bars are shaded in light pink and labeled ""GPT3.5,"" with values primarily at the 0 position and a shorter bar at 2, marked by an average line at approximately 0.33. The second chart features light orange bars labeled ""GPT4,"" with the tallest bars around 3 and 4, and a shorter bar at 7, averaging around 2.67. The third chart has light green bars labeled ""Claude V1.3,"" peaking at 3 and 4, with some presence at 8, averaging around 3.25. The fourth chart, with light purple bars labeled ""NewYorker,"" shows a significant bar at 13 and a shorter one at 11, averaging around 12.75. Each chart includes a vertical dashed line indicating the average value for that model. The bars within each chart are not sorted in any specific order beyond numerical progression along the category axis, and no bars are visually emphasized beyond the coloring and average lines."
044_2504.15784v1_Automated Creativity Evaluation for Large Language Models A   Reference-Based Approach.pdf,044_2504.15784v1_Figure5.png,Spearman correlation performance under different cutoff scores.,Line Plot,"The plot illustrates the performance of different models under varying cutoff scores, represented by the Spearman Correlation. The x-axis is labeled ""Cutoff Score,"" uses a linear scale, and ranges from -3.0 to 0.0 with tick marks spaced at intervals of 0.5. The y-axis is labeled ""Spearman Correlation,"" also uses a linear scale, and ranges from -0.1 to 0.5 with tick marks at intervals of 0.1. Six lines are present in the plot: a solid blue line with circle markers labeled ""Qwen,"" showing values at or below 0.0 across the x-axis; a dashed cyan line with square markers labeled ""Qwen-Ours,"" starting at 0.2 at -3.0, declining to 0.1 at -1.5, and increasing to 0.2 at 0.0; a solid green line with circle markers labeled ""GPT-4o,"" consistently at 0.2 throughout; a dashed green line with square markers labeled ""GPT-4o-Ours,"" peaking at 0.4 at -2.0 and decreasing to 0.3 at 0.0; a solid red line with circle markers labeled ""Claude 3.5,"" remaining at 0.1 across the x-axis; and a dashed orange line with square markers labeled ""Claude 3.5-Ours,"" peaking at 0.5 at -2.0, showing a descending trend to 0.3 by 0.0."
045_2505.23038v1_EL4NER Ensemble Learning for Named Entity Recognition via Multiple   Small-Parameter Large Language.pdf,045_2505.23038v1_Figure3.png,": Micro-f1 scores of three EL4NER variants  (using Phi-4, Qwen-2.5-14B-Instruct, and GLM-4-9B-Chat as verifer, respectively) on the three datasets.",Bar Chart,"The bar chart illustrates the Micro-F1 scores, expressed in percentages, across different datasets for three models: Qwen-2.5-14B-Inst., Phi-4, and GLM-4-9B-Chat. The horizontal axis categorizes the datasets, labeled as WNUT17, ACE05, and GENIA, while the vertical axis represents the Micro-F1 Score with a linear scale ranging from 0% to 80%, with increments of 20%. For the WNUT17 dataset, the bars are colored in green, orange, and blue, representing Qwen-2.5-14B-Inst., Phi-4, and GLM-4-9B-Chat respectively, with approximate values around 60%. For ACE05, the green bar reaches slightly above 60%, the orange bar is marginally higher, and the blue bar extends to approximately 65%. In the GENIA dataset, the green bar measures just above 60%, while both the orange and blue bars appear similar in height, slightly above this mark. No specific sorting or visual emphasis is applied to the bars; they are grouped by dataset, with each model consistently represented by its designated color across all datasets."
045_2505.23038v1_EL4NER Ensemble Learning for Named Entity Recognition via Multiple   Small-Parameter Large Language.pdf,045_2505.23038v1_Figure4.png,": Micro-f1 scores of three EL4NER variants  (using one, two, and three backbones, respectively) on the three datasets.",Bar Chart,"The bar chart illustrates the Micro-F1 Score achieved by different methods on three datasets: WNUT17, ACE05, and GENIA, with the purpose of comparing model performances. The horizontal axis contains the categories labeled as Dataset, ordered as WNUT17, ACE05, and GENIA. The vertical axis represents the Micro-F1 Score, using a linear scale ranging from 0 to 70, with labels every 20 units. Each dataset category includes three grouped bars, representing different methods: GLM-4-9B-Chat (blue), +Phi-4 (orange), and +Qwen-2.5-14B-Inst. (green). On the WNUT17 dataset, the bars have approximate values of 50 for GLM-4-9B-Chat, 55 for +Phi-4, and 60 for +Qwen-2.5-14B-Inst. For ACE05, values are around 65, 66, and 66 respectively, and for GENIA, all methods similarly reach approximately 66. The bars are not sorted in any particular order beyond their categorical grouping, and there is no visual emphasis or annotation on any specific bars."
045_2505.23038v1_EL4NER Ensemble Learning for Named Entity Recognition via Multiple   Small-Parameter Large Language.pdf,045_2505.23038v1_Figure5.png,EL4NER performance vs. #demonstrations in the three datasets.,Line Plot,"The plot illustrates how the Micro-F1 Score varies with the number of demonstrations across three datasets. The x-axis is labeled ""#Demonstrations,"" has a linear scale, ranges from 0 to 50, and numeric tick labels are marked at intervals of 10. The y-axis is labeled ""Micro-F1 Score,"" also with a linear scale, ranging from 50 to 70, with tick labels at intervals of 5. There are three lines: a blue line with circle markers representing ""ACE2005,"" a solid style starts around a score of 66 at 0 demonstrations and ends just above 70 at 50 demonstrations, showing a generally upward trend with minor fluctuations. An orange line with triangle markers represents ""GENIA,"" also solid, beginning slightly below 70 and remaining relatively stable across the x-axis, with minor variation. The green line with square markers represents ""WNUT17,"" with a solid style starting at about 54 and exhibiting a fluctuating trend, ending slightly above 55. Vertical dashed lines are present at approximately 40 for ""ACE2005"" and 10 for ""GENIA,"" marking certain points within their respective datasets."
"047_2504.15448v1_Real-Time Sentiment Insights from X Using VADER, DistilBERT, and   Web-Scraped Data.pdf",047_2504.15448v1_Figure2.png,Corporate Sentiment Index (CSI) Across Major Companies,Bar Chart,"The bar chart illustrates the Company Sentiment Index for various companies, categorized by an index scale ranging from 0 to 100. The horizontal axis represents the sentiment index labeled as ""Sentiment Index (0-100),"" using a linear scale from 0 to 100, with tick intervals of 10 units. The vertical axis lists the companies, labeled ""Company,"" arranged in descending order based on the sentiment index values. Each bar is color-coded according to the classification legend on the chart's right side, which includes four categories: Excellent (dark green), Good (light green), Average (yellow), and Poor (red). Microsoft has the shortest bar with an index value of 21.7, shaded in red indicating a ""Poor"" classification, while Amazon has the longest bar with an index value of 81.2, colored in dark green to signify an ""Excellent"" classification. Intermediate values include Visa at 33.5 in orange, Tencent at 35.3 in light green, and Samsung at 45.8 in dark green. The bars are visually differentiated by color and length to represent sentiments ranging from ""Poor"" to ""Excellent,"" without additional emphasis given to specific bars."
051_2505.18331v1_PerMedCQA Benchmarking Large Language Models on Medical Consumer   Question Answering in Persian Lan.pdf,051_2505.18331v1_Figure2.png,Gender Distribution in PerMedCQA,Bar Chart,"The bar chart illustrates the distribution of final gender categories, with each category being displayed on the horizontal axis labeled as ""Gender."" The categories are ""Male,"" ""Female,"" and ""Unknown,"" ordered from left to right. The vertical axis is labeled ""Count,"" and features a linear scale ranging from 0 to 40,000 with tick intervals at every 5,000 units. The bars represent the count of individuals in each gender category, and they are colored uniquely: ""Male"" is light blue with a count of approximately 17,949, ""Female"" is pink with a count of approximately 36,091, and ""Unknown"" is gray with a count of approximately 14,096. Each bar also includes the percentage representation above it: 26.3% for ""Male,"" 53.0% for ""Female,"" and 20.7% for ""Unknown."" The bars are not sorted in any specific way beyond the given categories, and there is no additional visual emphasis on any particular bar."
051_2505.18331v1_PerMedCQA Benchmarking Large Language Models on Medical Consumer   Question Answering in Persian Lan.pdf,051_2505.18331v1_Figure3.png,Distribution of ICD-11 Categories in PerMedCQA,Bar Chart,"The bar chart illustrates the distribution of ICD-11 categories within a QA dataset, with each bar representing a different category. The categories are displayed on the vertical axis, labeled ""ICD-11 Category,"" with labels arranged in descending order based on the number of instances. The horizontal axis is labeled ""Number of Instances"" and uses a linear scale with values ranging from 0 to 10,000 and tick intervals of 1,000 units. Each bar corresponds to a specific ICD-11 category and is uniquely colored. The longest bar represents ""Sexual health"" with approximately 9,266 instances, displayed in blue. ""Digestive system"" follows with about 6,869 instances, colored in orange, and ""Endocrine/metabolic"" with 6,721 instances, in green. The shortest bars, representing ""Extension codes"" and ""Special purposes,"" each have a count of 1 and are colored in pink and green, respectively. The bars are sorted in descending order by the number of instances, and no bars appear visually emphasized beyond color differentiation for each category."
051_2505.18331v1_PerMedCQA Benchmarking Large Language Models on Medical Consumer   Question Answering in Persian Lan.pdf,051_2505.18331v1_Figure4.png,Distribution of Question Type in PerMedCQA,Bar Chart,"The bar chart illustrates the distribution of different categories labeled as ""QuestionType_Tag"" within a dataset, utilizing a logarithmic scale for depiction. The horizontal axis presents the categories, each labeled with terms such as ""Information,"" ""Indication,"" and ""Side Effects,"" listed from left to right in descending order based on their counts, though not alphabetically. The vertical axis, labeled ""Count (log scale),"" uses a logarithmic scale with intervals at powers of 10, ranging from \(10^1\) to slightly above \(10^4\). Each bar is colored uniformly blue. The tallest bar, labeled ""Information,"" reaches a value of 25,788, while the shortest, labeled ""Manufacturer,"" measures 8. Other notable values include ""Indication"" at 17,386 and ""Pronunciation"" at 10. These categories are neither grouped nor stacked, and there is no visual emphasis on any particular bar aside from their standard presentation, which provides a clear visual comparison of the frequency of each question type in the dataset."
051_2505.18331v1_PerMedCQA Benchmarking Large Language Models on Medical Consumer   Question Answering in Persian Lan.pdf,051_2505.18331v1_Figure7.png,Correctness for baselines and advanced Prompting,Bar Chart,"The bar chart illustrates the correctness distribution by experiment, categorizing different experimental approaches and their corresponding correctness levels. The horizontal axis displays the categories, labeled as ""correct,"" ""partially_correct,"" ""incorrect,"" and ""contradictory,"" with a linear scale type. The vertical axis represents the count, with numeric values ranging from 0 to 3500, increasing at intervals of 500. Colors differentiate the bars according to a legend, with each color representing an experimental baseline or role, such as baseline models like ""bioMistral"" and ""gpt41,"" among others. The ""incorrect"" category exhibits a particularly tall light blue bar, representing ""baseline::gpt41nano,"" reaching approximately 3300 in count. Most bars in the other categories remain below 1000, with ""correct"" and ""contradictory"" categories showing notably short bars across all experiments. The chart does not indicate sorting by any specific criteria, and no bars are visually emphasized beyond the color-coding that identifies each experimental condition. Grouped bars within each category share the same order as presented in the legend, with colors corresponding to each experimental label."
051_2505.18331v1_PerMedCQA Benchmarking Large Language Models on Medical Consumer   Question Answering in Persian Lan.pdf,051_2505.18331v1_Figure8.png,Coverage for baselines and advanced Prompting,Bar Chart,"The bar chart displays the ""Coverage distribution by experiment,"" illustrating the counts across different experimental categories. The horizontal axis contains the categories labeled as ""equal,"" ""expert_subset,"" ""model_subset,"" and ""overlap_none,"" with a linear scale, while the vertical axis represents the ""Count"" with a linear scale ranging from 0 to 3000, with tick marks at intervals of 500. Each category contains multiple bars, each representing a different baseline or condition identified by corresponding colors, as indicated in the legend. For ""equal,"" the values of bars are significantly lower, with heights all under 100. In the ""expert_subset"" category, the bars reach their peak, with the tallest bar, colored light blue, corresponding to the baseline labeled ""bioMistral,"" reaching a value slightly above 2500. The ""model_subset"" bars have moderate heights, with no bar exceeding 1000. In the ""overlap_none"" category, the bars are shorter than in ""expert_subset"" but feature one notably tall blue bar under ""bioMistral,"" surpassing 2000. The chart is not sorted in a specific order, and no bars are visually emphasized beyond color differentiation, with the use of a legend on the right to identify each color-to-baseline correspondence."
051_2505.18331v1_PerMedCQA Benchmarking Large Language Models on Medical Consumer   Question Answering in Persian Lan.pdf,051_2505.18331v1_Figure9.png,Critical impact for baselines and advanced Prompting,Bar Chart,"The bar chart illustrates the distribution of clinical impact by experiment, categorized into four main groups: negligible, moderate, significant, and critical. The horizontal axis lists these categories in angled labels, while the vertical axis represents the count of occurrences, with a linear scale ranging from 0 to 3500, marked at intervals of 500. Each group contains multiple colored bars representing different experiments as indicated by the legend. In the negligible category, bars are generally between 1000 to 2000, with colors such as red, blue, green, and various others. For the moderate category, counts appear slightly lower, mostly between 500 and 1500, following a similar color variety. The significant category has shorter bars, mostly below 1000, while the critical category contains one notably tall blue bar representing ""baseline::bioMistral,"" reaching about 3200, with the rest of the bars significantly shorter. The layout shows grouped bars for each impact level, and colors aid in distinguishing each experiment, without any indication of sorting or visual emphasis aside from the notable height difference in the critical category."
055_2504.00927v1_Multi-Token Attention.pdf,055_2504.00927v1_Figure5.png,(left) Average accuracy on QA1-5 tasks in BabiLong. The models are all pretrained with 2K context and then finetuned with 4K context. Distraction text length varies from 0K (no distraction) to 4K tokens.  consistently outperforms the baseline models. (right) Ablation on the number of  layers with key-query convolutions (head convolution is applied to all layers). We report average validation perplexity on SlimPajama.,Line Plot,"The plot illustrates the average performance on Task QA1-5 by comparing the accuracy of four different models as the level of distraction increases. The x-axis represents ""Distraction"" with a linear scale ranging from 0K to 4K, with tick labels at intervals of 1K, indicating the number of distractions. The y-axis represents ""Accuracy (%)"" on a linear scale ranging from 20% to 45%, with tick labels at intervals of 5%. Four lines are depicted in the plot. The gray line with circular markers represents the ""Transformer"" model, starting at approximately 36% accuracy at 0K distractions and declining steadily to about 26% at 4K distractions. The teal line with star markers represents the ""Differential Transformer"" model, beginning at around 41% accuracy at 0K and decreasing steadily to roughly 30% at 4K. The pink line with square markers corresponds to the ""MTA"" model, starting at about 44% accuracy at 0K, with a gradual decline to approximately 35% at 4K. Lastly, the lime green line with triangular markers represents the ""MTA w/o group norm"" model, beginning at around 42% accuracy at 0K and ending at roughly 31% at 4K, showing a consistent decrease."
055_2504.00927v1_Multi-Token Attention.pdf,055_2504.00927v1_Figure7.png,"Accuracy (%) on QA1-5 tasks in BabiLong benchmark. Note the random performance on QA3 is 16.67%, thus all models perform poorly on QA3. On other tasks, MTA demonstrate strong performance compared to baselines especially when there is lengthy distraction text (4K).",Line Plot,"The plot illustrates the accuracy of different models under varying levels of distraction in the context of QA1: Single Supporting Facts. The x-axis is labeled ""Distraction"" with a linear scale ranging from 0K to 4K, with tick marks at intervals of 1K. The y-axis is labeled ""Accuracy (%)"" with a linear scale ranging from 25% to 55%, with tick marks at 5% intervals. There are four lines in the plot. The first line, colored gray with circular markers and a solid style, represents the ""Transformer"" model, starting at approximately 45% accuracy at 0K and decreasing to about 30% at 4K. The second line is teal with star markers and a solid style, representing the ""Differential Transformer"" model. It begins at around 47% and declines to around 33%. The third line is pink with square markers and a solid style representing the ""MTA"" model, starting near 55% and declining to approximately 40%. The fourth line is green with triangle markers and a solid style, representing ""MTA w/o group norm,"" starting at about 53% and declining to around 37%. All lines show a decreasing trend in accuracy as the distraction level increases."
055_2504.00927v1_Multi-Token Attention.pdf,055_2504.00927v1_Figure8.png,"Accuracy (%) on QA1-5 tasks in BabiLong benchmark. Note the random performance on QA3 is 16.67%, thus all models perform poorly on QA3. On other tasks, MTA demonstrate strong performance compared to baselines especially when there is lengthy distraction text (4K).",Line Plot,"The plot illustrates the relationship between distraction levels and accuracy for different models in the context of QA2 with two supporting facts. The x-axis represents ""Distraction"" with a linear scale ranging from 0K to 4K, marked at intervals of 1K. The y-axis represents ""Accuracy (%)"" also on a linear scale, ranging from 10% to 30%, with no specific interval labels shown. Four lines are depicted in the plot: a gray solid line with circular markers representing the ""Transformer"" model, starting near 18% accuracy at 0K and declining to around 12% at 4K; a teal solid line with star markers representing the ""Differential Transformer"" model, beginning at approximately 22% at 0K and decreasing to about 15% at 4K; a pink solid line with square markers representing the ""MTA"" model, which starts at about 30% and drops steadily to near 16% by 4K; and a green solid line with triangle markers for the ""MTA w/o group norm"" model, starting just above 25% and similarly declining to around 16% at 4K."
055_2504.00927v1_Multi-Token Attention.pdf,055_2504.00927v1_Figure9.png,"Accuracy (%) on QA1-5 tasks in BabiLong benchmark. Note the random performance on QA3 is 16.67%, thus all models perform poorly on QA3. On other tasks, MTA demonstrate strong performance compared to baselines especially when there is lengthy distraction text (4K).",Line Plot,"The plot illustrates the change in accuracy percentage in relation to the distraction level for four different models evaluated in a QA task with three supporting facts. The x-axis represents ""Distraction,"" with a linear scale ranging from 0K to 4K, with tick labels at intervals of 1K. The y-axis denotes ""Accuracy (%)"" on a linear scale, ranging from 8% to 20%, with tick labels at intervals of 2%. The gray line with circle markers represents the ""Transformer"" model, showing an increase from 8% at 0K to its peak around 16% at 2K, followed by a slight decline to approximately 15% at 4K. The green line with triangle markers stands for ""MTA w/o group norm,"" rising steeply from 8% at 0K to a peak at 21% at 1K, then declining to around 18% at 4K. The teal line with star markers signifies the ""Differential Transformer,"" beginning at 8% at 0K, peaking slightly over 18% at 1K, staying relatively consistent, and ending around 17% at 4K. Lastly, the pink line with square markers depicts ""MTA,"" starting at approximately 13% at 0K, slightly increasing to nearly 16% at 1K, and remaining stable near that level throughout to 4K."
055_2504.00927v1_Multi-Token Attention.pdf,055_2504.00927v1_Figure10.png,"Accuracy (%) on QA1-5 tasks in BabiLong benchmark. Note the random performance on QA3 is 16.67%, thus all models perform poorly on QA3. On other tasks, MTA demonstrate strong performance compared to baselines especially when there is lengthy distraction text (4K).",Line Plot,"The plot illustrates the impact of distraction on accuracy for various models in a QA4: Two Argument Relations task. The x-axis represents ""Distraction"" in a linear scale, with values ranging from 0K to 4K. The tick labels are spaced at 0K, 1K, 2K, and 4K intervals. The y-axis represents ""Accuracy (%)"" also in a linear scale, with values ranging from 20 to 50, with tick intervals at 5%. The plot includes four lines. The gray line with circular markers represents the ""Transformer"" model, starting at approximately 33% accuracy at 0K distraction and decreasing steadily to about 21% at 4K. The teal line with cross markers represents the ""Differential Transformer"" model, starting at 49% at 0K and decreasing gradually to 33% at 4K. The pink line with square markers represents the ""MTA"" model, beginning at 48% accuracy and declining to 34% as distraction increases. Lastly, the green line with triangular markers represents the ""MTA w/o group norm"" model, starting at approximately 37% and decreasing to 25% at 4K. Each line shows a decreasing trend as distraction increases."
055_2504.00927v1_Multi-Token Attention.pdf,055_2504.00927v1_Figure11.png,"Accuracy (%) on QA1-5 tasks in BabiLong benchmark. Note the random performance on QA3 is 16.67%, thus all models perform poorly on QA3. On other tasks, MTA demonstrate strong performance compared to baselines especially when there is lengthy distraction text (4K).",Line Plot,"The line plot illustrates the relationship between distraction levels and accuracy percentages for four different systems in the context of QA5: Three Argument Relations. The x-axis is labeled ""Distraction"" and uses a linear scale, with values ranging from 0K to 4K, marked at intervals of 1K. The y-axis is labeled ""Accuracy (%)"" and also uses a linear scale, with values ranging from 20 to 80, marked at intervals of 10. The plot features four lines. The gray line, representing ""Transformer,"" uses circle markers and a solid line style. It starts at 70% accuracy with a distraction level of 0K and decreases steadily to below 30% at 4K distraction. The blue line, representing ""Differential Transformer,"" has star markers and a solid line style, starting at 75% and reducing to just over 30% from 0K to 4K distraction. The pink line, representing ""MTA,"" features square markers and a solid line style, starting at approximately 72% accuracy and declining gently to about 45% at the end of the x-axis. The green line, representing ""MTA w/o group norm,"" uses triangle markers and a solid line style, commencing at 78% and decreasing to around 40% as the distraction level increases from 0K to 4K. Each line shows a downward trend in accuracy as the distraction level increases."
056_2503.23798v2_Adaptive Layer-skipping in Pre-trained LLMs.pdf,056_2503.23798v2_Figure4.png,: width=0.5,Line Plot,"The plot illustrates the performance of different tasks over a specified range of data points. The x-axis is labeled with numerical values ranging from 17 to 32, using a linear scale with tick intervals of 1. The y-axis represents performance values, also on a linear scale, ranging from 0.0 to 1.0 with tick intervals of 0.2. There are three lines depicted. The blue line, representing ""Copying,"" starts at a value slightly above 0.9 at x=17, gradually decreasing to 0.0 at x=24, before rising sharply back up to 1.0 by x=31. The orange line for ""Summarization"" initiates at approximately 0.85 at x=17, experiencing a decline to a low near 0.3 at x=24, then increasing to about 0.95 at x=31. The green line, indicating ""Continuation,"" begins at a value close to 1.0 at x=17 and remains relatively stable throughout the x-axis range, ending at 1.0 at x=31. All lines have circular markers connected by solid lines."
056_2503.23798v2_Adaptive Layer-skipping in Pre-trained LLMs.pdf,056_2503.23798v2_Figure5.png,: width=0.5,Line Plot,"The plot illustrates the variation of three different processes labeled as Repetition, Addition, and Product over a specific numerical range. The x-axis is labeled with numbers ranging from 17 to 32. It uses a linear scale with a tick interval of 1 unit, but does not specify any units. The y-axis represents a value range from 0.0 to 1.0, also on a linear scale with the same tick interval, though it is without stated units. The blue line, representing Repetition, begins at a y-value of 1.0 and decreases steadily to around 0.0 by x = 22, then remains low until rising sharply from x = 31, ending near 1.0 at x = 32. The orange line, depicting Addition, starts at a y-value of 0.8 at x = 17, decreases gradually, reaching a minimum around 0.1 at x = 24, holds steady through x = 29, and then increases sharply from x = 30, ending near 0.9 at x = 32. The green line, corresponding to Product, begins at a y-value slightly above 0.9 at x = 17, follows a similar pattern to the others but remains slightly higher at comparable points, declining to a minimum of about 0.2 at x = 24, maintaining this level through x = 29, and then increasing steeply from x = 30 to end slightly above 0.9 at x = 32. Each line pattern includes small dots as markers, connected by solid line styles."
058_2505.10202v1_VQ-Logits Compressing the Output Bottleneck of Large Language Models   via Vector Quantized Logits.pdf,058_2505.10202v1_Figure2.png,: Perplexity vs. $\codebooksize$.,Line Plot,"The plot illustrates the relationship between codebook size, measured in kilo-units (K), and perplexity, measured in perplexity units (PPL). The x-axis is labeled ""Codebook Size (K)"" and follows a linear scale, ranging from 512 to 8192 with evenly spaced tick labels at 512, 1024, 2048, 4096, and 8192. The y-axis is labeled ""Perplexity (PPL),"" also on a linear scale, ranging from 18.00 to 20.00, with tick labels at intervals of 0.25. The plot features a single blue line with circular markers and a solid line style. The line begins at a y-value of 20.00 PPL at an x-value of 512 K and ends at a y-value of approximately 18.15 PPL at an x-value of 8192 K. There is a consistent downward trend, with representative data points at (512, 20.00), (1024, 19.25), (2048, 18.75), (4096, 18.50), and (8192, 18.15) PPL, indicating a decrease in perplexity as codebook size increases."
058_2505.10202v1_VQ-Logits Compressing the Output Bottleneck of Large Language Models   via Vector Quantized Logits.pdf,058_2505.10202v1_Figure3.png,: Logit Speedup vs. $\codebooksize$.,Line Plot,"The plot illustrates the relationship between logit computation speedup and codebook size (K). The x-axis represents the codebook size (K), labeled as ""Codebook Size (K)"" with a linear scale, ranging from 512 to 8192, and tick labels are placed at intervals of 512. The y-axis represents the logit computation speedup (x), labeled as ""Logit Computation Speedup (x)"" with a linear scale, ranging from 0 to 10, with tick labels at intervals of 2. The plot features a single red line with square markers and a solid line style. The line starts at the upper left with a speedup of 10 at a codebook size of 512 and ends at the lower right with a speedup of 2 at a codebook size of 8192. Along the x-axis, the speedup decreases steadily with representative values showing 8 at 1024, 6 at 2048, 4 at 4096, reaching a minimum of 2 at the largest codebook size shown, 8192."
059_2505.15692v2_Thought-Augmented Policy Optimization Bridging External Guidance and   Internal Capabilities.pdf,059_2505.15692v2_Figure1.png,"Overall performance across five competition-level benchmarks (MATH-500, AIME 2024, AMC, OlmpiadBench, and Minerva Math). TAPO significantly outperforms existing RL methods, especially on the challenging AIME and AMC benchmark ($\uparrow$99$\%$ and $\uparrow$41$\%$  over GRPO).",Bar Chart,"The bar chart illustrates the accuracy percentages of different models tested across multiple math-related datasets. The categories are placed on the horizontal axis and include MATH500, AIME 24, AMC, Minerva Math, OlympiadBench, and Average. The value axis, labeled ""Accuracy (%)"", is vertical, using a linear scale ranging from 10 to 90, with tick marks at intervals of 10 percent. Each category features grouped bars representing different models: Qwen2.5-Math-7B, Prime-Zero-7B, OpenReasoner-Zero-7B, Naive GRPO, LUFFY-Zero-7B, Oat-Zero-7B, and TAPO-Zero-7B (Ours), differentiated by distinct colors such as white, pink, light yellow, light brown, light purple, light gold, and light blue, respectively. For MATH500, values range from approximately 50.8 for LUFFY-Zero-7B to 83.4 for TAPO-Zero-7B. In AIME 24, values vary from about 13.3 for LUFFY-Zero-7B to 33.3 for TAPO-Zero-7B. AMC shows a range from around 42.5 for LUFFY-Zero-7B to 77.5 for TAPO-Zero-7B. Minerva Math values fall between approximately 12.1 for LUFFY-Zero-7B and 38.2 for TAPO-Zero-7B. OlympiadBench results range from about 17.2 for LUFFY-Zero-7B to 46.2 for TAPO-Zero-7B. The Average bars display from approximately 27.2 for LUFFY-Zero-7B to 55.8 for TAPO-Zero-7B. No specific sorting or emphasis on bars is indicated, and each group is clearly labeled and consistently colored per the legend."
059_2505.15692v2_Thought-Augmented Policy Optimization Bridging External Guidance and   Internal Capabilities.pdf,059_2505.15692v2_Figure4.png,Results on three out-of-distribution benchmark datasets (Qwen2.5-Math-7B-Base).,Bar Chart,"The bar chart illustrates the accuracy percentages of three different methods across four categories, including an average measure. The horizontal axis contains the categories labeled as ""ARC-C,"" ""GPQA-Diamon,"" ""MMLU-Pro,"" and ""Average,"" and is categorized in this specific order. The vertical axis represents ""Accuracy (%)"" with a linear scale ranging from 10 to 90, with tick intervals of 10%. Each category has three bars representing ""CoT,"" ""GRPO,"" and ""TAPO (Ours),"" colored in light brown, light green, and light blue, respectively. In ""ARC-C,"" the ""CoT"" bar is approximately 47.9%, ""GRPO"" is about 75.6%, and ""TAPO (Ours)"" is around 81.6%. In ""GPQA-Diamon,"" the values are approximately 15.2% for ""CoT,"" 31.3% for ""GRPO,"" and 37.9% for ""TAPO (Ours)."" In ""MMLU-Pro,"" ""CoT"" has an accuracy of about 19.2%, ""GRPO"" is approximately 42.1%, and ""TAPO (Ours)"" is about 49.5%. For the ""Average"" category, the respective values are roughly 27.4% for ""CoT,"" 49.6% for ""GRPO,"" and 56.6% for ""TAPO (Ours)."" The chart annotates the differences between bars with percentage increases highlighted with arrows. There is no specific sorting of the bars beyond the category order, and there does not appear to be any visual emphasis beyond the annotations."
059_2505.15692v2_Thought-Augmented Policy Optimization Bridging External Guidance and   Internal Capabilities.pdf,059_2505.15692v2_Figure5.png,: Qwen2.5-Math-7B-Base,Line Plot,"The plot illustrates training rewards over time for two different methods, labeled as GRPO and TAPO. The x-axis is labeled ""Steps,"" featuring a linear scale with a value range from 0 to 500, with tick marks at intervals of 100 steps. The y-axis is labeled ""Training Rewards,"" also using a linear scale, ranging from 0.4 to 0.8, with tick marks at intervals of 0.1. The line representing GRPO is depicted in purple, with a solid line style and no specific marker shape. This line begins around 0.45 at 0 steps and increases with some variability to approximately 0.65 at the end of the axis, featuring several fluctuations throughout. The TAPO line is shown in green, also solid with no markers, starting close to 0.4 at the initial point and rising to about 0.7 at 500 steps. The TAPO line demonstrates notable variability, including peaks and troughs throughout the measured range."
059_2505.15692v2_Thought-Augmented Policy Optimization Bridging External Guidance and   Internal Capabilities.pdf,059_2505.15692v2_Figure6.png,: Training Reward Curve on Qwen2.5-Math-7B-Base and Llama-3.2-3B-Base.,Line Plot,"The plot illustrates training rewards across different steps for two algorithms. The x-axis is labeled ""Steps,"" with a linear scale ranging from 0 to 200 in increments of 25. The y-axis is labeled ""Training Rewards,"" with a linear scale ranging from 0.0 to 0.5 in increments of 0.1. The first line, representing ""GRPO,"" is colored purple with a solid line style and no distinct markers. It starts at a training reward of approximately 0.02 at 0 steps and follows an upward trend, peaking near 0.3 around the 100-step mark, before declining to near 0.0 by 200 steps. The second line, representing ""TAPO,"" is colored green, also with a solid line style and no distinct markers. It starts at a training reward of around 0.05 at 0 steps, shows an increase with fluctuations, reaching above 0.4 around 100 steps, and then remains relatively steady, ending just above 0.3 at 200 steps."
059_2505.15692v2_Thought-Augmented Policy Optimization Bridging External Guidance and   Internal Capabilities.pdf,059_2505.15692v2_Figure7.png,: Training Reward Curve,Line Plot,"The plot illustrates the relationship between training rewards and steps for different absolute values of |g|. The x-axis represents ""Steps,"" with a linear scale ranging from 0 to 150, using a tick label interval of 20 steps. The y-axis denotes ""Training Rewards,"" also on a linear scale, ranging from 0.0 to 0.8, with tick intervals of 0.1. There are four lines in the plot: the red line with solid style and no markers represents |g|=1, starting around 0.3 at step 0 and ending around 0.5 at step 150, showing a general increase with slight fluctuations. The blue line with solid style and no markers corresponds to |g|=2, beginning near 0.4 and ending above 0.6, also with a rising trend interspersed with small peaks. The green line, depicting |g|=4, starts near 0.4 at step 0, peaks around 0.7, and concludes close to 0.6 at step 150. The line is solid with no markers, showing fluctuations but overall positive growth. Finally, the magenta line for |g|=8 begins around 0.3, peaks around 0.5, and finishes slightly above 0.6, characterized by a solid style with no markers and moderate variation. Each line is accompanied by shaded regions indicating variability around the central trend."
060_2505.00358v1_R&B Domain Regrouping and Data Mixture Balancing for Efficient   Foundation Model Training.pdf,060_2505.00358v1_Figure2.png,"Instead of using pre-determined domains (e.g., by task type), we find that it is often better to first repartition the data into finer-grained, semantically related domains. Optimizing the proportions of these new semantic domains can significantly improve training performance.",Line Plot,"This plot illustrates how evaluation loss varies with the number of clusters. The x-axis is labeled ""Number of Clusters,"" is linear, and spans from 0 to 30 with tick marks at intervals of 5. The y-axis is labeled ""Evaluation Loss,"" is also linear, and ranges from 2.775 to 2.805 with tick marks at intervals of 0.005. There are two lines shown in the plot. The first line is a blue solid line with a blue shaded region around it, indicating variability or error. It features a significant decrease from about 2.803 at 0 clusters to approximately 2.775 at 6 clusters, marked by a green star, representing a local minimum. Following this, there is an increase to around 2.795 at 30 clusters, with fluctuations throughout. The second line is a red dashed line aligned horizontally at approximately 2.802, with a light red shaded area, indicating a constant reference level across all cluster numbers."
060_2505.00358v1_R&B Domain Regrouping and Data Mixture Balancing for Efficient   Foundation Model Training.pdf,060_2505.00358v1_Figure4.png,"Top Row: Across various data settings, we find that there is a “sweet spot” in the number of domains used for data mixing, indicated by the green star. The optimal number of groups varies significantly with the dataset, which motivates the need for compute-efficient data mixing. Bottom Row: We find that silhouette score often correlates with model performance, suggesting that it is possible to predict data mixing performance based on clustering metrics.",Line Plot,"The plot illustrates the relationship between the number of clusters and evaluation loss as part of a test labeled ""Super-NatInst Test."" The x-axis is labeled ""Number of Clusters,"" with a linear scale ranging from 0 to 200. The x-axis has tick labels at intervals of 50 units. The y-axis is labeled ""Evaluation Loss,"" also using a linear scale, with a range from 2.87 to 2.93 and tick labels spaced at intervals of 0.01. There is a blue line with no markers and a solid line style, starting at approximately 2.933 at zero clusters and showing a generally decreasing trend, with fluctuations. Notable points include a decrease to approximately 2.885 at around 100 clusters, followed by minor increases and decreases, ending near 2.876 at 150 clusters and rising slightly at 200 clusters. A dashed red horizontal line is present at the evaluation loss level of around 2.88. A vertical dashed gray line is indicated at 100 clusters. Additionally, a green star marker is located at approximately 150 clusters and 2.876 evaluation loss."
060_2505.00358v1_R&B Domain Regrouping and Data Mixture Balancing for Efficient   Foundation Model Training.pdf,060_2505.00358v1_Figure5.png,"Top Row: Across various data settings, we find that there is a “sweet spot” in the number of domains used for data mixing, indicated by the green star. The optimal number of groups varies significantly with the dataset, which motivates the need for compute-efficient data mixing. Bottom Row: We find that silhouette score often correlates with model performance, suggesting that it is possible to predict data mixing performance based on clustering metrics.",Line Plot,"The line plot illustrates the relationship between the number of clusters and evaluation loss in a dataset. The x-axis, labeled ""Number of Clusters,"" uses a linear scale ranging from 0 to 100 with intervals marked every 20 units. The y-axis, labeled ""Evaluation Loss,"" also utilizes a linear scale with values ranging from 2.46 to 2.58, with tick marks at every 0.02 units. The plot features two lines: a dashed red line and a solid blue line with a star marker at one point. The dashed red line is constant across the x-axis at an evaluation loss of approximately 2.58. The solid blue line begins at around (10, 2.47), decreases to a minimum at approximately (25, 2.46) with a green star marker, then steadily increases to end at about (100, 2.58). Notable points include a relatively steady increase in loss after the minimum is reached. A vertical dashed gray line is present at the x-value of 40 but is not connected to any line."
060_2505.00358v1_R&B Domain Regrouping and Data Mixture Balancing for Efficient   Foundation Model Training.pdf,060_2505.00358v1_Figure6.png,"Top Row: Across various data settings, we find that there is a “sweet spot” in the number of domains used for data mixing, indicated by the green star. The optimal number of groups varies significantly with the dataset, which motivates the need for compute-efficient data mixing. Bottom Row: We find that silhouette score often correlates with model performance, suggesting that it is possible to predict data mixing performance based on clustering metrics.",Line Plot,"This plot illustrates the relationship between the number of clusters and the evaluation loss for a dataset labeled as S1-59k. The x-axis is labeled ""Number of Clusters"" and uses a linear scale, ranging from 0 to 100 with tick marks at intervals of 20. The y-axis is labeled ""Evaluation Loss"" and also uses a linear scale, ranging from 0.745 to 0.752 with tick marks every 0.001. The plot includes three distinct lines: a solid red dashed line at a constant y-value of approximately 0.752, a solid black vertical line at x-value 60, and a solid blue line with no markers, shaded for variance. The blue line begins near 0.745 at x=10, shows slight variations with a peak just above 0.746 around x=40, drops below 0.746 around x=50, and stabilizes near 0.745 with a green star marker at x=80."
060_2505.00358v1_R&B Domain Regrouping and Data Mixture Balancing for Efficient   Foundation Model Training.pdf,060_2505.00358v1_Figure11.png,"Left: Regrouping skills before applying data mixing strategies can yield substantial improvements. Underlined values indicate where regrouping beats the original grouping for that method and dataset. Highlighted values (with brown background) indicate the best overall performance for each dataset. Note that we do not apply Balance to the original categorization of Sup-NatInst test, as we assume that training data and validation data are bucketed into the same $m$ groups. Right: Loss curve on the Sup-NatInst dataset.",Line Plot,"The plot illustrates the evaluation loss as a function of the number of steps, comparing five different methods. The x-axis is labeled ""Number of Steps,"" using a linear scale ranging from 0 to 2000 with tick marks at intervals of 200. The y-axis is labeled ""Evaluation Loss,"" also using a linear scale, with a value range from 2.4 to 3.4 and tick marks at intervals of 0.2. There are five lines, each representing a different method. The blue line with circle markers and a dashed style, labeled ""static,"" starts at a y-value of approximately 3.4 at 0 steps and declines to about 2.8 at 2000 steps. The orange line with no markers and a solid style, labeled ""r&b,"" begins around 3.4 at the start and decreases steadily to approximately 2.5 by 2000 steps. The green line with no markers and a solid style, labeled ""aioli,"" also starts near 3.4 and reduces to about 2.8, with a notable drop around 1000 steps before leveling off slightly. The brown line, labeled ""skillit,"" with no markers and a solid style, begins at about 3.4 and decreases rapidly to around 2.7 at 1000 steps, after which it remains relatively stable. Finally, the cyan line with no markers and a solid style, labeled ""dga,"" initially at 3.4, drops to around 2.8 by the 2000 steps mark."
060_2505.00358v1_R&B Domain Regrouping and Data Mixture Balancing for Efficient   Foundation Model Training.pdf,060_2505.00358v1_Figure12.png,"Left: Training loss curves for Dolly-15k trained for 40,000 steps with different data mixing methods using the original category partitioning. Right: Average test loss on Dolly-15k after 40,000 training steps using original category partitioning. Highlighted values (with brown background) indicate the best overall performance.",Line Plot,"The plot illustrates the evaluation loss over a series of training steps for different methodologies labeled as static, dga, r&b, aioli, and skillit. The x-axis is labeled ""Training Steps"" with a linear scale ranging from 0 to 40k, marked at intervals of 4k steps. The y-axis is labeled ""Evaluation Loss"" and also uses a linear scale, with values ranging from 2.750 to 2.770 in increments of 0.005. The blue line with a solid style represents the ""static"" method, starting at an evaluation loss of approximately 2.765 and ending around 2.755, showing small fluctuations. The orange line with a solid style represents ""dga,"" beginning near 2.770 and descending markedly to around 2.755, becoming steady with minor variations. The green line with a solid style represents ""r&b,"" starting at about 2.765, decreasing to around 2.750, and then remaining relatively stable. The brown line with a solid style represents ""aioli,"" beginning at approximately 2.770 and dropping sharply to near 2.755 by 12k steps, after which it stabilizes. The purple line with a solid style represents ""skillit,"" beginning at about 2.765, decreasing slightly to around 2.755, then remaining relatively constant with minor fluctuations."
063_2505.12183v1_Decoding the Mind of Large Language Models A Quantitative Evaluation of   Ideology and Biases.pdf,063_2505.12183v1_Figure4.png,Distribution of Questions by Bias (Splitted 103 Questions).,Bar Chart,"The bar chart titled ""Distribution of Splitted Questions per Bias Groups"" illustrates the distribution of questions across bias groups for two models, ChatGPT 4o mini and Gemini 1.5 flash. The horizontal axis represents different language categories labeled as Jap., Eng., Spa., and Fre. for both models. The vertical axis represents the distribution proportion with no specific numerical label or units provided, and no tick mark intervals are visible. Each language category includes a stacked bar composed of segments representing different bias classifications: ""Strong 'Yes'"" in orange, ""'Yes'"" in blue, ""Neutral, Originally 'No'"" in brown, ""Neutral, Originally 'Yes'"" in green, ""'No'"" in sky blue, and ""Strong 'No'"" in dark green. The chart visually arranges these segments vertically within each bar. The color scheme is consistent for each category across both models, with no specific sorting of bars or additional emphasis on any individual segment."
065_2505.20650v1_FinTagging An LLM-ready Benchmark for Extracting and Structuring   Financial Information.pdf,065_2505.20650v1_Figure2.png,The statistic of numerical entity type.,Bar Chart,"The bar chart illustrates the distribution of different item types based on their count and occurrence rate percentage. The horizontal axis contains the categories labeled as different item types, including monetaryItemType, percentItemType, sharesItemType, perShareItemType, integerItemType, decimalItemType, pureItemType, energyItemType, volumeItemType, areaItemType, and massItemType, arranged alphabetically. The left vertical axis is labeled ""Count"" with a linear scale ranging from 0 to 50000, with tick intervals of 10000. The right vertical axis is labeled ""Occurrence Rate (%)"" with a linear scale from 0 to 100 with tick intervals of 20. Each category is represented by two bars: a blue bar for ""Count"" and a red bar for ""Occurrence Rate (%)"". The monetaryItemType and percentItemType categories have the tallest bars, both reaching approximately 50000 for the count and 100% for the occurrence rate. Categories such as energyItemType, volumeItemType, and massItemType have shorter bars for both metrics, with the massItemType showing the shortest bars. The bars are consistent in color for their respective metrics, with no bars visually emphasized or annotated."
069_2505.22613v1_RICO Improving Accuracy and Completeness in Image Recaptioning via   Visual Reconstruction.pdf,069_2505.22613v1_Figure3.png,Performance of the pipeline under different numbers of refinement iterations.,Line Plot,"The plot illustrates model performance across different iteration steps with respect to accuracy metrics. The x-axis is labeled ""Iteration Steps"" and follows a linear scale, ranging from 0 to 5, with tick labels at intervals of 1. The y-axis is labeled ""Accuracy (%)"" and also follows a linear scale, with a range from 30 to 70 and tick labels at intervals of 10. There are four lines depicted in the plot. The blue line represents overall accuracy, is marked with circles, and is solid. It starts at approximately 40% at iteration step 0 and increases to about 55% at iteration step 5. The orange line represents color accuracy, is marked with squares, and is dashed. It begins at around 50% at iteration step 0, rising to about 65% at iteration step 1, and gradually stabilizes around 68% by iteration step 5. The green line, indicating relative position accuracy, is marked with triangles and is dashed. It starts at roughly 40% at iteration step 0 and rises to about 55% by iteration step 5. Lastly, the red line for shape accuracy, marked with crosses and dashed, starts at about 30% at iteration step 0 and increases to approximately 50% at iteration step 5."
070_2505.16984v1_UFT Unifying Supervised and Reinforcement Fine-Tuning.pdf,070_2505.16984v1_Figure1.png,"(top left, top right, middle, bottom). The illustration of SFT, RFT, SFT-RFT, and UFT, respectively. SFT-RFT refers to applying RFT after an initial SFT stage <cit.>. (Top, center). shows the annotation usage of different algorithms over training. Curves are slightly shifted for better visibility.",Line Plot,"The plot illustrates the annotation usage of different algorithms over a series of steps. The x-axis is labeled ""Steps"" and uses a linear scale, ranging from 0 to 500, with tick marks at intervals of 50 units. The y-axis is labeled ""Annotation Usage,"" also employing a linear scale, ranging from 0 to 1, with tick marks at intervals of 0.1 units. There are four lines on the plot. The first line, representing SFT, is shown in green with diamond markers and a solid line style, and it remains consistent at a y-value of 1 across all steps. The second line, representing RFT, is illustrated in orange with circle markers and a solid line style, staying constant at a y-value of 0 across the x-axis. The third line, representing SFT-RFT, is in blue with triangle markers and a solid line, starting at a y-value of 1 at step 0 and dropping sharply to 0 at step 300, remaining at 0 thereafter. The fourth line, representing UFT, is colored purple with square markers and a solid line style, beginning at a y-value of 1 at step 0 and steadily decreasing to reach 0 by step 300, where it remains constant."
070_2505.16984v1_UFT Unifying Supervised and Reinforcement Fine-Tuning.pdf,070_2505.16984v1_Figure4.png,"(left). An illustration of the UFT prompt. We adopt the prompting template from TinyZero <cit.>, which is similar to that used in Deepseek-R1 <cit.>. The hint consists of a slice of the full solution. During training, the question prompt and the hint are concatenated and fed to the model. (right). An illustration of the training curve of Qwen2.5-0.5B. Stage and UFT keep zero hint since step 300.",Line Plot,"This plot illustrates the ""Average Score on Training Batch"" for Qwen2.5-0.5B across different training steps. The x-axis represents ""Steps"", using a linear scale ranging from 0 to 500, with tick marks at intervals of 100. The y-axis is labeled ""Average Score"", also on a linear scale, ranging from 0.0 to 1.0, with tick marks at intervals of 0.2. There are two lines on the plot. The first line, labeled ""UFT"", is displayed in purple with no markers and a solid line style. It begins at an average score of approximately 0.3 at 0 steps, rises to about 0.9 around 50 steps, and then fluctuates with minor declines, finishing around 0.6 at 500 steps. The second line, labeled ""Stage"", is colored teal, has no markers, and is also solid. It starts at 0.0 at 0 steps, sharply increases to near 1.0 by around 25 steps, then exhibits fluctuations including noticeable drops and rises, and ends close to 0.5 at 500 steps."
070_2505.16984v1_UFT Unifying Supervised and Reinforcement Fine-Tuning.pdf,070_2505.16984v1_Figure5.png,Qwen2.5-0.5B's cumulative average success rate for exploring the correct answer at each step when trained on Logic.,Line Plot,"The plot illustrates the cumulative average exploration success rate on logic over a series of steps. The x-axis is labeled ""Steps"" with a linear scale ranging from 0 to 500, marked at regular intervals of 100 steps. The y-axis is labeled ""Success Rate"" and also follows a linear scale from 0.0 to 1.0, marked at intervals of 0.2. Four lines are present on the plot: an orange line with circle markers labeled ""RFT"" that starts at a success rate of approximately 0.03 and remains nearly constant across the x-axis; a blue line with triangle markers labeled ""UFT"" consistently at a success rate near 1.0; a purple line with star markers labeled ""SFT-RFT"" also maintains a success rate near 1.0 throughout; and a red line with star markers labeled ""R³,"" which begins at a lower success rate around 0.55, dips to approximately 0.3, and then steadily increases to approach 1.0 by the end of the x-axis range."
072_2505.19700v1_Leveraging Importance Sampling to Detach Alignment Modules from Large   Language Models.pdf,072_2505.19700v1_Figure2.png,: Llama3 RAM,Line Plot,"The line plot illustrates the relationship between model size and two metrics, Helpfulness and Harmlessness. The x-axis is labeled ""Model Size"" with a nominal scale showing the categories: 1B, 3B, and 8B. The y-axis is labeled ""LC/%"" and uses a linear scale, ranging from 55 to 73, with tick marks at intervals of 2. The Helpfulness metric is represented by a blue line with no marker shape and a solid line style. It starts around 59% at 1B, increasing to just above 60% at 3B, and reaches approximately 62% at 8B. The Harmlessness metric is shown by a red line, also lacking markers and using a solid line style. It begins at nearly 66% at 1B, rises to about 67% at 3B, and ends near 69% at 8B. Both lines display upward trends across increasing model sizes."
072_2505.19700v1_Leveraging Importance Sampling to Detach Alignment Modules from Large   Language Models.pdf,072_2505.19700v1_Figure3.png,: Qwen2.5 RAM,Line Plot,"The line plot illustrates the relationship between model size and LC percentage, depicting two metrics: helpfulness and harmlessness. The x-axis is labeled ""Model Size"" and uses a linear scale, displaying values at intervals of 0.5B, 1.5B, 3B, and 7B, without specific units. The y-axis is labeled ""LC/%"" and features a linear scale ranging from 55 to 73, with tick labels every two units. The plot contains two lines. The blue line, representing helpfulness, has no markers and is solid in style. It starts approximately at 59% for a model size of 0.5B and increases to around 69% at 7B, showing a consistent upward trend. The red line, indicating harmlessness, is also solid without markers, beginning near 65% at 0.5B and rising steadily to about 71% at 7B. Each data point includes an error bar, indicating variability in measurements."
072_2505.19700v1_Leveraging Importance Sampling to Detach Alignment Modules from Large   Language Models.pdf,072_2505.19700v1_Figure4.png,: Llama3 RAM,Line Plot,"The plot illustrates the relationship between the parameter ""alpha"" and the percentage of LC for two different metrics: Helpfulness and Harmlessness. The x-axis is labeled ""alpha,"" uses a linear scale, and includes values ranging from 0.00001 to 0.1 with a specified tick label interval at 0.00001, 0.0001, 0.001, 0.01, and 0.1. The y-axis is labeled ""LC/%,"" also on a linear scale, with values from 55 to 73 and tick marks at intervals of 2 units. The blue line represents Helpfulness, utilizing circular markers with a solid line style. It starts at approximately 57% at alpha 0.00001, shows a slight decrease, reaching a minimum below 57% at alpha 0.01, and ends at about 59% at alpha 0.1. The red line signifies Harmlessness, marked by square markers and a solid line. It begins at roughly 65% at alpha 0.00001, peaks near 68% at alpha 0.001, then decreases to about 64% at alpha 0.01 before rising again to approximately 66% at alpha 0.1."
072_2505.19700v1_Leveraging Importance Sampling to Detach Alignment Modules from Large   Language Models.pdf,072_2505.19700v1_Figure5.png,: Qwen2.5 RAM,Line Plot,"The plot illustrates the relationship between a parameter labeled ""alpha"" and LC percentage for two metrics: Helpfulness and Harmlessness. The x-axis is labeled ""alpha"" and uses a logarithmic scale, with values ranging from 0.00001 to 0.1. The tick labels are positioned at intervals of 0.00001, 0.0001, 0.001, 0.01, and 0.1. The y-axis is labeled ""LC/%"" and uses a linear scale with a range from 55 to 73, with major tick marks at intervals of 2. The ""Helpfulness"" metric is represented by a blue line with a solid style and circular markers. It starts at approximately 61 at an alpha of 0.00001, increases to around 62 at 0.0001, experiences a slight dip to about 61.5 at 0.001, then rises gradually to end at about 61.5 at 0.1. The ""Harmlessness"" metric is shown with a red line, also solid, but with square markers. This line begins at approximately 67 at 0.00001, rises to around 69 at 0.0001, drops to about 66.5 at 0.001, increases again to roughly 67.5 at 0.01, and finishes at around 67 at 0.1. Error bars are present for both lines at each data point."
074_2505.12716v2_Shadow-FT Tuning Instruct via Base.pdf,074_2505.12716v2_Figure3.png,": The average of Math-7, Code-3, and Reasoning-9 for different ranks when tuning Llama-3.2-1B using LoRA. We report the best performance searching learning rates in {5e-5, 1e-4, 2e-4, 5e-4}.",Line Plot,"The plot illustrates the comparison of two different methods, Shadow-FT and FT, based on their scores relative to rank. The x-axis is labeled ""Rank,"" with a linear scale ranging from 4 to 512 and tick marks at intervals of 4, 8, 16, 32, 64, 128, 256, and 512 with no units specified. The y-axis is labeled ""Score,"" also using a linear scale, ranging from 28 to 32 with tick marks at intervals of 1 unit. The plot features two lines: the ""Shadow-FT"" line is represented with a blue color, circular markers, and a solid line style. It begins at a score of 30.26 at rank 4, remains relatively steady around 30.15 at rank 32, and then shows a clear upward trend, reaching a score of 32.03 at rank 512. The ""FT"" line is illustrated in orange with square markers and a dashed line style. It starts at a score of 30.11 at rank 4, slightly declines to 29.99 at rank 8, and continues to decrease steadily, ending at a score of 28.47 at rank 512."
074_2505.12716v2_Shadow-FT Tuning Instruct via Base.pdf,074_2505.12716v2_Figure5.png,Average $\sigma$ values of more LLMs.,Bar Chart,"The bar chart illustrates the average sigma values for various models, highlighting their comparative performance. The horizontal axis represents the categories labeled 'Models,' featuring different model names in the order: Qwen3-0.6B, Qwen-3-4B, Qwen-3-8B, Qwen-3-14B, Qwen-2.5-32B, Llama-3.1-8B, Yi-Coder-1.5B, Yi-6B, and Yi-Coder-9B. The vertical axis, labeled 'Average sigma,' employs a linear scale ranging from 0.00 to 0.10 with tick intervals of 0.02. Each bar is colored distinctly: Qwen3-0.6B in dark blue with a value of 0.042, Qwen-3-4B in light blue at 0.033, Qwen-3-8B in orange at 0.027, Qwen-3-14B in peach at 0.027, Qwen-2.5-32B in green at 0.004, Llama-3.1-8B in light green at 0.018, Yi-Coder-1.5B in red at 0.012, Yi-6B in light red at 0.003, and Yi-Coder-9B in purple at 0.009. The bars are not sorted in ascending or descending order and do not appear to be visually emphasized beyond color differentiation."
074_2505.12716v2_Shadow-FT Tuning Instruct via Base.pdf,074_2505.12716v2_Figure6.png,Performance tuning Llama-3.1-8B with different ranks.,Line Plot,"The plot illustrates the relationship between 'Rank' on the x-axis and 'Score' on the y-axis for two different methods labeled as Shadow-FT and FT. The x-axis, labeled as ""Rank,"" uses a linear scale, with values ranging from 4 to 512 and tick label intervals at specific points: 4, 8, 16, 32, 64, 128, 256, and 512. The y-axis is labeled ""Score,"" also on a linear scale, with values ranging from 68.2 to 74.2 and major tick marks at intervals of 0.2. The blue solid line with circular markers represents the Shadow-FT method. It starts at 71.34 for a rank of 4, reaches a peak of 72.66 at a rank of 64, and ends at 72.01 at a rank of 512, showing fluctuations along the way. The orange dashed line with square markers represents the FT method, beginning at 70.68 for a rank of 4, declining to 69.03 at a rank of 256, and slightly further to 68.91 at a rank of 512 with minor fluctuations in between."
076_2505.18149v1_First Finish Search Efficient Test-Time Scaling in Large Language   Models.pdf,076_2505.18149v1_Figure5.png,Accuracy versus total token budget for three test-time scaling methods averaged over R1-Distill-Qwen and QwQ-32B on AIME24 and AIME25-I. FFS attains higher accuracy at lower budgets; MV improves more slowly and consumes more tokens; BF plateaus near the 32K token limit.,Line Plot,"The plot illustrates the relationship between accuracy and compute budget for various TTS methods. The x-axis is labeled ""Total Token Budget,"" with a logarithmic scale ranging from 10^3 to 10^5, with tick labels at 10^3, 10^4, and 10^5. The y-axis represents ""Accuracy,"" using a linear scale from 0.3 to 0.75, with tick labels at intervals of 0.05. The ""Budget Forcing"" method is depicted by a blue solid line with circular markers, beginning at a value of approximately 0.3 at 10^3, rising steeply to nearly 0.6 by 10^4, and showing a slight increase to about 0.7 by 10^5. The ""Majority Voting"" method is shown with an orange dashed line and square markers, starting slightly below 0.6 at 10^4 and gradually increasing to approximately 0.7 at 10^5. The ""First Finish Search"" method is illustrated with a green dashed line and triangular markers, starting just above 0.6 at 10^4, slightly rising to near 0.7 at 10^5, showing a consistent and minor upward trend throughout."
080_2505.23564v1_Segment Policy Optimization Effective Segment-Level Credit Assignment   in RL for Large Language Mod.pdf,080_2505.23564v1_Figure1.png,: Train Accuracy.,Line Plot,"The plot illustrates the training accuracy over iterations for a model labeled GRPO on the GSM8K dataset. The x-axis is labeled ""Iteration"" with a linear scale ranging from 0 to 1000, with tick marks at intervals of 200. The y-axis is labeled ""Train Accuracy (%)"" and also uses a linear scale, ranging from 50 to 100, with tick marks at 10% intervals. The plot features a single blue line with no markers and a solid line style. Starting around 50% at iteration 0, the accuracy shows an upward trend with fluctuations, reaching approximately 90% by iteration 1000. The graph displays several peaks and troughs throughout, indicating variability in accuracy, but maintains an overall increasing trend from start to finish."
080_2505.23564v1_Segment Policy Optimization Effective Segment-Level Credit Assignment   in RL for Large Language Mod.pdf,080_2505.23564v1_Figure2.png,: Train Num Unique Responses.,Line Plot,"The plot illustrates the trend in the number of unique responses over iterations for GRPO on GSM8K. The x-axis is labeled ""Iteration"" and uses a linear scale, ranging from 0 to 1000, with tick labels at intervals of 200. The y-axis is labeled ""Num Unique Responses,"" also using a linear scale, ranging from 4 to 8 with tick labels at intervals of 1. The plot features a single line that is blue with no specific marker shape, following a solid line style. The line begins near 7 unique responses at iteration 0 and shows a general declining trend, ending slightly above 5 unique responses at iteration 1000. Throughout the plot, there are fluctuations with minor peaks and troughs, but the overall trajectory is a steady decrease."
080_2505.23564v1_Segment Policy Optimization Effective Segment-Level Credit Assignment   in RL for Large Language Mod.pdf,080_2505.23564v1_Figure3.png,: Validation Accuracy.,Line Plot,"The plot illustrates the progression of validation accuracy as a function of iterations for a model named GRPO on GSM8K. The x-axis is labeled ""Iteration,"" using a linear scale ranging from 0 to 1000 with tick labels at intervals of 200. The y-axis is labeled ""Validation Accuracy (%)"" and also uses a linear scale, ranging from 54 to 60 with tick labels at intervals of 2. There is a single line in the plot, which is colored blue and features a solid line style without markers. At the start of the x-axis, the line begins at approximately 54%, and it generally trends upward, exhibiting several fluctuations. There are notable peaks near 60% and valleys around 56%. Toward the end of the x-axis, the line reaches around 58%, with visible minor fluctuations."
080_2505.23564v1_Segment Policy Optimization Effective Segment-Level Credit Assignment   in RL for Large Language Mod.pdf,080_2505.23564v1_Figure6.png,(a) Test accuracy comparison of different methods on GSM8K. Baseline results are from . (b) Episode generation time comparison between SPO-chain (int5) and VinePPO during training. (c) Validation accuracy of SPO-chain (int5) and GRPO during training.,Bar Chart,"The bar chart illustrates the test accuracy percentages for different models on GSM8K using RhoMath 1.1B. The horizontal axis contains the categories, which are labeled as different models: Init SFT, RestEM, RLOO, GRPO, DPO+, PPO, VinePPO, SPO-tree, and SPO-chain. The vertical axis is the value axis labeled ""Test Accuracy (%)"" with values ranging from 40% to 60%, marked on a linear scale with intervals of 5%. Each bar is labeled according to the model it represents. The bars for Init SFT, RestEM, RLOO, GRPO, DPO+, PPO, VinePPO, SPO-tree, and SPO-chain are colored blue or orange. Init SFT has the shortest bar at approximately 40.3%, and SPO-chain has the tallest bar at 56.7%. SPO-tree is also highlighted in orange, showing a value of 56.4%. The bars are arranged in series, with no specific sorting order such as descending or alphabetical. The orange coloring of SPO-tree and SPO-chain distinguishes these from the other bars, which are all blue, indicating a visual emphasis on these bars."
080_2505.23564v1_Segment Policy Optimization Effective Segment-Level Credit Assignment   in RL for Large Language Mod.pdf,080_2505.23564v1_Figure7.png,(a) Test accuracy comparison of different methods on GSM8K. Baseline results are from . (b) Episode generation time comparison between SPO-chain (int5) and VinePPO during training. (c) Validation accuracy of SPO-chain (int5) and GRPO during training.,Line Plot,"This plot illustrates the episode generation time on GSM8K, comparing two methods. The x-axis is labeled ""Step"" and is on a linear scale, ranging from 0 to 10,000 with tick marks at intervals of 2000. The y-axis is labeled ""Episode Generation Time (s)"" and also uses a linear scale, ranging from 0 to 1250 seconds with tick marks at intervals of 250. The blue line representing ""VinePPO"" has no markers and a solid style. It starts at a high value near 1250 seconds, showing fluctuations throughout, with values ranging between approximately 250 and 1250 seconds across the x-axis, displaying multiple peaks. The orange line, labeled ""SPO-chain (int5),"" also has no markers and follows a solid line style. It begins at around 250 seconds, maintaining a more consistent trend, fluctuating less and remaining generally closer to 250 seconds throughout the steps."
080_2505.23564v1_Segment Policy Optimization Effective Segment-Level Credit Assignment   in RL for Large Language Mod.pdf,080_2505.23564v1_Figure8.png,(a) Test accuracy comparison of different methods on GSM8K. Baseline results are from . (b) Episode generation time comparison between SPO-chain (int5) and VinePPO during training. (c) Validation accuracy of SPO-chain (int5) and GRPO during training.,Line Plot,"The plot illustrates validation accuracy over time for two different methods, labeled as ""SPO-chain (int5)"" and ""GRPO,"" on the GSM8K dataset. The x-axis represents time in hours, using a linear scale ranging from 0 to 60 with tick labels at intervals of 10 hours. The y-axis shows validation accuracy as a percentage, also on a linear scale, spanning from 50% to 70%, with tick labels at 5% intervals. The orange line, representing ""SPO-chain (int5),"" starts near 52% and increases steadily, reaching approximately 67% by the 60-hour mark. The line includes a noticeable upward trend with minor fluctuations towards the end. In contrast, the blue line for ""GRPO"" begins at about 53%, maintains a relatively steady trajectory with slight oscillations, and ends near 56%. This line also shows minor fluctuations but remains more level than the orange line throughout the plotted time frame."
080_2505.23564v1_Segment Policy Optimization Effective Segment-Level Credit Assignment   in RL for Large Language Mod.pdf,080_2505.23564v1_Figure9.png,(a) Variations of segment partition granularity (different cutpoint intervals). (b) Variations of segment partition strategies. (c) Ablation on probability-mask policy optimization strategy.,Line Plot,"The plot illustrates the variations in test accuracy over iterations for different partition granularities. The x-axis represents ""Iteration"" with a linear scale ranging from 0 to 800, with tick labels at intervals of 200. The y-axis represents ""Test Accuracy (%)"" also on a linear scale ranging from 40 to 55, with tick labels at intervals of 5. There are three lines in the plot: a green line with triangles, solid style, labeled ""SPO-chain (int2)"" shows an increase from approximately 44% at the beginning to about 55% at the end, with a notable peak at around 600 iterations. The orange line with circles, solid style, labeled ""SPO-chain (int5)"" begins near 44% and ends around 54%, showing a similar upward trend with minor fluctuations. The purple line with squares, dashed style, labeled ""SPO-chain (int100)"" starts just below 44% and rises steadily to approximately 47% by the end, showing smaller consistent increments compared to the other lines."
080_2505.23564v1_Segment Policy Optimization Effective Segment-Level Credit Assignment   in RL for Large Language Mod.pdf,080_2505.23564v1_Figure10.png,(a) Variations of segment partition granularity (different cutpoint intervals). (b) Variations of segment partition strategies. (c) Ablation on probability-mask policy optimization strategy.,Line Plot,"The plot illustrates variations in test accuracy percentages across different training iterations for three partition strategies. The x-axis represents ""Iteration"" on a linear scale, ranging from 0 to 700, with tick labels at intervals of 100. The y-axis shows ""Test Accuracy (%)"" on a linear scale, ranging from 40% to 55%, with tick labels at intervals of 5%. The orange line, representing ""SPO-chain (int5),"" uses circles as markers and a solid line style. It starts at approximately 44% at iteration 0 and ends around 55% by iteration 700, showing a generally increasing trend with some fluctuations. The blue line, labeled ""VinePPO,"" features triangle markers and a solid line style. It starts at about 42% and reaches approximately 52%, gradually increasing with minor fluctuations. The green line, corresponding to ""Fixed-token-count,"" uses square markers and a solid line style, beginning at 44% and concluding near 47%, with a relatively steadier but slight upward change."
080_2505.23564v1_Segment Policy Optimization Effective Segment-Level Credit Assignment   in RL for Large Language Mod.pdf,080_2505.23564v1_Figure11.png,(a) Variations of segment partition granularity (different cutpoint intervals). (b) Variations of segment partition strategies. (c) Ablation on probability-mask policy optimization strategy.,Line Plot,"The plot illustrates the effect of different probability-mask strategies on test accuracy over iterations. The x-axis is labeled ""Iteration,"" with a linear scale ranging from 0 to 1000, and tick marks are spaced at intervals of 200. The y-axis is labeled ""Test Accuracy (%)"" with a linear scale, ranging from 35% to 55%, with tick marks at intervals of 5%. The plot includes four lines. The first line is orange with circle markers and a solid style, representing ""SPO-chain (int5),"" starting at approximately 46% accuracy and ending around 55% as it increases steadily. The second line, in gray with x-markers and a dashed style, depicts ""SPO-chain (int5) (no prob-mask)"" and remains relatively flat, starting just under 46% and ending slightly below that value. The third line is blue with square markers and a solid style, indicating ""GRPO,"" starting at approximately 44% and remaining steady throughout, ending at about the same level. The fourth line, in cyan with diamond markers and a solid style, represents ""GRPO (with prob-mask),"" which begins at around 44% accuracy and rises consistently to end around 52%."
080_2505.23564v1_Segment Policy Optimization Effective Segment-Level Credit Assignment   in RL for Large Language Mod.pdf,080_2505.23564v1_Figure12.png,(a) Comparison of SPO-tree (6-6-6) and GRPO on MATH500 with a context size of 2K. (b) Variations of  tree structures on GSM8K. (c) SPO-tree with different advantage methods on GSM8K.,Line Plot,"The plot illustrates the accuracy of MATH500 over time, specifically comparing performance over hours. The x-axis is labeled ""Hour,"" utilizes a linear scale, and ranges from 0 to 80 with tick labels at intervals of 20 hours. The y-axis is labeled ""MATH500 Accuracy (%)"" and also employs a linear scale, ranging from 55 to 75 with tick labels at intervals of 5%. The plot features two lines: the first line represents ""SPO-tree (6-6-6)"" and is depicted in orange with a solid line style and no markers. It starts at an accuracy just above 60% at hour 0, climbs sharply to approximately 75% around hour 20, and maintains fluctuations around this level, ending slightly lower near hour 80. The second line represents ""GRPO,"" shown in blue with a solid line and no markers, beginning at around 58% at hour 0 and experiencing minor fluctuations, concluding near 65% at hour 80. Both lines display some peaks and troughs throughout the depicted time range."
080_2505.23564v1_Segment Policy Optimization Effective Segment-Level Credit Assignment   in RL for Large Language Mod.pdf,080_2505.23564v1_Figure13.png,(a) Comparison of SPO-tree (6-6-6) and GRPO on MATH500 with a context size of 2K. (b) Variations of  tree structures on GSM8K. (c) SPO-tree with different advantage methods on GSM8K.,Line Plot,"The plot illustrates the variations in test accuracy of different SPO-tree structures over time. The x-axis is labeled ""Hour"" and follows a linear scale, ranging from 0 to 80 with tick marks at regular intervals of 10. The y-axis represents ""Test Accuracy (%)"" also on a linear scale, ranging from 40% to 55% with intervals of 5%. There are three lines shown: a cyan line with triangle markers and a solid style, representing the ""SPO-tree (4-4-4)"" structure, starting around 40% and rising steadily to approximately 55% by the end of the x-axis. An orange line with circle markers and a solid style depicts the ""SPO-tree (6-6-6)"" structure, starting around 40% and reaching about 53% at the end, showing a more gradual increase. The purple line with square markers and a solid style represents the ""SPO-tree (8-8-8)"" structure, beginning at about 40% and increasing to around 54%, with notable fluctuations towards the end."
080_2505.23564v1_Segment Policy Optimization Effective Segment-Level Credit Assignment   in RL for Large Language Mod.pdf,080_2505.23564v1_Figure14.png,(a) Comparison of SPO-tree (6-6-6) and GRPO on MATH500 with a context size of 2K. (b) Variations of  tree structures on GSM8K. (c) SPO-tree with different advantage methods on GSM8K.,Line Plot,"The plot illustrates the variation in test accuracy over different iterations for two methods, labeled ""(c) Advantage Method Variations."" The x-axis represents ""Iteration"" on a linear scale, ranging from 0 to 550, with tick labels at intervals of 50. The y-axis displays ""Test Accuracy (%)"" on a linear scale, ranging from 40 to 55, with tick labels at intervals of 5. There are two lines in the plot: the first line is orange, uses circle markers, and has a solid line style, representing the method ""SPO-tree (6-6-6) w/o std. norm."" This line starts near 40% at iteration 0, gradually increases with slight fluctuations, and reaches approximately 52% by iteration 550. The second line is sky blue, uses triangle markers, and also has a solid line style, indicating the method ""SPO-tree (6-6-6) w/ std. norm."" It begins similarly near 40% at iteration 0, following a comparable upward trend with some fluctuations, and ends around 53% at iteration 550. Both lines show a general increase in test accuracy across the iterations."
080_2505.23564v1_Segment Policy Optimization Effective Segment-Level Credit Assignment   in RL for Large Language Mod.pdf,080_2505.23564v1_Figure15.png,Validation accuracy on GSM8K using policy iteration optimizing method.,Line Plot,"The plot illustrates the change in validation accuracy as a percentage over a number of iterations during the optimization process described as ""SPO-chain with policy iteration optimization."" The x-axis is labeled ""Iteration,"" using a linear scale, with a value range from 0 to 650 and tick labels displayed at intervals of 100 units. The y-axis is labeled ""Validation Accuracy (%)"" and also uses a linear scale, ranging from 52.5% to 65.0%, with tick labels at regular intervals of 2.5%. There is a single line on the plot, which is blue with no markers and a solid style. It starts at approximately 52.5% accuracy at iteration 0 and generally trends upward, reaching nearly 65.0% by iteration 650. The line displays several notable fluctuations, with noticeable rises and falls, including peaks at around iterations 150 and 450, and a general upward trend towards the end."
080_2505.23564v1_Segment Policy Optimization Effective Segment-Level Credit Assignment   in RL for Large Language Mod.pdf,080_2505.23564v1_Figure17.png,Performance of SPO-chain on DeepSeekMath 7B evaluated on the MATH500. Baseline results are adopted from .,Line Plot,"The plot illustrates the relationship between ""Step"" and ""Test Accuracy (%)"" for a model named DeepSeekMath 7B - MATH500. The x-axis is labeled ""Step"" and uses a linear scale with values ranging from 0 to 25,000. Tick labels are regularly spaced at intervals of 2,500. The y-axis is labeled ""Test Accuracy (%)"" and also uses a linear scale, with values ranging from 32.5% to 47.5%, with tick labels at intervals of 2.5%. The plot features a single blue line with solid line style and no marker shape, representing the ""SPO-chain"" model. The line starts at approximately 37.5% test accuracy at step 0, rises to about 40.0% by step 5,000, and continues to increase to around 42.5% at step 15,000. After this point, there is a sharp red dashed vertical line indicating a change in interval from 10 to 5. Beyond this point, the line continues to rise, exhibiting fluctuations around 45% test accuracy between steps 20,000 and 25,000."
080_2505.23564v1_Segment Policy Optimization Effective Segment-Level Credit Assignment   in RL for Large Language Mod.pdf,080_2505.23564v1_Figure18.png,"Comparison of the number of sampling points between SPO-chain and VinePPO. According to , there are approximately 8 reasoning steps on average for MATH.",Line Plot,"The plot illustrates the number of sampling points in relation to the iteration process. The x-axis is labeled ""Iteration"" and uses a linear scale, ranging from 0 to 1600 with tick marks at intervals of 200. The y-axis is labeled ""Num of sampling points,"" also on a linear scale, ranging from 0 to 8 with tick marks at intervals of 1. There are two lines depicted in the plot. The first line is a green solid line with no markers, showing fluctuations across the x-axis. It starts near 5 at iteration 0, displaying a decline and stabilization around a value close to 2 after iteration 400, with some variability until iteration 1600. The second line is a blue dashed line with no markers, reflecting a constant value of 8 across all iterations from 0 to 1600. Additionally, a vertical red dashed line is present at iteration 1000, indicating an interval change marked in text as ""interval changed from 10 to 5."""
086_2505.10583v1_Relative Drawing Identification Complexity is Invariant to Modality in   Vision-Language Models.pdf,086_2505.10583v1_Figure4.png,"Example of a drawing simplification for the concept car using the rdp algorithm. As the value of $\epsilon$ increases, the drawings become progressively simpler.",Line Plot,"The plot illustrates the relationship between two variables on a two-dimensional axis. The x-axis is unlabeled and appears to use a linear scale with values visible at several intervals without specific units or tick label intervals indicated. The range of values along the x-axis is from approximately 0 to 8. The y-axis is also unlabeled and seems to follow a linear scale similar to the x-axis, with values ranging from approximately -2 to 6, with multiple visible intervals. There is a single line in the plot, which is colored blue with circular markers. The line is comprised of solid segments connecting the markers. It begins at the point (0, -2), rises sharply to the point (2, 1), continues to increase to the point (4, 6), and then descends steeply to the point (6, 0). The line then returns closer to its starting y-value, at approximately (8, 0). The data points exhibit a combination of rising and falling trends throughout the plot, indicating variability across the x-axis values."
"087_2505.15392v1_An Empirical Study of the Anchoring Effect in LLMs Existence,   Mechanism, and Potential Mitigations.pdf",087_2505.15392v1_Figure3.png,"Percentages of sufficient anchor information mentions in DeepSeek-R1 reasoning contents. Legend: “Anchored” refers to the percentages of questions judged as anchored based on the metrics introduced in <Ref>; “All” and “Non-anchored” indicate the percentages over all questions and those judged non-anchored, respectively. We employ an LLM-as-a-Judge approach to automatically detect explicit mentions of anchor-influenced features in reasoning contents, guided by detailed criteria defining what extent can be counted as sufficient mention (see more in <Ref>).",Bar Chart,"The bar chart illustrates the scores across different categories, namely ""Semantic"" and ""Numerical,"" for three conditions: ""Anchored,"" ""All,"" and ""Non-anchored."" The horizontal axis represents two categories labeled as ""Semantic"" and ""Numerical."" The vertical axis is labeled ""Score"" and has a linear scale ranging from 0.0 to 0.8 with tick marks at intervals of 0.2. Each category consists of three bars distinguished by color: red for ""Anchored,"" gray for ""All,"" and blue for ""Non-anchored."" In the ""Semantic"" category, the red ""Anchored"" bar achieves a score of 75.3%, the gray ""All"" bar a score of 43.2%, and the blue ""Non-anchored"" bar a score of 32.4%. In the ""Numerical"" category, the red ""Anchored"" bar reaches a score of 40.0%, the gray ""All"" bar a score of 8.0%, and the blue ""Non-anchored"" bar a score of 5.4%. The bars are not sorted in a particular order but are grouped by category and condition. No bars appear to be visually emphasized beyond their color differentiation."
"087_2505.15392v1_An Empirical Study of the Anchoring Effect in LLMs Existence,   Mechanism, and Potential Mitigations.pdf",087_2505.15392v1_Figure5.png,Categories of topic in semantic anchoring questions,Pie Chart,"The pie chart illustrates the distribution of various categories related to a specific topic, with a total of ten slices represented. Each slice is labeled and color-coded, with percentage labels inside the slices. The categories include ""Resource Consumption"" in dark blue at 15.0%, ""Willingness to Pay"" in light blue at 16.7%, ""Legal Judgment"" in orange at 6.7%, ""Time Usage"" in peach at 8.3%, ""Transportation Mileage"" in green at 6.7%, ""Daily Life Negotiation"" in light green at 8.3%, ""Demographic Age"" in red at 10.0%, ""Market Size"" in pink at 10.0%, ""Resource Reserves"" in purple at 10.0%, and ""Geomorphic Features"" in lavender at 8.3%. A legend accompanies the chart to aid in distinguishing the colors. The slices are visually ordered in a non-descending sequence as observed from the chart."
"087_2505.15392v1_An Empirical Study of the Anchoring Effect in LLMs Existence,   Mechanism, and Potential Mitigations.pdf",087_2505.15392v1_Figure6.png,Categories of topic in numerical anchoring questions,Pie Chart,"The pie chart represents various categories with different percentage shares. It consists of ten slices: ""Circumference"" is shown in dark blue and comprises 10%, ""Weight"" is in light blue at 15%, ""Height"" is orange with 12.5%, ""Quantity"" is in light orange covering 20%, ""Length"" is light green and holds 27.5%, ""Speed"" is red and takes up 2.5%, ""Volume"" is pink with 5%, ""Price"" is purple at 2.5%, and ""Distance"" is light purple with another 2.5%. Finally, ""Temperature"" is shown in green at 2.5%. There is a legend present to identify each slice by its color. Percentages are labeled inside the slices. The chart is visually organized by descending size of the slices."
089_2504.00406v1_VerifiAgent a Unified Verification Agent in Language Model Reasoning.pdf,089_2504.00406v1_Figure2.png,: GPT-4o Reasoner,Line Plot,"The plot illustrates the accuracy of different approaches on MATH 350 test cases using the GPT-4o reasoner. The x-axis represents the ""Number of Samples"" with a linear scale ranging from 1 to 10, with tick marks at each integer. The y-axis represents ""Accuracy (%)"" using a linear scale, ranging from 67 to 74, with tick marks at each integer. The blue line with circle markers and a solid style represents ""Majority Vote @N,"" starting at approximately 68.5% at 1 sample and rising steadily to about 73.5% at 10 samples. The orange line with circle markers and a solid style represents ""Best-of-N (Qwen2.5-Math-PRM-7B),"" starting at about 68% at 1 sample, showing a mild increase and leveling off around 70.5% at 10 samples. The green line with circle markers and a solid style represents ""Best-of-N (Qwen2.5-Math-7B-PRM800K),"" starting at about 68% and increasing to approximately 72.5% at 10 samples. The red star marks a single point for ""GPT-4o+VerifiAgent"" at around (10, 74%). The purple triangle marks a single point for ""GPT-4o"" at around (1, 69%). The plot includes a legend for line identification and indicates steady trends, peaks, and gradual rises across the described lines."
089_2504.00406v1_VerifiAgent a Unified Verification Agent in Language Model Reasoning.pdf,089_2504.00406v1_Figure3.png,: Qwen2.5-Math-7B-Instruct Reasoner,Line Plot,"The plot illustrates the accuracy performance on MATH 350 test cases using the Qwen2.5-Math-7B-Instruct Reasoner. The x-axis is labeled ""Number of Samples"" and has a linear scale ranging from 2 to 10, with tick labels at evenly spaced intervals of 1. The y-axis is labeled ""Accuracy (%)"" and also has a linear scale ranging from 68 to 74, with tick labels at intervals of 1. There are five lines represented. The blue line with circular markers and a solid style shows the ""Majority Vote @N"" approach, starting at approximately 68% accuracy at 2 samples and ending around 73% at 10 samples, with a general upward trend. The orange line with square markers and a solid style represents the ""Best-of-N (Qwen2.5-Math-PRM-7B)"" method, starting at about 68% and rising to about 74% at 10 samples, with a peak around 4 samples. The green line with square markers and a solid style depicts the ""Best-of-N (Qwen2.5-Math-7B-PRM800K)"" approach, which starts around 68%, peaks at about 74% around 4 samples, then experiences fluctuations, ending at approximately 72%. The red star marker, indicating ""Qwen2.5-Math-7B-Instruct+VerifiAgent,"" shows a single data point at around 74% accuracy at 2 samples. The purple triangle marker represents the ""Qwen2.5-Math-7B-Instruct,"" with a single data point at about 71% accuracy at 2 samples."
089_2504.00406v1_VerifiAgent a Unified Verification Agent in Language Model Reasoning.pdf,089_2504.00406v1_Figure4.png,: width=,Bar Chart,"The bar chart illustrates the comparison of scores labeled as \( V_{\text{score}} \) across three categories: MATH, ProverQA, and StrategyQA. The categories are placed along the horizontal axis, which is arranged alphabetically. The vertical axis represents the value of \( V_{\text{score}} \), ranging from 0.65 to 1.00, with a linear scale and tick marks at intervals of 0.05. Each category contains two bars representing ""Correct Solution"" and ""Incorrect Solution,"" as indicated by the legend. ""Correct Solution"" bars are colored green, while ""Incorrect Solution"" bars are pink. In the MATH category, the green bar approximates a value close to 0.97, and the pink bar approximates 0.87. For ProverQA, the green bar is around 0.92, and the pink bar is about 0.82. In StrategyQA, the green bar is approximately 0.89, while the pink bar is about 0.76. All categories show the green bar (Correct Solution) as higher than the pink bar (Incorrect Solution). There is no specific sorting of the bars beyond the alphabetical arrangement of the categories. No bars are visually emphasized beyond their color differentiation."
089_2504.00406v1_VerifiAgent a Unified Verification Agent in Language Model Reasoning.pdf,089_2504.00406v1_Figure5.png,: The floating bar chart comparing $V_{score}$  distributions (mean ± std) for correct and incorrect solutions across three datasets. The horizontal grey line indicates the mean.,Pie Chart,"The chart set consists of four pie charts representing different categories labeled as MATH, StrategyQA, ProverQA, and ReWild. Each chart illustrates the distribution of Python Interpreter, Search Engine, and Symbolic Solver components. The MATH chart has two slices: Python Interpreter in blue covering 98.6% and Symbolic Solver in peach at 1.4%. StrategyQA has a single green slice for Search Engine taking up 100%. ProverQA is represented by one peach slice solely covering 100% for Symbolic Solver. The ReWild chart contains three slices: Python Interpreter in blue at 84.5%, Search Engine in green at 4.3%, and Symbolic Solver in peach at 11.2%. A legend on the right identifies the colors for each component, and percentage labels are included within the slices. The slices do not appear to be visually emphasized or ordered in a particular way."
089_2504.00406v1_VerifiAgent a Unified Verification Agent in Language Model Reasoning.pdf,089_2504.00406v1_Figure6.png,"The proportion of different question types among VerifiAgent’s incorrectly verified examples by GPT-4o Reasoner. From top to bottom, the bars represent MATH, ProverQA, and StrategyQA datasets, respectively. For MATH and ProverQA, the number of questions in each type is the same. For the imbalanced StrategyQA, the proportion is normalised by the total number of questions per difficulty level.",Bar Chart,"The bar chart illustrates the distribution of percentages across different categories. The horizontal axis represents the percentage values labeled as ""Percentage (%)"" with a linear scale ranging from 0 to 100, marked at intervals of 20%. The vertical axis contains three stacked bars, each split into segments representing different categories. The first bar depicts various mathematical topics, with segments labeled as PreCal (26.9%), Geo (23.9%), Count&Prob (11.9%), PreAlg (11.9%), Int Alg (10.4%), Alg (9.0%), and Num Thr (6.0%). These segments are colored pink, light blue, light purple, light green, pink, peach, and grey, respectively. The second bar indicates difficulty levels with segments labeled Hard (58.6%), Medium (27.1%), and Easy (14.3%) in pink, light blue, and light purple, respectively. The third bar shows performance levels labeled as Level 2 (30.1%), Level 5 (28.5%), Level 3 (21.7%), and Level 4 (19.7%) colored pink, light blue, light purple, and light green, respectively. The categories within each bar are stacked next to each other to illustrate their proportionate contributions to the overall percentage of 100%. There is no specific sorting or visual emphasis indicated in any of the bars."
089_2504.00406v1_VerifiAgent a Unified Verification Agent in Language Model Reasoning.pdf,089_2504.00406v1_Figure7.png,"The proportion of different question types among the incorrectly answered examples by GPT-4o Reasoner. From top to bottom, the bars represent MATH, ProverQA, and StrategyQA datasets, respectively. For MATH and ProverQA, the number of questions in each type is the same. For the imbalanced StrategyQA, the proportion is normalised by the total number of questions per difficulty level.",Bar Chart,"The bar chart illustrates the distribution of categories within three different domains labeled as MATH, ProverQA, and StrategyQA, represented horizontally. The horizontal axis represents the percentage, labeled as ""Percentage (%)"", with a linear scale ranging from 0 to 100, marked at intervals of 20 units. Each bar serves as a stacked representation of various subcategories, with distinct colors identifying each segment within the bars. In the MATH category, segments include PreCal (light blue, 25.2%), Geo (light blue, 22.5%), Int Alg (light pink, 19.8%), Count/Prob (pink, 14.4%), PreAlg (green, 8.1%), Num Thr (yellow, 5.4%), and Alg (orange, 4.5%). In ProverQA, the segments are labeled as Hard (red, 56.8%), Medium (blue, 29.7%), and Easy (purple, 13.5%). The StrategyQA category consists of Level 3 (purple, 32.0%), Level 2 (red, 27.2%), Level 5 (blue, 26.8%), and Level 4 (green, 13.9%). Stacks are arranged in a straightforward sequence, with no specific sorting pattern or visual emphasis on any particular bar segment."
091_2505.03052v1_Teaching Models to Understand (but not Generate) High-risk Data.pdf,091_2505.03052v1_Figure3.png,"Effect of toxic data quantity on model understanding and generation of toxicity. Models in the upper left region exhibit the best understanding-generation tradeoff. Masked  shines at high data scales, showing both high understanding and low toxicity.",Line Plot,"The plot illustrates the relationship between Generation size in RealToxicityPrompts and Understanding as measured by CivilComments AUROC. The x-axis is labeled ""Generation (RealToxicityPrompts)"" with a linear scale ranging from approximately 0.16 to 0.24 with no units indicated and tick marks at intervals of 0.02. The y-axis is labeled ""Understanding (CivilComments AUROC)"" with a linear scale ranging from 0.815 to 0.845 with no units indicated and tick marks at intervals of 0.005. There are four lines in the plot. The first line is black, represented by a solid circle marker, indicating a constant value at the point 0.16 on the x-axis and approximately 0.815 on the y-axis. The second line is blue, has circular markers, and a solid line style. It starts at (0.16, 0.825) and ends at (0.18, 0.838). Notable points include (0.17, 0.835) and (0.18, 0.84). The third line is orange, also with circular markers and a solid line. It begins at (0.16, 0.820) and steadily increases to reach (0.24, 0.841), passing through (0.18, 0.825) and (0.22, 0.838). The fourth line is green, following a similar pattern to the others with circular markers and a solid line. It starts at (0.16, 0.835), peaks at (0.18, 0.838), and concludes at (0.20, 0.837), displaying fluctuations at (0.17, 0.837) and (0.19, 0.836)."
092_2505.20612v1_Roboflow100-VL A Multi-Domain Object Detection Benchmark for   Vision-Language Models.pdf,092_2505.20612v1_Figure16.png,"Dataset Statistics. The table on the left provides details on the number of classes, images, and annotations across different dataset types within RF100-VL. The figure on the right illustrates the distribution of dataset types by count. Notably, despite containing 100 datasets, RF100-VL is 50% the size of COCO <cit.> (by number of images) and can feasibly be benchmarked on academic-level compute.",Pie Chart,"The chart represents the distribution of various categories, with a total of seven slices. The categories and their attributes are as follows: ""Aerial"" comprises 11% and is colored dark purple; ""Document"" accounts for 10% with a slightly lighter purple; ""Flora & Fauna"" is the largest segment at 23% and a bright purple; ""Industrial"" follows at 22% in medium purple; ""Medical"" is 13% in a lighter shade of purple; ""Sports"" represents 6% in the lightest purple; and ""Other"" is 15% in medium-dark purple. Each slice is labeled with its respective category and percentage. There is no legend, but the percentages are displayed within the slices. The segments do not appear to be ordered by size or visually emphasized."
093_2505.15117v1_An Empirical Study on Reinforcement Learning for Reasoning-Search   Interleaved LLM Agents.pdf,093_2505.15117v1_Figure1.png,: Training reward ($\lambda_f$),Line Plot,"The line plot illustrates different levels of train rewards over a series of steps, comparing the effects of varying lambda (λ) values on training performance. The x-axis is labeled ""Step"" and follows a linear scale, ranging from 0 to 400 with tick labels at intervals of 50. The y-axis is labeled ""Train Reward,"" also using a linear scale, ranging from 0.1 to 0.9 with tick labels at intervals of 0.1. There are five lines on the plot. The blue line with circle markers and a solid style, representing λ=0.0, begins at a train reward near 0.1 at step 0 and generally increases to approximately 0.5 at step 400. The orange line with square markers, also solid, representing λ=0.2, follows a similar trend, starting around 0.1 and rising steadily to about 0.6 by the end. The green line with diamond markers, solid style, for λ=0.4, starts near 0.1 and reaches approximately 0.7 at step 400. The red line with triangle markers, solid style, representing λ=0.6, begins near 0.1 and rises more steeply to around 0.8 towards the end. The purple line with cross markers, solid style, representing λ=0.8, starts close to 0.1 and increases rapidly to approximately 0.9, maintaining a stable trend from step 100 onward."
093_2505.15117v1_An Empirical Study on Reinforcement Learning for Reasoning-Search   Interleaved LLM Agents.pdf,093_2505.15117v1_Figure2.png,: Training reward ($\lambda_r$),Line Plot,"The plot illustrates the relationship between format reward and final score. The x-axis, labeled ""format reward,"" uses a linear scale, ranging from 0.2 to 0.8, with ticks at intervals of 0.1. The y-axis, labeled ""final score,"" also employs a linear scale, ranging from 0.405 to 0.435, with tick intervals of 0.005. The plot contains a single line depicted in blue, with circular markers and a solid line style. Beginning at approximately 0.2 on the x-axis, the final score is near 0.405. The line then rises to a peak at x = 0.4, where the final score reaches 0.435. Subsequently, the line descends, reaching a final score of about 0.405 at x = 0.8."
093_2505.15117v1_An Empirical Study on Reinforcement Learning for Reasoning-Search   Interleaved LLM Agents.pdf,093_2505.15117v1_Figure3.png,": Empirical analyses on format reward and intermediate retrieval reward. (a) Training reward curves with varying format reward scaling factors ($\lambda_f$); larger $\lambda_f$ values lead to faster convergence. (b) Impact of $\lambda_f$ on final model performance; a small $\lambda_f$ is ineffective, while an excessively large $\lambda_f$ may cause overfitting to format reward. (c) Training reward curves under different intermediate retrieval reward scaling factors ($\lambda_r$); varying $\lambda_r$ has limited effect on learning dynamics. (d) Effect of $\lambda_r$ on final model performance; increasing $\lambda_r$ degrades performance, suggesting that intermediate retrieval rewards are unnecessary, as the outcome reward sufficiently encourages effective query formulation. (LLM: Qwen2.5-7B-Base; RL Algorithm: PPO)",Line Plot,"The plot illustrates the changes in train reward as a function of steps for different values of λ_r. The x-axis is labeled ""Step,"" is on a linear scale, ranges from 0 to 400, and has tick labels at intervals of 50 steps. The y-axis is labeled ""Train Reward,"" is also on a linear scale, ranges from 0.0 to 0.9, and has tick labels at intervals of 0.1. There are four lines on the plot. The blue line with circular markers and a solid style starts at approximately 0.2 at step 0, increasing steadily to about 0.6 by step 400. The orange line with square markers and a solid style starts at approximately 0.2 and rises more sharply to around 0.7 by step 400. The green line with diamond markers and a solid style begins at approximately 0.2, climbing more rapidly to reach about 0.75 by step 400. The red line with triangular markers and a solid style also commences at about 0.2, increasing quickly to approximately 0.8 by step 400, showing a prominent upward trend throughout."
093_2505.15117v1_An Empirical Study on Reinforcement Learning for Reasoning-Search   Interleaved LLM Agents.pdf,093_2505.15117v1_Figure4.png,: Caption not extracted,Line Plot,"The plot illustrates the relationship between retrieval reward and final score. The x-axis is labeled ""retrieval reward,"" with a linear scale ranging from 0.0 to 0.5, marked at intervals of 0.1. The y-axis is labeled ""final score,"" with a linear scale ranging from 0.39 to 0.43, marked at intervals of 0.01. There is a single line on the plot, which is blue with circular markers and a solid line style. The line starts at a final score of 0.43 when the retrieval reward is 0.0 and decreases steadily as the retrieval reward increases. The line passes through a final score of 0.42 at a retrieval reward of 0.1, then continues to 0.41 at 0.3, and ends at 0.39 when the retrieval reward is 0.5."
093_2505.15117v1_An Empirical Study on Reinforcement Learning for Reasoning-Search   Interleaved LLM Agents.pdf,093_2505.15117v1_Figure5.png,: Training Reward,Line Plot,"This plot illustrates the changes in train reward over time, represented by the step count. The x-axis is labeled ""Step,"" with a linear scale ranging from 0 to 1000, with tick intervals at every 100 steps. The y-axis is labeled ""Train Reward,"" also on a linear scale, ranging from 0 to 0.8, with tick intervals at every 0.1 units. The plot includes four lines. The blue line with circle markers and solid style represents ""Deepseek w. PPO,"" starting at a reward slightly above 0.1 at step 0 and showing an upward trend to approximately 0.7 by step 900, with some fluctuations. The orange line with square markers and solid style represents ""Deepseek w. GRPO,"" beginning near 0.08 at step 0, showing minimal increase and mostly remaining steady around 0.1. The green line with diamond markers and solid style represents ""Base w. PPO,"" starting near 0.15 and peaking at about 0.65 by step 200, then maintaining a steady level. The red line with triangle markers and a solid style represents ""Base w. GRPO,"" starting near 0.2 and reaching a peak of 0.62 by step 100, before declining around step 200 and stabilizing near 0.1."
093_2505.15117v1_An Empirical Study on Reinforcement Learning for Reasoning-Search   Interleaved LLM Agents.pdf,093_2505.15117v1_Figure6.png,: Training Reward,Line Plot,"The plot illustrates the number of search calls in relation to the step count for different configurations. The x-axis is labeled ""Step,"" with a linear scale ranging from 0 to 1000, marked at intervals of 200. The y-axis is labeled ""# Search Call,"" also employing a linear scale, extending from 0 to 5 with increments of 1. There are four distinct lines in the plot. The first line is blue, with circular markers and a solid style, representing ""Deepseek w. PPO."" It starts near 1 at Step 0 and gradually increases, reaching approximately 4.5 at Step 900, showing variability throughout. The second line, orange with square markers and a solid style, indicates ""Deepseek w. GRPO."" It begins slightly above 1 at Step 0 and maintains a relatively steady trajectory, with fluctuations around 1 until Step 300, after which it slightly declines. The third line, green with diamond markers and a solid style, represents ""Base w. PPO."" It starts around 2 at Step 0, briefly rises to about 3 at Step 100, then decreases back to near 2 around Step 180, followed by an upward trend from Step 250, ending near 3.5 at Step 360. Lastly, the red line, featuring triangular markers and a solid style for ""Base w. GRPO,"" begins around 2 at Step 0, briefly ascending to approximately 2.5 at Step 100, and subsequently fluctuates around 1.5 until Step 180, post which it remains stable."
093_2505.15117v1_An Empirical Study on Reinforcement Learning for Reasoning-Search   Interleaved LLM Agents.pdf,093_2505.15117v1_Figure7.png,": The study of underlying pretrained LLM for development of LLM-based search agents with RL. (a) Training reward with different type of LLMs - general-purpose LLM (Qwen2.5-7B-Base) and reasoning LLM (DeepSeek-R1-Distill-Qwen-7B). We observe that general-purpose LLM performs better than reasoning LLMs with both PPO and GRPO. (b) # of Search engine calls with different type of LLMs: General LLM learns to call the search engine faster than the reasoning LLM. This potentially stems from the fact the general LLMs are better for following instructions. (c) Training reward with different size of LLMs: Larger LLMs can lead to higher training reward. (d) Test accuracy with different size of LLMs: On the challenging Bamboogle dataset <cit.>, the performance increases consistently as the LLM size increases.",Line Plot,"The line plot illustrates the relationship between ""Step"" and ""Train Reward"" across four different datasets labeled as 3B, 7B, 14B, and 32B. The x-axis represents ""Step"" with a linear scale ranging from 0 to 250, with major tick marks at intervals of 50 units. The y-axis denotes ""Train Reward"" on a linear scale, spanning from 0.1 to 0.6 with tick marks at intervals of 0.1. The line corresponding to 3B is blue with circle markers and a solid line style. It starts at a Train Reward of approximately 0.1 at Step 0 and rises steadily to about 0.3 at Step 50, continuing to about 0.5 at Step 150 before maintaining a nearly consistent level. The line for 7B, marked in orange with square markers, begins at around 0.1 at Step 0, increasing to about 0.4 by Step 50, reaching approximately 0.55 by Step 100 and fluctuating slightly near 0.55 through to Step 250, with a steep drop towards the end. The 14B line is represented in green with diamond markers and a solid line, starting just under 0.1 at Step 0, rising sharply to around 0.55 by Step 50, and showing small fluctuations near 0.55 through to Step 250. Lastly, the 32B line, depicted in red with triangle markers, begins just over 0.1 at Step 0, sees an increase to above 0.5 by Step 50, then maintains near 0.6, exhibiting a slight upward trend toward the end."
093_2505.15117v1_An Empirical Study on Reinforcement Learning for Reasoning-Search   Interleaved LLM Agents.pdf,093_2505.15117v1_Figure8.png,: Caption not extracted,Line Plot,"The plot illustrates the relationship between LLM Size, measured in billion parameters, and the Bamboogle Score. The x-axis is labeled ""LLM Size (Billion Parameters)"" and features a linear scale ranging from 0 to 35, with tick marks at intervals of 5. The y-axis is labeled ""Bamboogle Score,"" also using a linear scale, ranging from 0.30 to 0.55, with tick marks at intervals of 0.05. The plot presents a single blue line with circular markers and a solid line style. Starting at approximately (4, 0.30), the line shows an upward trend, reaching around (14, 0.51), and ending near (30, 0.55). The plot shows a steady increase without any visible peaks or declines along the x-axis."
093_2505.15117v1_An Empirical Study on Reinforcement Learning for Reasoning-Search   Interleaved LLM Agents.pdf,093_2505.15117v1_Figure10.png,: # Search calls,Line Plot,"The plot illustrates the train reward as a function of the step in a process, comparing different methods. The x-axis represents the ""Step"" with a linear scale ranging from 0 to 400, with tick labels at intervals of 50. The y-axis represents the ""Train Reward,"" also on a linear scale, ranging from 0.0 to 0.5, with tick labels at intervals of 0.1. There are four lines in the plot. The first line is blue with circle markers and a solid style, starting around 0.05 at step 0 and gradually increasing to about 0.2 by step 400, showing a steady upward trend. The second line is orange with square markers and a solid style, beginning at approximately 0.05 and rising to about 0.45 by step 250, before dropping sharply to below 0.1 by step 400. The third line is green with diamond markers and a solid style, starting at about 0.02 and increasing to around 0.4 by step 400, illustrating a steady increase. The fourth line is red with triangle markers and a solid style, beginning at close to 0.05 and increasing to about 0.5 by step 400, showing a consistent upward trend with a peak around step 300."
093_2505.15117v1_An Empirical Study on Reinforcement Learning for Reasoning-Search   Interleaved LLM Agents.pdf,093_2505.15117v1_Figure11.png,: Caption not extracted,Line Plot,"The line plot illustrates the performance measured by the number of search calls over a series of steps for different methods. The x-axis is labeled ""Step,"" with a linear scale ranging from 0 to 400, with tick marks at intervals of 50. The y-axis is labeled ""# Search Call,"" also using a linear scale with values from 0 to 3.5, with tick marks at intervals of 0.5. The plot features four lines: a blue line with circular markers representing ""Random,"" starting at approximately 3.0 and steadily declining to about 0.5 around step 125, then remaining stable; an orange line with square markers for ""BM25,"" showing variably high peaks, starting near 2.0 and fluctuating substantially after step 350, ending around 3.3; a green line with diamond markers labeled ""E5 (HNSW),"" which begins around 1.5, maintaining a relatively steady trend with minor fluctuations; and a red line with triangular markers for ""E5 (Exact),"" starting just below 2.0, showing slight fluctuations throughout and ending slightly below its initial value. Each line denotes a distinct behavior across the steps, as visually represented by their unique styles and trends."
093_2505.15117v1_An Empirical Study on Reinforcement Learning for Reasoning-Search   Interleaved LLM Agents.pdf,093_2505.15117v1_Figure12.png,: Training Reward,Line Plot,"The plot illustrates the relationship between ""Step"" and ""Train Reward"" for various datasets. The x-axis is labeled ""Step,"" with a linear scale ranging from 0 to 500, marked at intervals of 100. The y-axis is labeled ""Train Reward,"" also on a linear scale, ranging from 0.0 to 0.8 with tick intervals of 0.1. The plot contains five lines, each corresponding to different dataset sizes. The blue line, representing |D|=1, uses circular markers and a solid line style. It begins at a reward of approximately 0.1 at step 0, rising steeply and stabilizing at around 0.8 by step 100. The orange line, representing |D|=10, with square markers and a solid line style, starts around 0.1 at step 0, increases steadily, and approaches 0.7 by step 400, where it remains. The green line, representing |D|=100, employs diamond markers and a solid style, starting at about 0.1, increasing steadily to approximately 0.7 by step 250 before leveling off. The red line for |D|=1000, with triangle markers and solid style, begins near 0.1, following a similar upward trend to the green line, stabilizing around 0.7 by step 250. Lastly, the purple line for |D|=10000 uses x-shaped markers and a solid style, starting at 0.1, climbing gradually but consistently, achieving a reward near 0.6 by step 500."
093_2505.15117v1_An Empirical Study on Reinforcement Learning for Reasoning-Search   Interleaved LLM Agents.pdf,093_2505.15117v1_Figure13.png,: Training Reward,Line Plot,"The plot illustrates the number of search calls over a series of steps for different values of |D|. The x-axis is labeled ""Step"" and features a linear scale ranging from 0 to 500, with tick labels at regular intervals of 100 units. The y-axis is labeled ""# Search Call"" and also uses a linear scale, ranging from 0.0 to 3.0, with tick labels at intervals of 0.5 units. The plot contains five lines, each representing a different |D| value. The blue line with circle markers, representing |D|=1, starts at 1.0 at step 0, drops to 0.0 by step 30, and remains steady through step 500. The orange line with square markers, representing |D|=10, begins at about 1.4, fluctuates, and peaks at around step 70 with a value of approximately 1.8, before declining steeply to 0.0 at step 380. The green line with triangle markers, representing |D|=100, starts just above 1.0 and stays relatively stable around 1.0 throughout the extent of the plot. The red line with upside-down triangle markers, representing |D|=1000, exhibits a start value of around 1.0, maintains an almost steady trend, and slightly declines to just below 1.0 by step 500. Lastly, the purple line with cross markers, representing |D|=10000, starts around 1.0, with increased fluctuations peaking at approximately 2.7 near step 370, and then lowers slightly to around 1.5 by the last observed step."
093_2505.15117v1_An Empirical Study on Reinforcement Learning for Reasoning-Search   Interleaved LLM Agents.pdf,093_2505.15117v1_Figure14.png,": Data scaling effects in RL training for search agents. (a) Training reward under PPO with varying dataset sizes: Smaller training sets result in faster convergence and higher training rewards, likely due to overfitting. (b) Number of search engine calls under PPO: Training with a single example fails to induce search behavior, while 10 samples lead to unstable learning. In contrast, using 100 or 1,000 samples enables the model to learn stable search behavior, and training with 10,000 samples further improves performance. (c) Training reward under GRPO with varying dataset sizes: Similar to PPO, smaller datasets yield faster convergence and higher rewards, again suggesting potential overfitting. (d) Number of search engine calls under GRPO: A single training sample is insufficient for search behavior to emerge, whereas larger datasets facilitate stable learning of search interactions.",Line Plot,"The plot illustrates the relationship between ""Step"" on the x-axis and ""Train Reward"" on the y-axis for different dataset sizes denoted by |D|. The x-axis is labeled ""Step"" and uses a linear scale ranging from 0 to 200 with tick marks at intervals of 25. The y-axis, labeled ""Train Reward,"" is also on a linear scale and ranges from 0.0 to 1.0, with tick marks at intervals of 0.2. There are five lines representing different values of |D|. The blue line with circle markers for |D|=1 starts near a reward of 0.2, quickly rises and fluctuates around 1.0 after 25 steps, and then stabilizes. The orange line with square markers for |D|=10 starts similarly but oscillates between 0.8 and 1.0 after about 50 steps. The green line with diamond markers for |D|=100 begins at a similar initial value and steadily increases to about 0.8 near 150 steps, maintaining this level with minor fluctuations. The red line with triangle markers for |D|=1000 follows a similar pattern to the green line, reaching approximately 0.7 at around 200 steps. The purple line with cross markers for |D|=10000 exhibits a gradual upward trend, reaching around 0.5 at the graph's end, showing a slower increase throughout."
093_2505.15117v1_An Empirical Study on Reinforcement Learning for Reasoning-Search   Interleaved LLM Agents.pdf,093_2505.15117v1_Figure15.png,: Caption not extracted,Line Plot,"The plot illustrates the relationship between ""Step"" and ""# Search Call"" for different values of |D|. The x-axis is labeled ""Step,"" has a linear scale, and ranges from 0 to 200 with tick marks at intervals of 25. The y-axis is labeled ""# Search Call,"" also employs a linear scale, ranging from 0.0 to 1.4 with tick marks at intervals of 0.2. The blue solid line with circular markers represents |D|=1, starting at 0.8, sharply declining to nearly 0 around step 50, and remaining stable thereafter. The orange dashed line with square markers represents |D|=10, starting around 0.8, peaking near 1.3 at step 50, then stabilizing slightly above 1.0. The green solid line with diamond markers for |D|=100 starts at 0.8, peaks near 1.2 at step 50, and fluctuates around 1.0 towards the end. The red solid line with triangle markers represents |D|=1000, beginning at 0.8 and maintaining a fairly stable trend around 1.0 after a slight rise. The purple line with X-shaped markers for |D|=10000 starts at 0.8, peaking at approximately 1.3 around step 70, and finally stabilizes around 1.0."
093_2505.15117v1_An Empirical Study on Reinforcement Learning for Reasoning-Search   Interleaved LLM Agents.pdf,093_2505.15117v1_Figure16.png,: Train Reward,Line Plot,"The plot illustrates the training reward over time for different models across varying steps. The x-axis, labeled ""Step,"" employs a linear scale ranging from 0 to 400, with numeric tick labels at intervals of 50 steps. The y-axis is labeled ""Train Reward"" and also uses a linear scale, with values ranging from 0.2 to 0.7, and tick labels at intervals of 0.1 units. Four lines are present in the plot: a blue line featuring circular markers and a solid style represents ""Deepseek w. PPO."" This line starts at a value of approximately 0.2, maintains a steady increase peaking around step 100 at roughly 0.45, then shows variation while generally stabilizing around 0.4 towards the end. The orange line, depicted with square markers and a dashed style, represents ""Deepseek w. GRPO."" It begins near 0.25, steadily increasing to a peak of about 0.35 around step 150, and then fluctuating while maintaining a level around 0.3 towards the final steps. A green line with diamond markers and a solid style signifies ""Base w. PPO,"" starting at approximately 0.25 and consistently rising up to a peak close to 0.55 at step 100, before slightly decreasing and stabilizing around 0.5. Lastly, the red line, using triangle markers and a dashed style for ""Base w. GRPO,"" initiates at a similar starting point of around 0.25, climbing steadily to a peak near 0.6 at step 200, followed by minor fluctuations while maintaining a consistent level towards 0.6 by the plot's conclusion."
093_2505.15117v1_An Empirical Study on Reinforcement Learning for Reasoning-Search   Interleaved LLM Agents.pdf,093_2505.15117v1_Figure17.png,": The study of underlying pretrained LLM for development of search agents with RL. (a) Training reward with different type of LLMs - general-purpose LLM (Qwen2.5-14B-Base) and reasoning LLM (DeepSeek-R1-Distill-Qwen-14B). We observe that general-purpose LLM performs better than reasoning LLMs with both PPO and GRPO. (b) # of Search engine calls with different type of LLMs: Both the general-purpose LLM and the reasoning-specialized LLM demonstrate the ability to learn when to call the search engine. However, the general-purpose LLM achieves better final performance, potentially due to its superior capability in formulating effective search queries.",Line Plot,"The plot illustrates the number of search calls over a range of steps for different conditions or models. The x-axis represents the ""Step,"" using a linear scale ranging from 0 to 400, with tick marks at intervals of 50. The y-axis shows the ""# Search Call,"" also on a linear scale, ranging from 0 to 4, with intervals of 1 unit. The plot includes four lines: a blue line with circle markers, labeled ""Deepseek w. PPO,"" is solid and exhibits values starting around 0 at step 0, gradually increasing and fluctuating, reaching approximately 4 at step 400. The orange line with square markers, labeled ""Deepseek w. GRPO,"" is solid, beginning near 0 around step 0, rising and leveling off between 3 and 4 by step 200, and remaining stable thereafter. A green line with diamond markers, labeled ""Base w. PPO,"" is solid, starting around 2 at step 0, maintaining a steady trend toward step 400. The red line with triangle markers, labeled ""Base w. GRPO,"" is solid, commencing near 1 at the start, and exhibiting a stable trend, with minor fluctuations, throughout the plot to step 400."
097_2504.17480v3_Unified Attacks to Large Language Model Watermarks Spoofing and   Scrubbing in Unauthorized Knowledg.pdf,097_2504.17480v3_Figure3.png,"Accuracy of watermark classifiers with increasing training samples. Evaluation is conducted across six watermarking schemes: KGW (N=1, N=2, N=3), Unigram, and SynthID-Text (N=2, N=3).",Line Plot,"The plot illustrates the relationship between the number of training samples and the classifier accuracy for different methods. The x-axis is labeled ""Training Samples"" with a scale type that appears to be logarithmic, ranging from 0.1K to 10K samples, with tick marks at 0.1K, 1K, 5K, and 10K. The y-axis is labeled ""Classifier Accuracy (%)"" with a linear scale, ranging from 50% to 90%, with tick marks at intervals of 10%. Each line represents a different method. The blue line with circle markers and a solid style for ""KGW (N=1)"" starts around 50% and increases to about 75% at 10K. The orange line with square markers for ""KGW (N=2)"" follows a similar trend, starting near 50% and reaching about 75%. The green line with triangle markers for ""KGW (N=3)"" starts near 50%, peaking at approximately 77% at 10K. The red line with diamond markers for ""Unigram"" begins at 50% and sharply rises to about 90% at 10K. The purple line with inverted triangle markers labeled ""SynthID-Text (N=2)"" starts around 55% and climbs to approximately 80% at the last point. Lastly, the brown line with star markers for ""SynthID-Text (N=3)"" begins slightly above 50% and increases steadily to about 73% at 10K."
097_2504.17480v3_Unified Attacks to Large Language Model Watermarks Spoofing and   Scrubbing in Unauthorized Knowledg.pdf,097_2504.17480v3_Figure4.png,Token length sensitivity analysis of watermark detection accuracy across multiple watermark schemes.,Line Plot,"The plot illustrates the relationship between token lengths and accuracy for different models. The x-axis represents ""Token Lengths,"" using a linear scale with values ranging from 50 to 1000, labeled in units of tokens, and with tick intervals at 50, 100, 500, and 1000. The y-axis represents ""Accuracy (%)"" on a linear scale, ranging from 50% to 90%, with tick intervals of 10%. There are six lines, each representing different models: The blue line with circle markers represents ""KGW (N=1),"" starting at 50% at 50 tokens and increasing steadily to about 68% at 1000 tokens. The orange line with square markers for ""KGW (N=2)"" starts similarly at 50% and ends near 70%. The green line with triangle markers, ""KGW (N=3),"" shows a similar trend, starting at 50% and rising to approximately 72%. The red line with diamond markers, ""Unigram,"" begins at 51% and displays a sharp ascent to 90% by 1000 tokens. The purple line with inverted triangle markers, ""SynthID-Text (N=2),"" starts at 51% and increases to about 73%. The brown line with star markers for ""SynthID-Text (N=3)"" starts at 51% and moves upward to around 69% by the end of the x-axis."
098_2505.13718v2_Warm Up Before You Train Unlocking General Reasoning in   Resource-Constrained Settings.pdf,098_2505.13718v2_Figure2.png,"Absolute percentage increase relative to the base model for MATH, HumanEval+, and MMLU-Pro subsets (Physics & History) for various models: RLVR on base model, warmup only, and RLVR on warmed-up model",Bar Chart,"The bar chart illustrates the performance difference by training task, highlighting the absolute difference from a baseline across four categories: MATH, HumanEval+, Physics, and History. The horizontal axis categorizes these tasks, with each category further divided into three bars representing different training methods: ""Base + RL (ckpt-x)"", ""Warmup"", and ""Warmup + RL (ckpt-x)"", where ""x"" varies for each method. The vertical axis represents the ""Absolute Difference from Baseline,"" using a linear scale ranging from 0 to 30, with tick label intervals at every 5 units. In the MATH category, the bars display values for ""Base + RL (ckpt-250)"" at approximately 14.0, ""Warmup"" at 10.2, and ""Warmup + RL (ckpt-100)"" at 20.7. For HumanEval+, the values are 24.3 for ""Base + RL (ckpt-100),"" 15.3 for ""Warmup,"" and 29.3 for ""Warmup + RL (ckpt-50)."" In Physics, the values are 10.0 for ""Base + RL (ckpt-200),"" 9.8 for ""Warmup,"" and 15.9 for ""Warmup + RL (ckpt-150)."" Lastly, in History, they are 14.8 for ""Base + RL (ckpt-150),"" 5.1 for ""Warmup,"" and 10.8 for ""Warmup + RL (ckpt-200)."" The bars are visually represented in three different shades of blue corresponding to each training type and arranged side-by-side within each category. The chart is not sorted in any specific order, and no bars are visually emphasized with additional shading or outlining."
098_2505.13718v2_Warm Up Before You Train Unlocking General Reasoning in   Resource-Constrained Settings.pdf,098_2505.13718v2_Figure6.png,"Results of Qwen2.5-3B K&K distillation. Loss curve shown on top & performance on MATH500 shown on bottom. Choosing a higher learning rate, $2e-5$ has a chance of overfitting to the K&K domain rather than learning generalizable reasoning behaviors.",Line Plot,"The plot illustrates the loss curves of two different learning rates over a series of training steps. The x-axis is labeled ""Training Steps"" and uses a linear scale ranging from 0 to 1000 with tick marks at intervals of 200 steps. The y-axis is labeled ""Loss"" and also uses a linear scale, ranging from 0.3 to 1.0, with tick marks at intervals of 0.1. There are two lines in the plot. The first line is dark green with no marker and a solid line style, representing a learning rate of 1e-6. This line starts near 0.95 at 0 training steps, declines steeply at first, and then exhibits a gradual decrease, ending around 0.5 at 1000 training steps. The second line is light green, also with no marker and a solid line style, representing a learning rate of 2e-5. It starts slightly below 0.9 at 0 training steps, drops more sharply to around 0.4, and continues with smaller oscillations, stabilizing around 0.35 by 1000 training steps."
098_2505.13718v2_Warm Up Before You Train Unlocking General Reasoning in   Resource-Constrained Settings.pdf,098_2505.13718v2_Figure7.png,"Results of Qwen2.5-3B K&K distillation. Loss curve shown on top & performance on MATH500 shown on bottom. Choosing a higher learning rate, $2e-5$ has a chance of overfitting to the K&K domain rather than learning generalizable reasoning behaviors.",Line Plot,"The plot illustrates the performance comparison of two learning rates over training steps, as measured by accuracy in MATH500. The x-axis is labeled ""Training Steps,"" has a linear scale, and spans from 0 to 1092 with tick labels at intervals of 200. The y-axis is labeled ""Accuracy in MATH500,"" also uses a linear scale, and ranges from 30 to 55 with tick marks every 5 units. The plot contains two lines representing different learning rates. The first line, representing a learning rate of 1e-6, is dark teal with circular markers and a solid line style. It starts at approximately 45 at the initial step, reaches a peak of about 54 at 200 steps, then exhibits a slight decline and rise, ending at around 53 at 1092 steps. The second line, representing a learning rate of 2e-5, is light green with circular markers and a solid line style. It begins at approximately 45 at the initial step, declines steadily to a low of about 27 at 600 steps, and maintains this level with minor fluctuations, ending at 28 at 1092 steps."
098_2505.13718v2_Warm Up Before You Train Unlocking General Reasoning in   Resource-Constrained Settings.pdf,098_2505.13718v2_Figure8.png,"Training curves for Reward (left) and Completion Length (right) for MATH training, smoothed with a moving average (window size 10)",Line Plot,"The plot illustrates the reward trends over training steps for two conditions labeled ""Base"" and ""Warmup."" The x-axis is labeled ""Training Steps,"" using a linear scale, with a value range from 0 to 250. The tick labels are regularly spaced by intervals of 50 units. The y-axis, labeled ""Reward,"" also utilizes a linear scale, with a value range from -1.0 to 1.5, with tick labels at intervals of 0.5 units. The first line, representing the ""Base"" condition, is in blue with no marker shape and a solid line style. It starts below zero near -1.0 at the beginning, moves upward with fluctuations reaching close to 0.5 around the midpoint, and ends just below 1.0. The second line, representing the ""Warmup"" condition, is in purple with no marker shape and a solid line style. It begins just below zero, rising rapidly with oscillations to approximately 1.0, experiencing peaks and troughs, and concluding slightly above 1.0."
098_2505.13718v2_Warm Up Before You Train Unlocking General Reasoning in   Resource-Constrained Settings.pdf,098_2505.13718v2_Figure9.png,"Training curves for Reward (left) and Completion Length (right) for MATH training, smoothed with a moving average (window size 10)",Line Plot,"The plot illustrates the relationship between training steps and completion length for two different conditions, labeled ""Base"" and ""Warmup."" The x-axis is labeled ""Training Steps"" with a linear scale, ranging from 0 to 250, with tick marks at intervals of 50. The y-axis is labeled ""Completion Length,"" also with a linear scale, ranging from 0 to 2500, with tick marks at intervals of 500. The ""Base"" line is depicted in a darker blue with a solid line and no marker shapes. It starts around 100 at 0 training steps and exhibits a fluctuating trend, reaching around 500 by 250 training steps, with local peaks and troughs along the way. The ""Warmup"" line is shown in light blue with a solid line and no marker shapes. It begins at approximately 500 at 0 training steps and shows more significant fluctuations, peaking at over 2000 between 50 and 100 steps, and then decreasing to about 1000 by 250 training steps, with several ups and downs throughout the range."
098_2505.13718v2_Warm Up Before You Train Unlocking General Reasoning in   Resource-Constrained Settings.pdf,098_2505.13718v2_Figure10.png,"Training curves for Reward (left) and Completion Length (right) for HumanEval+ training, smoothed with a moving average (window size 10)",Line Plot,"The plot illustrates the change in reward over different training steps. The x-axis represents ""Training Steps"" and uses a linear scale ranging from 0 to 150, with tick labels at intervals of 25 steps. The y-axis represents ""Reward"" with a linear scale ranging from -0.5 to 1.5, with tick labels at intervals of 0.5 units. There are two lines depicted on the plot. The ""Base"" line is blue, depicted with a solid line and no markers. It starts at a reward slightly above -0.5 at 0 training steps, rises to around 0.5 at 50 training steps, and fluctuates around 0.5 until reaching near 0.7 at 150 training steps. The ""Warmup"" line is a dark purple color, also with a solid line and no markers. It starts slightly above -0.5 at 0 training steps, increases steadily to approximately 1.0 at 150 training steps, with some variability throughout the range. Both lines show an upward trend with variability but at different levels on the y-axis."
098_2505.13718v2_Warm Up Before You Train Unlocking General Reasoning in   Resource-Constrained Settings.pdf,098_2505.13718v2_Figure11.png,"Training curves for Reward (left) and Completion Length (right) for HumanEval+ training, smoothed with a moving average (window size 10)",Line Plot,"The plot illustrates the relationship between completion length and training steps in a dataset labeled ""HumanEval+."" The x-axis is labeled ""Training Steps,"" with a linear scale ranging from 0 to 150, marked at intervals of 25. The y-axis is labeled ""Completion Length,"" also on a linear scale, extending from 0 to 1200, with tick labels at intervals of 200. There are two lines present in the plot. The ""Base"" line is depicted in light blue with a solid line style and no markers. It begins at approximately 500 completion length at 0 training steps, decreases with some fluctuations, and ends near 200 completion length at the 150 training step mark. The ""Warmup"" line is shown in dark blue, also with a solid line style and no markers. It starts at around 1200 completion length at 0 training steps, experiences a sharp decline initially, followed by a more gradual decrease, and ends slightly below 400 completion length at 150 training steps."
098_2505.13718v2_Warm Up Before You Train Unlocking General Reasoning in   Resource-Constrained Settings.pdf,098_2505.13718v2_Figure12.png,"Training curves for Reward (left) and Completion Length (right) for Physics training, smoothed with a moving average (window size 10)",Line Plot,"The plot illustrates the relationship between training steps and reward for two different conditions labeled as ""Base"" and ""Warmup"" in a physics context. The x-axis is labeled ""Training Steps,"" with a linear scale ranging from 0 to 200, and tick marks are placed at intervals of 50. The y-axis is labeled ""Reward,"" also on a linear scale, ranging from -1.0 to 0.5, with tick marks at intervals of 0.5. The plot features two lines: a light blue line labeled ""Base"" and a darker blue line labeled ""Warmup,"" both with solid line styles. The ""Base"" line starts at approximately -1.0 at the origin and exhibits a generally increasing trend with fluctuations, ending around 0.5 at 200 training steps. Similarly, the ""Warmup"" line begins at around -1.0, showing a similar pattern of increase and fluctuation, with a noticeable dip below 0 at around 150 training steps, but it also ends near 0.5 at 200 training steps."
098_2505.13718v2_Warm Up Before You Train Unlocking General Reasoning in   Resource-Constrained Settings.pdf,098_2505.13718v2_Figure13.png,"Training curves for Reward (left) and Completion Length (right) for Physics training, smoothed with a moving average (window size 10)",Line Plot,"The plot illustrates the variation of completion length against training steps in a physics context. The x-axis is labeled ""Training Steps,"" with a linear scale ranging from 0 to 200, with tick marks at intervals of 50 steps. The y-axis is labeled ""Completion Length,"" with a linear scale ranging from 0 to 4000, also marked at intervals of 1000. There are two lines depicted: the ""Base"" line is blue with a solid line style and no markers, starting around 600 at the beginning, showing minor fluctuations, and ending slightly above 700. The ""Warmup"" line is represented in a lighter blue with a solid line style, starting at around 4200, dipping sharply to approximately 1500, and then experiencing fluctuations, with a low of around 1300, and ending closer to 1800. Both lines show various patterns of peaks and declines throughout the plot."
098_2505.13718v2_Warm Up Before You Train Unlocking General Reasoning in   Resource-Constrained Settings.pdf,098_2505.13718v2_Figure14.png,"Training curves for Reward (left) and Completion Length (right) for History training, smoothed with a moving average (window size 10)",Line Plot,"The plot represents the reward over training steps, illustrating the changes in two different conditions labeled as ""Base"" and ""Warmup."" The x-axis is labeled ""Training Steps,"" with a linear scale ranging from 0 to 200, and tick intervals at every 50 steps. The y-axis is labeled ""Reward,"" also with a linear scale, ranging from -0.50 to 1.00, and tick intervals at 0.25 units. The ""Base"" line is displayed in a blue color with a solid line style. It starts around 0.00, fluctuates with declines and peaks, and ends near 1.00 at the 200 mark, showing several noticeable variations in its trajectory. The ""Warmup"" line is in a lighter blue color with a solid line style. It begins below 0.00, follows a similar fluctuating trend, and concludes slightly below 0.75 at the 200 mark. Both lines exhibit a general upward trend with variations throughout the training steps."
098_2505.13718v2_Warm Up Before You Train Unlocking General Reasoning in   Resource-Constrained Settings.pdf,098_2505.13718v2_Figure15.png,"Training curves for Reward (left) and Completion Length (right) for History training, smoothed with a moving average (window size 10)",Line Plot,"The plot illustrates the change in completion length over training steps, comparing two conditions labeled ""Base"" and ""Warmup."" The x-axis represents ""Training Steps,"" using a linear scale with values ranging from 0 to 200, and tick marks are placed at 50-step intervals. The y-axis is labeled ""Completion Length,"" with a linear scale ranging from 0 to 1200, and ticks every 200 units. The line labeled ""Base"" is blue with a solid line style, featuring no markers. It starts at a completion length of around 200, rises to approximately 1000, then decreases, peaking again at several points before ending around 400. The line labeled ""Warmup"" is purple with a solid style, showing no markers. It begins at approximately 1000 and maintains a relatively steady, slightly decreasing trend, ending near 800."
